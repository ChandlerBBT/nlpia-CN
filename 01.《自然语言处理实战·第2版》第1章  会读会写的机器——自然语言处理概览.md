:::success
<h2 id="bESpv">Wordy machines：向量化自然语言模型</h2>
**第一部分**将开启你的自然语言处理（Natural Language Processing，NLP）之旅，介绍一些真实世界中的应用场景。

在第 1 章中，你将迅速开始思考，如何将这些处理文本的机器应用到你自己的生活中。希望你能够逐步体会到这些机器从自然语言文档中提取信息的神奇与强大。“词”（Words）是所有语言的基础，无论是编程语言中的关键词，还是你从小学习的自然语言词汇。

在第 2 章中，我们会提供一些工具，帮助你训练机器从文档中提取词语。这一过程远比你想象的复杂，我们会一一揭示其中的技巧。你将学会如何让机器自动将自然语言中的词归为语义相近的词组，而无需手动构造同义词表。

在第 3 章中，我们会统计这些词，并将其组合为向量（vectors），以表示一段文本的语义。这些向量可以表示任何长度文档的含义，无论是一条 280 字符的推文（tweet），还是一本 500 页的小说。

在第 4 章中，你将学习一些经过时间验证的数学技巧，把这些词向量压缩为更有用的主题向量（topic vectors）。

到本部分结束时，你将掌握构建常见 NLP 应用所需的基本工具，包括：信息检索（Information Retrieval）、语义搜索（Semantic Search），以及构建聊天机器人的业务逻辑或行为边界（Guardrails）。

:::

---

词语具有强大的力量。它们可以改变观念，甚至改变世界。若想掌控词语的力量，你需要理解自然语言处理（NLP）的工作原理，以及如何将其为你所用。近年来，NLP 的飞速发展引发了社会与商业各领域的技术爆炸式增长。本章将帮助你认识 NLP 的力量，并引导你思考如何将其应用于工作与生活中。

当你构建出真正能“读”和“写”词语的机器时，它们会让人感觉像人工智能（Artificial Intelligence，AI）。事实上，人们在社交媒体或新闻中谈论“人工智能”时，往往指的正是会话型自然语言处理（Conversational NLP）。阅读完本章后，你将更理性地使用“AI”这一术语，也能看清那些常见的误解与炒作（hype）背后的真相。

你不仅将学会如何构建 NLP 软件，还会了解如何将其整合进更大系统中，从而创造出智能且实用的行为逻辑，帮助你实现目标。更重要的是，你将成为一位“聪明的 AI 和 NLP 用户”，确保你构建的软件真正兑现其承诺。

随着你学习如何构建能读写词语的机器，你也将步入一个强大而前沿的领域——自然语言处理（NLP）与会话型人工智能（Conversational AI）。你可以借助这一领域的力量，为全人类建设更美好的未来。



<h2 id="CnPa2">1.1  编程语言 vs. 自然语言处理（NLP）</h2>
编程语言和自然语言（例如英语）其实非常相似。两者都是信息处理系统之间传递指令的媒介，可以实现人类与人类、人类与机器，甚至是机器与机器之间的思想交流。无论是自然语言还是编程语言，它们都包含“标记”（tokens）的概念——你可以先把它们理解为“词”。当一段文本输入机器时，第一步都是将其拆分成这些标记。

在编程语言中，标记的种类通常非常有限。例如，Python 编程语言仅包含 33 个保留关键字。而自然语言中的词汇可能多达几十万种。

编程语言与自然语言都使用“语法”（grammar）。语法是一组规则，用来指导如何将词组合成有意义的表达或语句。在计算机科学和英语语法课中，“表达式”（expression）和“语句”（statement）这两个词的含义也非常相似——它们都是构建语法规则的基础。你可能听说过“正则表达式”（regular expressions），它们用于匹配各种文本中的模式，包括自然语言和计算机程序。本书中你也会用到正则表达式。不过，相比你即将学习的 NLP 机器学习方法，正则表达式只是初级阶段。

尽管编程语言和自然语言有这些相似性，但想要用机器处理自然语言，你仍需要新的技能和工具。编程语言是人为设计的，用于明确指示计算机完成一系列基于二进制（0 和 1）的数学操作。编程语言是**确定性的**——也就是说，一行代码只有一种解释方式。而且，它们只需要被机器执行，而不需要被“理解”。有些编程语言通过“编译”（compilation）转为机器代码；另一些（如 Python）则由“解释器”逐行读取并执行。这些程序的目的就是让机器完成开发者指定的任务，无需理解背后的意图或构建更高层次的抽象模型。

相比之下，自然语言是自发、自然演化而来的。它用于在拥有大脑的生物体之间传达思想、理解和知识，而不是在“CPU”之间通信。自然语言必须能被各种“生物硬件”（wetware）——即大脑——理解。在某些情况下，自然语言甚至实现了跨物种的交流。例如，著名的大猩猩 Koko、黑猩猩 Washoe、鹦鹉 Alex 都掌握了一定数量的英语词汇。当 Alex 临终前，它对主人说出了一句似乎意义深远的话：“你要乖……我爱你。”

由于编程语言和自然语言的演化路径截然不同，它们也被用于完全不同的目的。我们不会使用编程语言来聊天或指路，也不会用自然语言来编写可以直接执行的程序。但这本书会教你如何**让自然语言可以被“运行”起来**。你将学习到，机器学习其实也是一种编程形式，而在 NLP 中，它能帮助程序推理出结论、发现新信息、生成抽象概念，甚至实现有意义的对话。

尽管自然语言无法直接被“编译”，但我们可以用**解析器（parsers）**将其拆解成组成部分，并理解这些部分之间的关系。在本书中，你会学到很多 Python 工具包，可以帮助你分析自然语言文本、比较文本之间的相似性、对文本进行摘要，甚至生成全新的文本。

不过，目前还没有任何单一算法或 Python 包，能够将自然语言完整转换成可执行的计算机指令。Stephen Wolfram——Mathematica 和 Wolfram Alpha 的创造者——毕生致力于打造一个通用、智能的“计算型机器”，能够用英语与人交流。为此，他不得不整合各种 NLP 和 AI 技术，并持续扩展这些系统，以支持新的自然语言表达方式。

在阅读本书的过程中，你将站在这些先驱者的肩膀上。如果你掌握了本书的全部内容，你也能将这些方法组合起来，构建出令人惊艳的智能对话系统。你将拥有足够的技能，加入那些致力于构建开源、负责任、可替代 ChatGPT 的开发者行列。

你也将学会如何将这些技能用于构建一个更公平、更具协作性的世界。

本章将向你展示，如何让软件处理自然语言并产生有用的输出。你甚至可以将你写的程序看作一种“自然语言解释器”，就像 Python 解释器处理代码一样。当你的程序能够理解自然语言的表达，它就能据此采取行动，甚至进行回应。

与每个关键词只有唯一含义的编程语言不同，自然语言具有高度的模糊性（fuzziness）。比如这句话：“The chicken is ready to eat.” 它既可能意味着“鸡要吃饭了”，也可能意味着“鸡已经烤好了”。这种模糊性正是自然语言中理解与生成的挑战所在。

一个自然语言处理系统通常被称为“**管道**”（pipeline），因为语言的处理需要分阶段进行。文本从管道的一端进入，经过若干 Python 代码段（“管道段”）处理后，输出的是结构化数据或新的文本。你可以把它想象成一条由 Python 蛇组成的康加舞（conga line），它们一个接一个地传递数据。

这本书将教你如何将简单的文本指令转化为可以与人对话的应用程序。虽然刚开始可能看起来像是魔术，但随着你揭开技术的“魔法”面纱，你会发现自己也能掌握所有实现这些魔术所需的“道具与技巧”。



<h3 id="v8iWd">1.1.1 自然语言理解（NLU）</h3>
自然语言理解（Natural Language Understanding, NLU）是自然语言处理（NLP）的一个子领域，专注于让机器能够“理解”和分析自然语言的语义。NLU 的核心任务之一是将文本自动转化为可以用数字表示的语义表达方式。我们通常把这个过程看作 NLP 中的 “理解” 部分。

这种语义的数字表达通常表现为一行数字，也就是一个**向量**。计算机非常擅长处理向量，可以对它们执行各种数学操作。

在本书中，你将学习如何以不同方式将自然语言表示为向量。这种将文本转换成数值向量的功能，在 `scikit-learn` 包中被称为 **向量化器（vectorizer）** 或 **编码器（encoder）**（参见第 3 章）。在第 3 章和第 4 章中，你将熟悉几种常见的文本向量表示方法，比如**词频向量**（token count vectors）和**词频-逆文档频率向量**（term frequency vectors）。你将学习如何使用这些向量实现关键词搜索、全文搜索，甚至识别社交媒体中的有害言论。

到了第 6 章，你将接触一种更高级的语言向量表示形式，叫做 **嵌入（embedding）**。嵌入让我们可以对词语的“语义”进行数学计算，而不仅仅是统计它们的出现次数。你将了解到，现代搜索引擎是如何借助嵌入理解用户搜索意图，从而返回与查询内容真正相关的网页。第 6 章结束时，你将能够构建一个**混合型搜索引擎**，结合多种文本编码方法的优点。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747456755754-46c7f009-c3eb-4a4e-aa15-7da6aa6fee99.png)

图 1.1 展示了 NLP 管道中 NLU 部分的流程：从原始文本输入开始，最终输出的是该文本的语义向量。

一旦自然语言被转化成数字形式，机器就可以通过各种方式从中提取意义。事实上，许多典型的 NLU 任务，如今已经可以实现较高的准确率，包括：

+ **语义搜索（Semantic search）**
+ **同义句识别（Paraphrase recognition）**
+ **意图分类（Intent classification）**
+ **情感分析（Sentiment analysis）**
+ **主题建模（Topic modeling）**
+ **作者识别（Authorship attribution）**

近年来，随着深度学习的发展，机器已经能够完成很多原本在十年前看似不可能的 NLU 任务，例如：

+ **类比推理（Analogy problem solving）**
+ **阅读理解（Reading comprehension）**
+ **抽取式摘要与问答系统（Extractive summarization and question answering）**

但也仍然存在许多任务是人类明显优于机器的。部分任务需要机器具备常识性知识，并能够理解这些知识之间的逻辑关系，再结合上下文进行推理。这些任务对机器而言难度极高，例如：

+ **委婉语和双关语识别（Euphemism and pun recognition）**
+ **幽默与讽刺检测（Humor and sarcasm recognition）**
+ **仇恨言论和网络喷子检测（Hate speech and troll detection）**
+ **逻辑蕴含与谬误识别（Logical entailment and fallacy recognition）**
+ **知识抽取（Knowledge extraction）**

不过，近年来的自然语言处理技术，尤其是**大型语言模型（LLMs）**，在处理这些复杂任务方面已经有了显著进展，而且仍在不断改进中。

本书将带你了解这些最前沿的 NLU 方法，正是它们让我们能够解决这些曾经被认为几乎无法解决的问题。掌握这些知识之后，你将能够构建功能强大、针对不同应用场景高度优化的 NLU 管道，甚至能够处理最具挑战性的语言理解问题。



<h3 id="c1D2b">1.1.2 自然语言生成（NLG）</h3>
在本书写作十年前，机器能够轻松生成类人类语言的文字，听起来还像是科幻小说里的未来设想。然而如今，距离 ChatGPT 等工具被大众广泛使用不过一年半时间，**机器根据人类的意图和情感生成可读文本**已然成为日常现实，而不再是遥不可及的幻想。这项突破来自自然语言处理中的另一个核心方向：**自然语言生成（Natural Language Generation, NLG）**。

机器生成文本有多种方式，但除非你显式进行字符串拼接（比如将用户的名字插入 `Hello {{name}}!` 模板中），大多数生成过程的中间结果都会采用数字序列的形式表示。要把这些数字转化为可供人类理解的语言，就需要执行与编码相反的操作——**解码（decoding）**，如图 1.2 所示。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747457037745-71d6b197-368b-4ee3-9607-c13e10e1c0f4.png)

你很快就会掌握许多基于 NLU 技术的常见 NLG 任务。这些任务主要依赖你将自然语言编码为语义嵌入向量的能力，包括：

+ **同义词替换**
+ **常见问题回答（信息检索）**
+ **电子邮件与消息中的自动补全**
+ **检索增强生成（Retrieval-Augmented Generation, RAG）**
+ **拼写与语法纠正**

一旦掌握这些打基础的技能，你就可以进一步挑战更高级的自然语言生成任务，例如：

+ **抽象式文本摘要与简化**
+ **基于神经网络的机器翻译**
+ **句子释义（意图保持、表达多样）**
+ **具备心理疏导能力的对话 AI**
+ **面向事实的问题生成**
+ **对话主持与讨论引导**
+ **自动撰写论证型文章**

最后，在第 10 章，你将看到现代大型语言模型（LLMs）如何将生成能力发挥到极致，用于以下这些高度复杂的任务：

+ **参与社交媒体辩论**
+ **自动总结冗长的技术文档**
+ **创作自然流畅的诗歌与歌词**
+ **创作幽默段子与讽刺评论**
+ **根据自然语言描述生成编程语言代码表达**

特别值得注意的是，**NLG 的这一发展正在显著提升机器编程的能力**：机器已经可以根据自然语言描述生成几乎符合用户意图的正确代码。虽然机器尚未具备完全自主编程的能力，但根据 Metaculus（一个集体预测平台）在 2024 年 9 月发布的共识预测，到 **2028 年 9 月**，我们可能将进入一个“AI 编写会编写 AI 的程序”的新时代。<sup>5</sup>

NLU 与 NLG 的结合，正赋予你打造真正能与人类自然互动系统的能力。你可能听说过 Microsoft 与 OpenAI 推出的 Copilot 项目。事实上，开源的 GPT-J 表现也非常接近，并且完全免费、数据公开。<sup>6</sup>



<h3 id="QuqbZ">1.1.3 将一切整合起来：为正向影响而设计的 AI</h3>
一旦你掌握了自然语言理解（NLU）与自然语言生成（NLG）的基本原理，你就能够像一名熟练的水管工那样，将它们组装成属于自己的自然语言处理（NLP）流水线。事实上，如今许多企业已经在使用类似的流水线从用户那里提取价值。

你也可以将这些技术**用于实现你在生活、事业和社会影响方面的目标**。这场技术爆炸如同一枚火箭，你既可以搭乘它，也许还能稍微掌舵引导。你可以用它处理邮箱、日记，并在此过程中保护隐私、提升心理健康；你也可以借助它在职场中脱颖而出，向同事展示拥有“理解与生成语言能力”的机器，如何提高几乎所有信息密集型工作的效率与质量。作为一名关心社会影响的工程师，你还能帮助非营利组织搭建 NLU/NLG 流水线，从而真正帮助有需要的人群。若你是创业者，更可以借此打造具有再生能力的亲社会型企业，催生出一批共生共赢的新产业和新社区。

我们希望，通过理解 NLP 的运作机制，你能**更加清醒地认识到机器是如何在日常生活中被使用**的——很多时候，我们并不知情。这些技术可能会用于挖掘我们的语言以牟利、温和地引导我们走向某种特定结果，甚至训练我们成为更容易被操控的个体。好消息是，只要你了解 NLP 的原理，你就能更好地识别、抵御，甚至**反制 NLP 被恶意使用**的风险。在一个充满操控性算法的世界里，这种能力无比重要。

能够理解并生成自然语言的机器，掌握了“语言的力量”。而由于它们现在可以生成高度自然的文本，在某些场景中，它们甚至可以**代表你在现实世界中采取行动**。很快，你就可能拥有能够自动执行你意愿、完成你设定目标的智能代理。但也要小心落入“阿拉丁三愿陷阱”——这些智能体**可能会引发业务或个人生活上的连锁灾难**。

前面提到的那些强大功能，正是促使一些律师失业<sup>7</sup>、向进食障碍患者提供错误建议<sup>8</sup>，甚至欺骗航空公司客户、造成严重声誉损失<sup>9</sup>的技术根源。这就是所谓的**“AI 控制问题”，**又称**“AI 安全挑战”**。<sup>10</sup>

不过，“控制问题”与 AI 安全，并不是你在构建正向 NLP 系统的唯一挑战。超级智能 AI 可能在几十年后才会出现，但那些“并不那么智能”的 AI 已经**多年以欺骗和操控的方式存在**了。<sup>11</sup> 比如，主导你能看到哪些内容的搜索与推荐引擎，并不是按照你的意愿运行的，而是按照平台投资者的意图——**它们的目标是攫取你的注意力、时间与金钱**。



<h2 id="jHNfG">1.2 自然语言的魔力</h2>
一台能读写自然语言的机器，为什么会显得如此神奇？

自从计算机诞生以来，机器就已经可以处理“语言”了。但那是计算机语言，例如 Ada、Bash 和 C，这些语言是专门为让计算机能够理解而设计的。编程语言尽量避免歧义，以确保计算机始终严格执行你输入的指令——即便那未必是你真正想要的结果。

计算机语言只能以唯一正确的方式被解释（或编译）。而自然语言处理（NLP）的出现，让用户可以用自己的语言与计算机交流，而无需学习“计算机语言”。当软件能够处理本不是为机器理解而设计的语言时，就像魔法一般——这曾经是我们以为只有人类才能完成的能力。

更神奇的是，机器可以访问海量的自然语言文本，比如维基百科，从中学习关于世界和人类思维的知识。Google 构建的自然语言文档索引超过 1 亿 GB，而这还只是索引本身——且并不完整！当前互联网上的自然语言内容实际可能已超过 1,000 亿 GB。正是这些庞大的自然语言文本，使 NLP 成为一个非常实用的工具。

> **注**：今天的维基百科列出了大约 1,000 种编程语言。而自然语言的列表中包含超过 7,000 种自然语言，而且这还不包括许多其他自然语言序列，它们也可以用本书中介绍的技术来处理。例如动物的声音、手势和身体语言，甚至细胞中的 DNA 和 RNA 序列，都可以通过 NLP 技术进行分析处理。
>

目前你只需要关注一种自然语言：英语。稍后在本书中，你会逐步接触更复杂的语言，比如普通话。但你将在学习过程中掌握构建通用语言处理软件的方法——这些软件甚至可以处理你不理解的语言，甚至是那些考古学家和语言学家尚未破译的语言。我们将教你如何用唯一一种编程语言——Python，来完成这些语言的处理与生成。

Python 从一开始就被设计为一种“可读性强”的语言，同时它还对底层的语言处理机制开放。这两个特性使它成为学习 NLP 的绝佳选择。它也是构建可维护的 NLP 算法生产管线（尤其在企业环境中多开发者协作）的理想语言。事实上，我们在可能的情况下，会用 Python 来代替“通用语言”中的数学符号。毕竟，Python 能以无歧义的方式表达数学算法，而且对于你这样的程序员来说也尽可能易读。



<h3 id="X4Wzx">1.2.1 语言与思维</h3>
语言学家和哲学家，如萨丕尔（Sapir）和沃尔夫（Whorf），提出词汇影响我们的思维。例如，许多澳大利亚原住民语言，如Guugu Yimithirr和Kuuk Thaayorre，会使用基于罗盘方位的身体部位词汇来描述物体的位置。墨西哥南部恰帕斯州的原住民所讲的Tzeltal语言，也采用表示罗盘方向的词汇，而非以自我为中心的相对方向。Tzeltal语中甚至使用“上坡”“下坡”或“高度”来描述相对于当前时间事件的位置。这些语言和文化使得相关人群具有比其他文化更强的内部指南针感知能力。讲者不会说某物在右手边，而是说它位于身体的北侧。这种在语言中常用罗盘方位的现象，有助于使用者在执行某些任务时表现更佳。例如，这类语言使用者会不断更新自己在空间中的方向感，从而提升沟通效率和狩猎时的定位能力。

斯蒂芬·平克（Stephen Pinker）则持相反观点，他认为语言是观察我们大脑及思维方式的窗口：“语言是人类的集体创造，反映了人类本性、我们如何概念化现实以及我们如何相互关联。”无论你认为词汇是影响思维的工具，还是帮助我们观察和理解思维的媒介，它们无疑都是思想的载体。你很快将了解到自然语言处理（NLP）在操控这些思想载体中的强大能力，甚至可能提升你对“词”乃至“思维”本身的理解。难怪许多企业将NLP和聊天机器人视为人工智能（AI）。

那么数学呢？人类既可以用精确的数学符号和编程语言思考，也可以用“模糊”的自然语言词汇和符号表达逻辑思维，如数学概念、定理和证明。但思维并非仅限于文字。哈佛大学几何学家乔丹·艾伦伯格（Jordan Ellenberg）在其新书《形状》（Shape，Penguin Press 2021）中讲述了他如何“发现”代数的交换律。当时，他注视着一只带有6x8点阵的立体声音箱。他记住了乘法表和数字符号，也知道乘法符号两侧的顺序可以互换，但直到他意识到这48个点既可以看作6列8个点，也可以看作8行6个点，且数量相同，他才真正理解这一点。这种理解达到了比课堂上学到的符号运算规则更深层次的认知。

你用词汇与他人和自己交流思想。当短暂的思维凝结成词汇或符号时，它们成为压缩的思想载体，更容易被大脑记忆和处理。你或许未曾意识到，构句时你实际上在重新思考、操控和重新包装这些思想。你想表达的观点是在说话或写作过程中被构建出来的。这种在大脑中操控思想载体的过程，AI研究者和神经科学家称之为符号操控（symbol manipulation）。在传统的人工智能（GOFAI）时代，研究者们假设AI需要像编译编程语言一样，学习操控自然语言的符号和逻辑表达。第11章你将学习如何教机器进行自然语言的符号操控。

但这还不是NLP最令人印象深刻的能力。回想你曾经需要给亲近的人发一封难写的邮件，比如向老板、老师、伴侣或好友道歉。你可能在开始敲字前反复思考用词，甚至推敲理由和借口，脑中模拟对方如何理解你的话。你可能多次回顾要说的话，最终动笔时所写的文字远多于发送的内容。你小心选择、舍弃或聚焦某些词句。

修订和编辑的行为本身就是思考过程。它帮助你整理和完善思路。最终输出的文字与最初的想法往往大相径庭。写作提升了你的思考能力，也将随着机器在读写上的进步，提升机器的思考能力。

因此，阅读和写作本身就是一种思维。这些行为压缩了我们的思维，使其更易于记忆和管理。一旦我们找到一个概念的完美词汇，就能将其存档于脑海，无需不断刷新记忆。当再次想到该词时，相关概念便会涌现，我们可以再次运用。

这就是认知（cognition）。尽管机器使用与人类不同的工具来处理和生成文本，当我们看到它们执行这些任务时，便会联想到伴随人类读写的思维过程。这也是人们将自然语言处理（NLP）视作人工智能（AI）的原因。对话式人工智能（conversational AI）则是AI中最广为人知且实用的形式之一。



<h3 id="JQivz">1.2.2 会话机器</h3>
  
虽然你在脑海中反复使用语言进行思考，但真正有趣的是用语言与他人互动。对话行为使两人（或更多人）共同参与思维过程，形成一种强有力的正反馈循环，强化优秀的观点，同时剔除薄弱的想法。

词汇是这一过程的关键，它们构成了我们共享的思维词汇表。当你想激发他人脑中某种想法时，只需说出合适的词汇，使对方理解你的部分思维。例如，当你感到极度痛苦、沮丧或震惊时，使用咒骂词汇往往能传达你的情绪冲击与不适感。虽然我们无法用语言“编程”另一个人，但可以借助词汇交流极其复杂的思想。

自然语言不能直接转化为精确的数学运算，但其中蕴含的信息和指令可以被提取、存储、索引、检索，甚至立即执行。比如，系统可以根据输入语句生成一段回复文字。这正是你将构建的“对话引擎”或聊天机器人的核心功能。

本书专注于英文文本文件和消息的处理，而非口语语句。第7章会简要介绍音频文件处理，以摩尔斯电码为例。但除此之外，我们主要关注已经被记录成文字——或者至少存储在计算机中电子信号形式的词汇。目前已有大量关于语音识别、语音转文本（STT）和文本转语音（TTS）系统的专著。开源项目也提供了成熟的STT和TTS解决方案。若你开发移动应用，现代智能手机软件开发包（SDK）通常内置语音识别与语音合成的接口（API）。若你希望虚拟助手运行在云端，有许多Python库可支持在任何可访问音频流的Linux服务器上实现STT和TTS。

本书聚焦于音频转文字后的处理环节。这将帮助你在开源项目（如Home Assistant或Mycroft AI）基础上，构建更智能的语音助手。同时，你也能理解大型科技公司语音助手背后所用的强大自然语言处理技术——前提是这些商业语音助手愿意为你提供超越“轻松掏钱”之外的真正帮助。



<h3 id="Ft9GE">1.2.3 数学原理</h3>
  
处理自然语言以提取有用信息是一项复杂且繁琐的统计记账工作，但这正是机器的优势所在。像许多其他技术难题一样，一旦你掌握了解决方案，问题便迎刃而解。尽管目前机器在会话理解和阅读理解等大多数实际自然语言处理（NLP）任务上，还无法达到人类的准确性和可靠性，但你可以通过本书学到的算法，稍作调整，提升某些NLP任务的表现。

你将学习的技术足够强大，能够让机器在某些微妙任务上以更高的速度和准确性超越人类。例如，你可能不会想到，机器在识别孤立的推特消息中的讽刺意味上，表现甚至优于人类。经过良好训练的人类评审准确率约为68%，而简单的讽刺检测NLP算法已能达到这一水平。基础的词袋模型（Bag-of-Words, BOW）准确率为63%，而最先进的Transformer模型准确率高达81%。别担心——在人类对话的上下文中识别幽默和讽刺仍是人类的优势，因为我们能持续跟踪语境；不过机器在上下文维护方面正日益进步。本书将指导你如何将上下文（元数据）融入NLP流程，助你推动技术前沿。

一旦从自然语言中提取出结构化的数值数据或向量，你就可以利用数学和机器学习的所有工具。我们采用与3D物体投影到二维屏幕相同的线性代数技巧，这是计算机和绘图师早在NLP兴起前就已运用的技术。这些突破性的思想开启了“语义分析”的新纪元，使计算机能够理解和存储语句的“含义”，而不仅仅是词频或字符数。语义分析结合统计学，有助于解决自然语言的歧义问题——即词语和短语常常存在多重含义或解释。

因此，信息提取与编写编程语言编译器完全不同（这对你来说是幸运的）。当前最有效的方法绕过了正规语法（规则模式）或形式语言的严格规则。你可以依赖词与词之间的统计关系，而非复杂的逻辑规则体系。试想如果你必须用嵌套的if-then语句定义英语语法和拼写规则，能否写出足够覆盖所有单词、字母和标点组合的规则？又如何捕捉英语语句的语义？即便某些语句适用，软件也会极其有限且脆弱，遇到未预料的拼写或标点就会崩溃或出错。

自然语言还有更难解决的“解码”挑战。说话者和写作者假设接收者是人类，而非机器。例如，当我说“good morning”（早上好）时，我默认你知道“morning”指的是上午，且上午依次在中午、下午、晚上和午夜之前。你需要理解“morning”既能代表一天中的时间段，也能泛指一段时间。你还需知道“good morning”是常用的问候语，并非真正传递关于早晨的详细信息，而是反映说话者的心情和交流意愿。

这种关于语言接收者“**心智理论”（theory of mind）**的假设非常强大。它让我们用很少的词表达大量信息，前提是假设“处理者”拥有丰富的常识知识。然而，这种高度的信息压缩对于机器来说仍不可及。现有的NLP流程中尚无明确的“心智理论”模型。但本书后续章节会介绍如何帮助机器构建**本体论（ontologies）**或知识库，以整合常识知识，从而更好地理解依赖于此类知识的语句。



<h2 id="voEpi">1.3 应用领域</h2>


自然语言处理（NLP）无处不在，以至于你几乎每小时都会与多个NLP算法互动。图1.3中的一些例子可能会让你感到惊讶。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747458665110-c3d96b60-0d81-4840-97a0-3e0eb3804fa8.png)

在这张网络图的核心是NLP的自然语言理解（NLU）和自然语言生成（NLG）两大部分。从NLU的枢纽节点出发，有基础应用，如情感分析和搜索。这些基础应用最终会与NLG工具（如拼写纠正器和自动代码生成器）结合，进而构建对话式人工智能，甚至辅助程序员进行配对编程。

搜索引擎通过理解自然语言文本的含义来索引网页或文档存档，能提供更精准的搜索结果。自动补全功能则广泛应用于搜索引擎和手机键盘，帮助用户快速完成输入。许多文字处理器、浏览器插件和文本编辑器都内置了拼写纠正、语法检查、词频分析以及风格指导等功能。一些对话引擎（聊天机器人）则利用自然语言搜索，寻找与对话伙伴消息相关的回复。

生成文本的NLP管线不仅能用于聊天机器人和虚拟助手中回复短消息，还能撰写较长篇幅的文章。例如，美联社使用NLP“机器人记者”自动撰写财经新闻和体育赛事报道。天气预报机器人生成的播报内容听起来与本地气象员类似，部分原因是人类气象员也会借助内置NLP功能的文字处理软件起草稿件。

越来越多企业开始用NLP自动化业务流程，这不仅提升了团队效率和工作满意度，也提高了产品质量。举例来说，聊天机器人能自动回复大量客户服务请求。早期邮件程序中的垃圾邮件过滤器利用NLP技术帮助电子邮件超越电话和传真，成为主流通信渠道。现在一些团队也使用NLP自动化个性化团队内部邮件或与求职者的沟通。

需要注意的是，NLP算法和所有算法一样，会犯错且常带有各种偏见。因此，使用NLP自动化人与人之间的沟通时要格外谨慎。作者在Tangible AI任CTO时，就将NLP用于招聘开发者的关键业务流程中，严格监管管线。NLP只在候选人未响应或回答与问题无关时，才自动筛选申请；同时通过定期抽样检验模型预测，采用简洁且样本高效的模型，将人力集中在机器信心最低的预测上。你将在第2章学习如何用scikit-learn的predict_proba方法实现类似机制。虽然NLP在招聘上并未节省成本，却帮助团队更广泛地招募全球候选人，快速筛选英语和技术能力。

邮件垃圾过滤器在猫鼠游戏中保持优势，但在社交网络环境下可能逐渐失利。2016年美国总统选举期间，估计约20%的推文由聊天机器人发布，这些机器人放大了其拥有者或开发者（如外国政府、大企业）的观点，影响公众舆论。

NLP系统生成的不只是短社交媒体内容，还能撰写电影和产品评论。许多评论由从未看过电影或购买过产品的自动NLP系统生成。事实上，排名靠前且展示在电商平台的许多产品评论是假冒的。NLP能辅助搜索引擎和公益社交社区检测并清理误导性或虚假帖子和评论。

Slack、IRC甚至客服网站都部署了聊天机器人，需处理模糊指令或提问。结合语音识别与生成，聊天机器人甚至能进行无固定目标的长对话，比如帮你预约本地餐厅。部分公司用NLP系统接听电话，代替传统的电话菜单树，减少人力成本。

:::danger
**警示：**  
任何时候，当你或你的上司决定用NLP欺骗用户，都要认真考虑伦理影响。谷歌I/O上的Duplex演示就引发了关于让聊天机器人欺骗人类伦理问题的争议。在多数娱乐社交网络上，机器人无需公开身份，我们经常无意识地与这些机器人互动。如今，机器人和深度伪造技术能极具欺骗性地迷惑我们，AI控制问题已被构建伦理AI的挑战所取代。尤瓦尔·赫拉利（Yuval Harari）在《未来简史》（Homo Deus）中警告的机器人能使人类决策短路的预测正逐渐成真。

:::

NLP系统还能充当企业的邮件“接待员”或管理者的执行助理，安排会议、记录摘要信息至电子名片或客户关系管理系统，并代表老板与他人通过邮件沟通。许多公司已将品牌形象托付给NLP系统，让机器人执行市场营销和信息传播任务。甚至有些NLP教科书作者大胆让机器人帮忙撰写几句话——后文会详细介绍。

NLP在心理学领域的应用也极具潜力。历史上首个对话系统ELIZA使用简单的模式匹配方法，竟能产生出令人惊讶的人类对话感。此后，类似治疗师的聊天机器人取得显著进步。中国的小冰，以及美国的Replika.AI和Woebot等商业虚拟伴侣，帮助数亿孤独者度过2020和2021年疫情封锁期间的心理挑战。幸运的是，你无需依赖大公司的工程师，许多心理治疗和认知辅助工具都是免费且开源的。



<h3 id="rCWHk">1.3.1 使用NLP处理编程语言</h3>
现代深度学习的NLP管线展现出强大且多样的能力，已经能够准确理解和生成编程语言。基于规则的编译器和生成器在简单任务中表现良好，比如自动补全和代码片段推荐。此外，开发者还常通过信息检索系统或搜索引擎寻找代码片段，以完成软件开发项目。

这些工具变得更加智能。早期的代码生成工具属于提取式（extractive）方法，即从历史文本中找到最相关的内容并原封不动地重复输出，作为建议。例如，如果算法训练文本中“prosocial artificial intelligence”（亲社会人工智能）频繁出现，自动补全可能推荐“artificial intelligence”而非简单的“intelligence”。这不仅影响你输入的内容，还潜移默化地影响你的思维方式。

而最近，基于transformer的大型深度学习网络带来了更抽象的生成能力，可以创造出你之前未见过的新文本。例如，拥有1750亿参数的GPT-3模型被用来训练一个名为Codex的模型。Codex集成在Visual Studio Code（VS Code）的Copilot插件中，能根据简短的注释和函数定义的第一行，自动生成完整的函数和类定义。

```typescript
// Determine whether the sentiment of text is positive
// Use a web service
async function isPositive(text: string): Promise<boolean> {
```

在Copilot主页的演示动画中，系统生成了一个用于估计文本情感的TypeScript函数的完整代码。想象一下，一个算法正在帮你写代码，用于分析自然语言文本（如电子邮件或个人文章）的情感倾向。值得注意的是，演示中所有例子都偏向微软的产品和服务，这意味着你最终得到的NLP管线会带有微软对“积极”和“消极”的价值判断。换句话说，它反映了微软对价值的定义。类似于谷歌间接影响了你写代码的方式，现在微软的算法直接在帮你写代码。

既然你正在阅读这本书，很可能计划构建一些非常酷的NLP管线。你可能会开发帮助写博客文章、聊天机器人的管线，甚至参与开源数据集和算法的贡献。你可以创造一个正向反馈循环，影响像你这样的工程师开发和部署的NLP模型和管线方向。

所以，请关注你用来辅助编程和思考的“元工具”（meta tools），它们对你的代码风格和人生轨迹都有深远影响。

<h2 id="KBPGy">1.4 用计算机的“眼睛”看语言</h2>
当你输入“Good morning Rosa”这样的句子时，计算机看到的只是一串像“01000111 01101111 01101111 …”这样的二进制数据。那么，我们该如何编程让聊天机器人能够智能地理解并回应这样的信息呢？是否可以通过嵌套的条件判断（if-else语句）来逐位检查这些比特并采取相应操作？这实际上就是一种特殊程序——**有限状态机（Finite State Machine, FSM）**。

如果这个FSM在运行过程中还能输出新的符号序列，比如 Python 的 `str.translate` 函数的行为，那它就被称为**有限状态变换器（Finite State Transducer, FST）**。实际上，你很可能已经在不知不觉中构建过FSM，比如编写正则表达式的时候。正则表达式正是我们将在下一节中探讨的NLP方法之一：**基于模式的NLP方法**。

那是否可以通过查找已有语料库中完全匹配的字符串，来返回历史中人类对这些语句的常见回应呢？

可以，但这种方法过于脆弱：一旦输入有拼写错误或词序稍有不同，匹配就会失败。这是因为比特流是**离散的、不可宽容的**——两个字符串要么匹配，要么完全不匹配，根本没有“语义相似度”的概念。因此，从“good”到“bad”或“OK”的比特流之间，看不出任何内在联系。

但在我们介绍更优雅的解决方案前，还是先来看一个简单的实现：用正则表达式识别问候语并做出回应——这将是我们打造的**第一个迷你聊天机器人！**



<h3 id="CWaLs">1.4.1 锁的语言</h3>
你可能没有意识到，其实**普通的密码锁**就是一个简单的语言处理系统，它有自己的“语言”规则。

想象你在和一把锁对话。只要你“说”出符合其语言语法（也就是正确密码）的内容，锁就能“理解”这句话的意义——唯一的反应就是**打开锁扣**。这就像聊天机器人识别某种关键词然后触发某种行为。

锁的语言使用的是正则表达式，这使得它成为一种特别简单的语言——但这并不意味着它过于简单，无法用于聊天机器人！我们可以用它来识别某个关键词或命令，从而解锁某种行为或动作。想象你正在构建一个聊天机器人，希望它能够识别并回应问候语（例如，“Hello, Rosa”）。这种语言，就像锁的语言一样，是一种形式语言，意味着它对一条可接受语句的组成和解释有严格的规则。如果你曾经写过数学公式，或者编写过编程语言中的表达式，那你已经写过形式语言的语句了。

形式语言是自然语言的一个子集。许多自然语言中的语句可以通过形式语言的语法（比如正则表达式或正规文法）来匹配或生成。而这正是我们转向机械化（咔哒、嗡嗡）锁之语言的原因。



<h3 id="u4Jrt">1.4.2 正则表达式</h3>
正则表达式使用的是一类特殊的形式语言文法，称为**正规文法**。正规文法具有可预测、可证明的行为特性，但它们的灵活性又足以支撑市面上一些最复杂的对话引擎和聊天机器人。Amazon Alexa 和 Google Now 主要是基于模式的引擎，它们依赖于正规文法。深度且复杂的正规文法规则通常可以通过一行代码来表达，这种代码称为**正则表达式**。在 Python 中有一些成功的聊天机器人框架，比如 Will<sup>46</sup> 和 qary<sup>47</sup>，它们完全依赖这种语言处理方式，打造出了一些非常高效的聊天机器人。

> 注意：Python 和 POSIX（Unix）应用程序（如 grep）中实现的正则表达式，并不是真正意义上的正规文法。它们拥有一些语言和逻辑功能，比如前瞻（lookahead）和回顾（lookback），这些功能包含了正规文法中不允许的逻辑跳跃和递归。因此，正则表达式并不是可证明会终止的算法；有时候，它们可能会崩溃，或者永远运行下去。<sup>48</sup>
>

你可能会想，“我听说过正则表达式。我用过 `grep`，但那只是用来搜索而已！”你说得没错。正则表达式确实主要用于搜索、用于序列匹配，但任何可以在文本中找到匹配项的东西，也可以用于开展对话。一些聊天机器人会使用搜索来识别用户陈述中它们知道该如何回应的字符序列。这些被识别出来的序列会触发一个针对特定正则表达式匹配的脚本化回应，而同样的正则表达式也可以用来从陈述中提取有用的信息。聊天机器人可以将这些信息添加到它对用户或用户所描述世界的知识库中。

处理这种语言的机器可以被看作是一个**形式数学对象**，即 FSM，或者叫**确定性有限自动机（DFA）**。本书中会一再提到 FSM，因此你最终会熟悉它们的用途，而不必深入探讨 FSM 的理论与数学内容。图 1.4 展示了用于编程有限状态自动机的形式语言是如何像**乌克兰套娃**一样被层层嵌套的。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747459772953-db7b4add-186b-46f9-9c08-368b2255f998.png)

组合逻辑是这个维恩图中最小、最简单的语言核心。有限状态机包含组合逻辑，而下推自动机是两者的超集。**图灵机**能够实现图中所有其他自动机的行为。

:::warning
**形式语言**  
Kyle Gorman 对编程语言和形式语言的描述如下：

+ 大多数（如果不是全部的话）编程语言都属于**上下文无关语言**这一类。
+ 上下文无关语言可以用**上下文无关文法**解析，这种文法具有高效的解析能力。
+ **正规语言**同样可以高效解析，并在计算中被广泛用于**字符串匹配**。
+ 字符串匹配的应用很少需要上下文无关文法所具有的表达能力。
+ 以下是一些**形式语言类别**，按复杂度递减排列，最复杂的是**递归可枚举文法**：  
– 递归可枚举（Recursively enumerable）  
– 上下文相关（Context-sensitive）  
– 上下文无关（Context-free）  
– 正规（Regular） 

<sub>a 参见维基百科“Chomsky Hierarchy”（乔姆斯基层级）页面：</sub>[<sub>https://en.wikipedia.org/wiki/Chomsky_hierarchy</sub>](https://en.wikipedia.org/wiki/Chomsky_hierarchy)

:::

:::warning
**自然语言**  
自然语言与形式编程语言有很大不同，主要体现在它**不具备以下特征**：

+ **不是正规语言**<sup>a</sup>
+ **不是上下文无关语言**<sup>b</sup>
+ **不能被任何形式文法所定义**<sup>c</sup>

<sub>a  Shuly Wintner, “English Is Not a Regular Language” (http://cs.haifa.ac.il/~shuly/teaching/08/nlp/complexity.pdf#page=20). </sub>

<sub>b  Shuly Wintner, “Is English Context-Free?” (http://cs.haifa.ac.il/~shuly/teaching/08/nlp/ complexity.pdf#page=24). </sub>

<sub>c “Foundations of Python Programming” (https://runestone.academy/ns/books/published/fopp/GeneralIntro/FormalandNaturalLanguages.html).  </sub>

:::

即使形式编程语言无法直接实现非常复杂的自然语言处理流程，它们仍然是任何 NLP 流程中的关键组成部分。毕竟，你将在本书中使用形式编程语言 Python 来创建所有的 NLP 流程。有时，只需几个 if 语句和一些 Python 字符串处理功能，你就能完成所需任务。这意味着你可以构建一个不依赖任何机器学习或非确定性处理的简单聊天机器人。



<h2 id="wFOlZ">**1.5 构建一个简单的聊天机器人**</h2>
  
现在我们来快速搭建一个简单粗糙（quick and dirty）的聊天机器人。它的功能不会很强大，而且需要你花些心思思考英语语言本身。你还得手动编写正则表达式，以匹配用户表达某个意思的各种方式。但不用担心，如果你觉得自己无法独立写出这些 Python 代码也没关系。你也不需要像我们这样考虑人们说话的所有方式，更不用手写正则表达式才能构建出一个厉害的聊天机器人。我们将在后续章节中带你一步步实现一个可以通过**阅读（处理）大量英文文本自主学习**的聊天机器人——无需任何硬编码。

这个基于模式匹配的聊天机器人，是一种**高度受控的聊天机器人**。在现代机器学习聊天技术出现之前，这类基于模式匹配的聊天机器人是很常见的。我们在此介绍的模式匹配方法，也有变体被应用在 Amazon Alexa 及其他虚拟助手中。

我们现在要构建的是一个 FSM（有限状态机），通过一个能“说”正规语言的正则表达式来实现。我们希望它能理解**问候语**——像是 “Open sesame” 或 “Hello, Rosa” 这样的短语。能够识别问候，是一个具有亲社会性（prosocial）的聊天机器人应具备的重要功能。在高中时，老师可能会批评学生在赶去上课时无视他人问候，这被视为不礼貌。而我们当然不希望自己的机器人也如此失礼。

如果是两台机器之间的通信，通常会通过某种类似 ACK（确认）信号的“握手”协议来确认消息的接收。但我们的机器是要与人类互动，人们更可能说的是 “Good morning”。我们可不想让它发出一堆哔哔声或 ACK 消息，就像是某个调制解调器或 HTTP 连接在会话或网页加载前同步那样。

人类的问候和“握手”要非正式得多，也更具灵活性。因此，仅仅构建一个类似机器握手的机制并不足以识别问候意图。你需要准备好**多种方法**放进工具箱中，以应对这一任务。

:::danger
**注意**：**意图（intent）**是指用户希望 NLP 系统或聊天机器人在不同上下文中执行的某类目标。例如，像 hello 和 hi 这样的词可以归类为问候意图（greeting intent），这样聊天机器人就知道在用户想要开始对话时该如何回应。另一种意图可能是执行某个任务或指令，比如回答 “How do I say ‘Hello’ in Ukrainian?” 这样的查询，或响应翻译命令。你将在本书后续章节学习意图识别的相关内容，并在第 12 章将其运用到聊天机器人中。

:::



<h3 id="uiW4J">**1.5.1 基于关键词的问候识别器**</h3>
你的第一个聊天机器人将让人想起 1980 年代的风格。如果你看过 1983 年的科幻经典电影《战争游戏》（_WarGames_），你也许还记得 Joshua —— 一个运行在 WOPR 计算机上的 AI 聊天机器人，由 Steven Falken 教授编写。想象一下你希望一个聊天机器人帮助你选择一个游戏，比如国际象棋……或者热核战争。这种方法也可以扩展，用于实现简单的基于关键词的意图识别器，正如下面的代码清单所展示的那样，适用于本章前面提到的类似项目。

```python
>>> greetings = "Hi Hello Greetings".split()
>>> user_statement = "Hello Joshua"
>>> user_token_sequence = user_statement.split()
>>> user_token_sequence
['Hello', 'Joshua']
>>> if user_token_sequence[0] in greetings:
... bot_reply = "Thermonuclear War is a strange game. "
... bot_reply += "The only winning move is NOT TO PLAY."
>>> else:
... bot_reply = "Would you like to play a nice game of chess?"
>>> bot_reply
'Thermonuclear War is a strange game. The only winning move is NOT TO PLAY.'
```

这个简单的 NLP 流水线（程序）使用了一种非常简单的算法，叫做**关键词检测**，它只有两个意图类别：**greeting（问候）****和****unknown（未知）**（即其他）。能够像这样识别用户意图的聊天机器人，其功能大致相当于现代的命令行应用程序，或者是 90 年代电话语音导航系统。

基于规则的聊天机器人可以比这个简单程序更有趣、更灵活。开发者在构建和使用聊天机器人的过程中常常乐在其中，以至于他们甚至会构建聊天机器人来协助部署和监控服务器，从而让这些原本无趣的任务变得有趣。这就是 ChatOps（基于聊天机器人的 DevOps）在大多数软件开发团队中流行起来的原因之一。

你可以通过在 `else` 语句前添加更多的 `elif` 条件语句来让聊天机器人识别更多的意图。或者，也可以超越关键词式的 NLP，开始思考如何使用正则表达式来提升系统能力。



<h3 id="YfyIg">**1.5.2 基于模式的意图识别**</h3>
一个基于关键词的聊天机器人可以识别诸如 Hi、Hello 和 Greetings 这样的词，但却无法识别 Hiiii 或 Hiiiiiiiiiiii——这些是 Hi 的更激动的表达方式。也许你可以将前 200 种 Hi 的变体硬编码进程序，比如 ["Hi", "Hii", "Hiii", …]，或者通过编程的方式生成这样一个关键词列表。但其实有一个更好的方法，可以让你的机器人识别无限种 Hi 的变体：**使用正则表达式**。

正则表达式模式比任何硬编码规则或关键词列表都能更可靠地匹配文本。它们可以识别任何符号或标记（token）序列的模式。甚至可以用来匹配符号序列和其他字符，比如单词、词性标签，甚至是 n-gram（连续的几个词）。

无论是使用正则表达式还是关键词匹配器，你都需要预测用户可能使用的所有单词，以及他们如何拼写和书写大小写。所以，如果你的模式匹配器中没有包含 Hey 或 hi 这样的字符串，它就可能无法识别这些问候语。而如果用户使用了带有标点符号的问候，例如 ’sup 或 Hi,，那么你可能会错过它们。

在这种情况下，你可以通过 `str.lower()` 方法对你的问候词和用户语句做**大小写折叠**（case folding），也可以向你的问候语列表中添加更多的词。你甚至可以加入拼写错误和打字错误，以确保不会漏掉用户的输入。但这就意味着要在 NLP 流水线中做大量的**手动数据硬编码**，而且每当你不得不折叠大小写、处理标记（token），或手动定制文本的预处理流程时，你实际上就是在改变这些文本的含义，从而**破坏了 NLP 可能需要的某些信息**。

例如，大小写信息可以帮助 NLP 流水线识别专有名词或人名。如果你把 John 全部转为小写，NLP 流水线可能会错误地把 _john_ 理解为“厕所”这个俚语词。

**机器学习承诺让你的文本数据“自己说话”**，它更多依赖于你所处理文本中的统计信息，而不是你自己猜测哪些模式是你的 NLP 流水线需要去匹配的。令人惊奇的是，机器学习不仅可以识别单词拼写的模式，还可以识别其语义（meaning）的模式。

一旦你学会了在第 3–6 章中使用机器学习方法来处理 NLP，你会发现设计和评估 NLP 流水线的许多繁重工作都可以自动完成。而当你进入第 7 章及后续内容，学习更复杂、更精确的深度学习模型时，你会发现现代 NLP 流水线中的一些部分仍然可能非常脆弱。你将学会如何聪明地构建用于训练深度学习 NLP 模型的数据集，从而让系统变得更加健壮。

不过现在，你可以从基础开始。当用户希望用类似编程语言命令的精确字符模式来指定操作时，**正则表达式就大显身手了**。

```python
import re

# | 代表或，* 表示前面的字符可以出现0次或多次
r = "(hi|hello|hey)[ ,:.!]*([a-z]*)"

# re.IGNORECASE 表示不区分大小写
re.match(r, 'Hello Rosa', flags=re.IGNORECASE)
<re.Match object; span=(0, 10), match='Hello Rosa'>

re.match(r, "hi ho, hi ho, it's off to work ...", flags=re.IGNORECASE)
<re.Match object; span=(0, 5), match='hi ho'>

re.match(r, "hey, what's up", flags=re.IGNORECASE)
<re.Match object; span=(0, 9), match='hey, what'>
```

在**正则表达式（regular expressions）****中，你可以使用方括号来指定一个****字符类（character class）**，并用连字符（`-`）表示一个字符范围，无需逐个输入所有字符。因此，正则表达式 `[a-z]` 会匹配任意一个小写字母，从 `a` 到 `z`。如果在字符类后加上星号（`*`），则表示正则表达式将匹配任意数量的连续字符，只要这些字符都属于该字符类。

我们可以让正则表达式更详细一些，以便匹配更多的问候语：

```python
>>> r = r"[^a-z]*([y]o|[h']?ello|ok|hey|(good[ ])(morn[gin']{0,3}|"
>>> r += r"afternoon|even[gin']{0,3}))[\s,;:]{1,3}([a-z]{1,20})"

# 你可以编译正则匹配语句，这样你就不用每次使用时都要指定flags选项
>>> re_greeting = re.compile(r, flags=re.IGNORECASE)
>>> re_greeting.match('Hello Rosa')
<re.Match object; span=(0, 10), match='Hello Rosa'>
>>> re_greeting.match('Hello Rosa').groups()
('Hello', None, None, 'Rosa')
>>> re_greeting.match("Good morning Rosa")
<re.Match object; span=(0, 17), match="Good morning Rosa">

# 注意这条正则匹配语句无法匹配笔误单词
>>> re_greeting.match("Good Manning Rosa")

# 我们的聊天机器人可以将问候语的不同部分分组，但它不会识别罗莎（Rosa）著名的姓氏，
# 因为我们没有匹配名字后面任何字符的模式。
>>> re_greeting.match('Good evening Rosa Parks').groups()
('Good evening', 'Good ', 'evening', 'Rosa')
>>> re_greeting.match("Good Morn'n Rosa")
<re.Match object; span=(0, 16), match="Good Morn'n Rosa">
>>> re_greeting.match("yo Rosa")
<re.Match object; span=(0, 7), match='yo Rosa'>
```



:::warning
**提示**：在字符串前加上 `r`（如 `r'...'`）表示这是一个**原始字符串（raw string）**。在 Python 中，原始字符串使我们更方便地书写正则表达式中使用的反斜杠（`\`），用于转义特殊符号。如果告诉 Python 这是一个原始字符串，它就不会处理这些反斜杠，而是直接将它们传递给正则表达式解析器（`re` 模块）。否则，你就需要为每个反斜杠都加上转义符（即两个反斜杠 `\\`）。比如，表示空白符的 `\s` 就要写成 `'\\s'`，而表示字面上的花括号也要写成 `'\\{'` 和 `'\\}'`。

:::

代码中的第一行——那个正则表达式——其实蕴含了很多逻辑，它在处理各种问候语时相当有效。但它错过了那个 "Manning" 的拼写错误，这也正是自然语言处理（**NLP**）困难的原因之一。在机器学习和医学诊断测试中，这种情况被称为**假阴性分类错误（false negative classification error）**。不幸的是，它也可能匹配一些人类根本不会说的话，这就是**假阳性（false positive）**，同样是一个问题。

既出现假阳性，又出现假阴性，说明我们的正则表达式既**过于宽松（liberal / inclusive）**，又**过于严格（strict / exclusive）**。这些错误可能会让我们的聊天机器人听起来有些呆板、机械。为了让机器人更智能地运作，我们还需要花很多时间来微调它所能识别的短语。

即使花费大量精力，这种繁琐的工作也很难囊括人类语言中常见的俚语和拼写错误。幸运的是，手动编写正则表达式并不是训练聊天机器人的唯一方式——关于这一点，我们将在后续章节中深入讨论。所以，我们只在需要对聊天机器人的行为进行**精确控制（precise control）**时使用它，比如在手机上向语音助手发出命令时。

不过，现在让我们继续完成这个功能单一的聊天机器人，为它添加一个输出模块。它需要有话可说。我们使用 Python 的字符串格式化功能来为响应创建一个模板：

```python
>>> my_names = set(['rosa', 'rose', 'chatty', 'chatbot', 'bot',
... 'chatterbot'])
>>> curt_names = set(['hal', 'you', 'u'])

# 我们尚未知道谁正在与机器人（bot）进行聊天（chatting），在此我们也不会对此进行关注。
>>> greeter_name = ''
>>> match = re_greeting.match(input())
...
>>> if match:
... at_name = match.groups()[-1]
... if at_name in curt_names:
... print("Good one.")
... elif at_name.lower() in my_names:
... print("Hi {}, How are you?".format(greeter_name))
```

如果你运行这个小脚本，并用像 “Hello Rosa” 这样的短语与机器人打招呼，它会回应一句关心你一天如何的话。如果你用略带不礼貌的方式称呼机器人，它的回应会变得较为冷淡，但不会具有攻击性，目的是鼓励用户保持礼貌。如果你提到的是某个可能监听对话的人，比如聊天室中的其他参与者，机器人将保持沉默，允许你与那个人继续交流。当然，现实中并没有人监听我们的 `input()` 行，但在某些更真实的使用场景中（比如大型聊天机器人），你就需要考虑到这些情况。

由于早期计算资源有限，早期的 NLP 研究者必须依靠人脑的计算能力，来设计并手动微调一系列复杂的逻辑规则，从自然语言字符串中提取信息。这种方式被称为**基于模式的方法（pattern-based approach）**。这些模式不仅限于字符序列的匹配，比如正则表达式；NLP 中还经常涉及**词序列（word sequences）的模式**、**词性（parts of speech）**等更高层级的语言结构。NLP 的核心组件，比如词干提取器（**stemmers**）、分词器（**tokenizers**），以及像 ELIZA 那样的复杂端到端对话引擎（**dialog engines / chatbots**），最初就是用正则表达式和模式匹配构建出来的。基于模式的方法的艺术在于发现恰到好处的模式，用尽可能少的正则表达式代码，准确地提取你所需的信息。

:::warning
**提示**：这种经典的 NLP 模式匹配方法基于**心智计算理论（computational theory of mind, CTM）**。CTM 假设“思维”是一种确定性的计算过程，在一个单一、逻辑化的线程中运行。随着神经科学和 NLP 的发展，21 世纪初，人们提出了一种新的理论，叫做**联结主义心智理论（connectionist theory of mind）**。这一理论启发了**深度学习（deep learning）**中对**人工神经网络（artificial neural networks）**的应用，这些网络能以并行、多样的方式处理自然语言序列。

:::

在第 2 章中，你将进一步学习如何使用基于模式的方法来进行**分词（tokenizing）**，即使用算法（如 Treebank 分词器）将文本切分为词语（token）。你还将学习如何通过模式匹配来进行**词干提取（stemming）**，比如使用 **Porter 词干提取器（Porter stemmer）** 来缩短和归一化词形。而在之后的章节中，我们将利用指数级增长的计算资源以及更大的数据集，来避免这种繁重的手工编程和微调过程。

如果你是正则表达式的新手，并希望深入了解，可以查阅附录 B 或 Python 正则表达式的在线文档——不过你现在还不必完全掌握它们。我们会持续为你提供正则表达式的示例，因为它们构成了我们 NLP 流水线的基础构件。所以即便你觉得它们一开始像是天书也不必担心。人类大脑具有从多个实例中归纳出一般规则的能力，我们相信你在读完本书后自然会理解。而事实证明，机器也能以类似的方式进行学习。



<h3 id="Hqah2">1.5.3 识别问候语的另一种方法</h3>
设想你能够访问一个庞大的人类对话会话数据库--其中包含成千上万甚至数百万次对话中语句与对应回复的配对。在这种情况下，你完全可以通过复制用户输入，并在数据库中搜索完全相同的字符序列来构建一个聊天机器人（chatbot）。然后，你可以简单地复用其他人过去对该句子或短语使用过的回复。这将形成一种统计的、基于数据驱动（data-driven）的方法来设计聊天机器人，取代非常繁琐的模式匹配（pattern-matching）算法设计。

想象一下，哪怕是一个拼写错误或语句中的细微变体，也会让模式匹配机器人 、 甚至是拥有数百万条语句（utterances）数据库的数据驱动机器人陷入困境。比特（bit）和字符序列是离散且非常精确的——它们要么匹配，要么不匹配。而人类是富有创造力的。虽然有时看起来不是这样，但人们经常会说出使用前所未见的新字符模式的句子。因此，你希望你的机器人能够衡量字符序列之间的语义差异。在后续章节中，你将逐步掌握从文本中提取语义的能力！

当我们使用字符序列匹配来衡量自然语言短语之间的距离时，往往会出现错误。具有相似含义的词汇和短语，比如 good（好）和 okay（还行），其字符序列往往不同，按字符逐个匹配计算距离时，距离值可能很大。而有时两个词看起来几乎相同，但含义却完全不同，比如 bad（坏）和 bag（袋子）。你可以使用 Jaccard 和 Levenshtein 等算法计算一个词变成另一个词所需的字符变化数量。但这些距离或“变化”计数未能捕捉两个不同字符序列之间关系的本质，比如 good 和 okay 之间的语义关系；同样也未能区分小的拼写差异是拼写错误还是完全不同的词汇，如 bad 和 bag。

为数值序列和向量设计的距离度量在某些自然语言处理（NLP）应用中非常有用，比如拼写纠正器和专有名词识别，因此在合适的场景下我们会使用这些距离度量。但对于更关注自然语言语义而非拼写的 NLP 应用，有更优的方法。在这些情况下，我们使用自然语言词汇和文本的向量表示，以及针对这些向量设计的距离度量。我们将在介绍不同应用及其对应向量类型时，逐一展示这些方法。

我们不会长时间停留在这个令人困惑的二元逻辑世界中，但不妨假设我们是著名的二战时代密码破译者梅维斯·贝蒂（Mavis Batey），身处布莱切利园（Bletchley Park），刚刚接手了一条二进制的摩尔斯电码（Morse code）消息，这条消息是从两名德国军官的通信中截获的。它可能是赢得战争的关键，那么我们该从哪里开始呢？第一步是对这串比特流做一些统计分析，看看是否能发现模式。我们可以先用摩尔斯电码表（或在我们的例子中用 ASCII 表）将每组比特转换成字母。然后，如果这些字符对我们来说像乱码一样（就像二战时的计算机或密码学家看到的一样），我们可以开始统计它们，查找短序列在我们之前见过的所有词汇字典中的出现次数，并在每次出现时做个标记。我们还可能在其他日志中标记该词出现在哪条消息中，从而创建一个百科全书式的文档索引。这个文档集合称为语料库（corpus），而我们在索引中列出的词汇或序列称为词典（lexicon）。

如果幸运的话--我们并不处于战争状态，且所查看的消息没有被强加密——我们会看到这些德语词频统计与用于传达类似信息的英语词频统计有相似的模式。与试图破译德国摩尔斯电码截获消息的密码学家不同，我们知道这些符号具有一致的含义，并不会随着每次按键而改变以试图迷惑我们。这种繁琐的字符和词汇计数正是计算机无需思考即可完成的任务。令人惊讶的是，这几乎足以让机器看似理解我们的语言。它甚至可以对这些统计向量进行数学运算，这些运算与我们人类对短语和词汇的理解相吻合。当我们在后续章节中向你展示如何使用 Word2Vec 教机器学习我们的语言时，可能会觉得这很神奇，但其实不过是数学——计算。

但让我们稍作思考，在统计消息中的所有词汇时，我们丢失了哪些信息？我们将词汇分配到“箱子”中，并将它们存储为位向量，就像硬币或代币分类机（见图1.5）一样，通过一系列决策将不同类型的代币引导到不同方向，最终堆积到底部的箱子里。我们的分类机必须考虑数十万甚至数百万种可能的代币“面额”，对应说话者或作者可能使用的每个词汇。我们输入的每个短语、句子或文档都会在底部输出一个“向量”，其中每个槽位记录对应代币的计数。即使是词汇丰富的大型文档，大多数计数也为零。但我们还没有丢失任何词汇。那么，我们到底丢失了什么？作为人类，如果以这种方式呈现给你一篇文档——仅仅是你语言中每个可能词汇的计数，而没有任何词序或顺序信息——你能理解这篇文档吗？我们对此表示怀疑。但如果是一句简短的句子或推文，你大概率能够将这些词汇重新排列成其原本的顺序和含义。

图1.5展示了一个加拿大硬币（或代币）分类机。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747462683713-e656fcd2-90ba-44dd-9439-20d287561365.png)



我们的代币分类器（token sorter）在自然语言处理（NLP）流水线中紧跟分词器（tokenizer）（见第2章）。在图1.6所示的机械代币分类器草图中，我们加入了停用词过滤器（stop word filter）和罕见词过滤器（rare word filter）。字符串从顶部流入，底部根据代币“堆栈”的高度轮廓生成词袋（Bag of Words，BOW）向量。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747463172136-5be39c11-d33f-4c3e-a1e5-995bf3b0b5e4.png)

事实证明，机器能够很好地处理这种词袋表示，并从中获取即使是中等长度文档的大部分信息内容。每个文档经过代币分类和计数后，都可以被表示为一个向量。图1.6展示了一个粗略的例子，在第2章中，我们将研究一些更有用的词袋向量数据结构。

这是我们语言的第一个向量空间模型（vector space model）。这些箱子及其包含的每个词的数字被表示为长向量，这些向量中大多是零，只有在对应词出现的位置散布着少数的1或2。所有不同的词组合成这些向量的方式被称为向量空间。向量空间中向量之间的关系构成了我们的模型，该模型试图预测这些词在各种词序列（通常是句子或文档）中的组合出现情况。

在 Python 中，我们可以将这些稀疏（大部分为空）的向量（数字列表）表示为字典。Python 的 Counter 是一种特殊的字典，它能够像我们想要的那样对对象（包括字符串）进行分类并计数：

```python
>>> from collections import Counter
>>> Counter("Guten Morgen Rosa".split())
Counter({'Guten': 1, 'Rosa': 1, 'morgen': 1})
>>> Counter("Good morning Rosa!".split())
Counter({'Good': 1, 'Rosa!': 1, 'morning,': 1})
```

你大概可以想象出一些清理这些代币（tokens）的方法，我们将在下一章中正是这样做的。但你也可能认为有更好的方式来表示一句话，比如 “Guten morgen Rosa.”。我们将在第3、4和6章学习不同的代币序列表示方法。

我们可以想象将所有找到的文档、陈述、句子甚至单个词逐条输入这台机器。处理完每条陈述后，我们会统计底部每个槽位中的代币数量，并将其称为该陈述的向量表示。这个文档、陈述和词汇的模型称为向量空间模型（vector space model）。现在，我们可以使用线性代数操作这些向量，计算自然语言陈述的距离和统计量，这有助于我们用更少的人为编程和更低的脆弱性解决更广泛的自然语言处理（NLP）问题。

对词袋（BOW）向量序列常被提出的一个统计问题是：“**<font style="color:#DF2A3F;">在给定特定词袋的情况下，最有可能出现的词汇组合是什么？</font>**”或者更进一步，如果用户输入一串词，可以问：“**<font style="color:#DF2A3F;">数据库中与用户提供的词袋向量最相近的词袋是什么？</font>**”这就是搜索查询。输入的词汇就像你在搜索框中输入的内容，而最接近的词袋向量对应你想要找到的文档或网页。有效回答这两个问题的能力足以构建一个随着数据增多而不断改进的机器学习聊天机器人。

但请稍等，这些向量可能与你以往处理过的向量不同——它们的维度极高。对于从大型语料库计算得到的三元组词汇表（trigram vocabulary），维度可能达到数百万。在第3章，我们将讨论维度灾难（curse of dimensionality）及其他使高维向量难以处理的特性。

<h2 id="uaYIr">1.6 超空间简介</h2>
在第三章，你将学习如何将词汇整合到更少的向量维度中，以应对维度灾难（curse of dimensionality）。你甚至可能将这种灾难转化为祝福，通过利用所有这些维度来识别你希望自然语言理解（NLU）管道理解的细微之处。你可以将向量相互投影以确定每对向量之间的距离。这给你提供了一个合理的估计，用于衡量它们在含义上的相似性，而不仅仅是统计词汇使用情况。当你以这种方式计算向量距离时，它被称为余弦距离度量（cosine distance metric）。你将首先在第三章使用余弦距离，然后在第四章，你将通过将主题向量的数千个维度降低到仅几个维度来发现其真正的威力。你甚至可以将这些向量投影（更准确的术语是嵌入（embed））到二维平面上，以在图表和图形中"查看"它们。这是发现高维数据中模式和聚类的最佳方法之一。然后，你可以教计算机以反映产生这些向量的词语底层含义的方式来识别并对这些模式采取行动。

想象一下人类可能写出的所有可能的推文、消息或句子。尽管我们经常重复自己，但这仍然是大量的可能性。而当这些标记每个都被视为独立的、不同的维度时，我们没有理由相信"Good morning, Hobs"与"Guten Morgen, Hannes"有任何共享的含义。我们需要创建一些降维的向量空间模型（vector space model）来表示消息，这样我们就可以用一组连续的（浮点数）值来标记它们。我们可以按主题和情感等品质对消息和单词进行评分，提出这样的问题：

+ 这条消息是问题的可能性有多大？
+ 它有多少是关于一个人的？
+ 它有多少是关于我的？
+ 它听起来有多愤怒或开心？
+ 这是我需要回应的事情吗？

想想我们可以给陈述赋予的所有评分。我们可以将这些评分排序并为每个陈述"计算"它们，为每个陈述编制一个"向量"。我们可以给出一组陈述的评分或维度列表应该比可能的陈述数量小得多，而且含义相同的陈述在所有问题上的值应该相似。这些评分向量成为机器可以被编程响应的东西。我们可以通过将陈述聚集（聚类（clustering））在一起来进一步简化和泛化向量，使它们在某些维度上接近，在其他维度上相距更远。

但是计算机如何为每个向量维度分配值呢？通过简化我们的向量维度问题，例如"它是否包含单词good？"或"它是否包含单词morning？"等等。你可以想象我们可以提出大约一百万个问题，从而产生计算机可以对短语进行的数值分配。这是第一个实用的向量空间模型，称为位向量语言模型（bit vector language model）。你可以理解为什么计算机现在才有足够的能力来理解自然语言。人类可以生成的百万维向量的庞大数量在80年代的超级计算机上"无法计算！"，但在现代普通笔记本电脑上却不是问题。使自然语言处理（NLP）变得实用的不仅仅是原始硬件功率和容量；增量、常量RAM、线性代数算法是使机器能够破解自然语言代码的最后一块拼图。

还有一个更简单但更大的表示可用于聊天机器人。如果我们的向量维度完全描述了字符的确切序列呢？每个字符的向量将包含关于字母表中每个字母和标点符号的二进制（是/否）问题的答案：

+ "第一个字母是A吗？"
+ "第一个字母是B吗？"
+ …
+ "第一个字母是z吗？"

下一个向量将回答关于序列中下一个字母的相同无聊问题：

+ "第二个字母是A吗？"
+ "第二个字母是B吗？"
+ …

尽管这个向量序列中有很多"否"的答案，或者零，但它确实比所有其他可能的文本表示有一个优势：它保留了原始文本中的每一个微小细节，每一位信息，包括字符和单词的顺序。这就像一个只演奏单个音符的自动钢琴的纸带表示法。这种自然语言机械自动钢琴的"音符"是26个大小写字母加上钢琴必须知道如何"演奏"的任何标点符号。纸卷不必比真正的自动钢琴宽多少，而且某些长钢琴曲目中的音符数量不超过小文档中的字符数。

但是这种独热字符序列编码（one-hot character sequence encoding）表示主要用于记录然后重放确切的片段，而不是创作新内容或提取片段的本质。我们不能轻易比较一首歌的钢琴纸卷与另一首歌的纸卷。而且这种表示比文档的原始ASCII编码表示更长。可能的文档表示数量刚刚爆炸，以保留有关每个字符序列的信息。我们保留了字符和单词的顺序，但扩大了我们的NLP问题的维度。

在这个基于字符的向量世界中，这些文档表示不能很好地聚类在一起。俄罗斯数学家弗拉基米尔·列文斯坦（Vladimir Levenshtein）提出了一种在这个世界中快速找到向量（字符串）之间相似性的出色方法。列文斯坦算法（Levenshtein's algorithm）使得只用这种简单、机械的语言观点就能创建一些令人惊讶地有趣和有用的聊天机器人成为可能。但是当我们弄清楚如何将这些高维空间压缩或嵌入到模糊含义或主题向量的低维空间中时，真正的魔术才开始发生。

我们在第四章揭开魔术师的帷幕，届时将讨论潜在语义索引（latent semantic indexing）和潜在狄利克雷分配（latent Dirichlet allocation），这两种技术用于创建更加密集且有意义的语句和文档向量表示。



<h2 id="lKp6L">1.7 词序和语法</h2>
词序--语法--很重要。这是我们在前面例子中词袋（BOW）或词向量丢弃的内容。幸运的是，在大多数短语，甚至是许多完整句子中，这种词向量近似效果很好。如果你只是想编码一个短句的一般含义和情感，词序并不非常重要。看看我们的"Good morning Rosa"例子的所有这些排序：

```python
>>> from itertools import permutations
>>> [" ".join(combo) for combo in
... permutations("Good morning Rosa!".split(), 3)
... ]
['Good morning Rosa!',
 'Good Rosa! morning',
 'morning Good Rosa!',
 'morning Rosa! Good',
 'Rosa! Good morning',
 'Rosa! morning Good']
```

现在，如果你试图孤立地解释这些字符串中的每一个（不看其他的），你可能会得出它们都有类似意图或含义的结论。你甚至可能注意到"Good"一词的大写，并在你的脑海中将这个词放在短语的前面。但你也可能认为"Good Rosa"是某种专有名词，比如餐厅或花店的名称。尽管如此，一个聪明的聊天机器人或20世纪40年代布莱切利园（Bletchley Park）的聪明女性可能会对这六种排列中的任何一种做出相同的无害问候，"早上好，亲爱的将军。"

让我们在一个更长、更复杂的短语上尝试（在我们的头脑中），这是一个词序非常重要的逻辑陈述：

```python
>>> s = """Find textbooks with titles containing 'NLP',
... or 'natural' and 'language', or
... 'computational' and 'linguistics'."""
>>> len(set(s.split()))
12
>>> import numpy as np
>>> np.arange(1, 12 + 1).prod() # factorial(12) = arange(1, 13).prod()
479001600
```

排列数从我们简单问候语的阶乘(3) == 6爆炸式增长到我们更长陈述的阶乘(12) == 479001600！很明显，词序中包含的逻辑对于任何想要做出正确回应的机器都很重要。尽管常见的问候语通常不会被词袋（BOW）处理打乱，更复杂的陈述在被扔进袋子里时可能会失去大部分含义。词袋不是开始处理数据库查询的最佳方式，比如前面例子中的自然语言查询。

无论是用SQL这样的正式编程语言书写的陈述，还是用英语这样的非正式自然语言书写的陈述，当陈述意图传达事物之间的逻辑关系时，词序和语法都很重要。这就是为什么计算机语言依赖于严格的语法和语法规则解析器。幸运的是，自然语言句法树解析器的最新进展使得以显著的精确度（大于90%）从自然语言中提取句法和逻辑关系成为可能。在后面的章节中，我们将向你展示如何使用SyntaxNet（Parsey McParseface）和spaCy等软件包来识别这些关系。

就像在布莱切利园的问候语例子中一样，即使一个陈述不依赖于词序来进行逻辑解释，有时，注意词序可以揭示可能促进更深层次回应的细微含义提示。这些更深层次的自然语言处理（NLP）在下一节中讨论。第二章向你展示了一种将词序传达的部分信息整合到词向量表示中的技巧。它还向你展示如何改进前面例子中使用的粗糙分词器（str.split()），以便更准确地将单词归入词向量中更合适的槽，这样像good和Good这样的字符串会被分配到同一个槽中，而rosa和Rosa可以分配到不同的槽中，但Rosa!不会。



<h2 id="Oae2n">1.8 聊天机器人自然语言处理管道</h2>
构建对话引擎或聊天机器人（chatbot）所需的自然语言处理（NLP）管道与《驯服文本》（Taming Text）中描述的问答系统管道类似。然而，五个子系统模块中列出的一些算法可能对你来说是新的。事实上，一些最有前景的生成式方法，如大型语言模型（LLMs），只是最近才被发明出来。每一章将帮助你实现并测试图中的一个或多个算法，使你能够为你的应用组装正确的管道。图1.7中的四个圆角方块显示了分析和生成文本所需的四种处理类型。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747464222065-a5350ff3-eb53-49b6-98e0-9cb9a922a1e6.png)

你需要一个数据库来维护过去语句和回应的记忆，并建立结构化知识库来帮助为你的管道提供业务逻辑和决策逻辑。图1.7中显示的五个子系统可以通过一个或多个算法实现：

1. 解析（Parse）—— 从自然语言文本中提取特征，即结构化的数值数据。
2. 分析（Analyze）—— 通过对文本的情感、语法和语义进行评分来生成和组合特征。
3. 生成（Generate）—— 使用模板、搜索和语言模型来构建可能的回应。
4. 决策（Decide）—— 决定哪个生成的回应最有可能将对话引向用户的会话目标。
5. 数据库（Database）—— 存储对话历史、用户信息和一般世界知识，用于决策子系统。

这四个阶段中的每一个都可以使用方框图中相应框内列出的一个或多个算法来实现。你可以在自己的自然语言处理系统中组合本书中的Python示例，以实现大多数应用的最先进性能。到本书结束时，你将掌握实现这五个子系统的几种替代方法。大多数聊天机器人将包含所有五个子系统的元素，但许多应用只需要几行Python就能实现的简单算法。有些聊天机器人更善于回答事实性问题，而其他的则更擅长信息检索或搜索。有些聊天机器人甚至可以生成冗长、复杂、听起来像人类的回应。你可能已经猜到，看似合理的回应并不一定正确或有用。你将了解所有最流行的自然语言处理方法的优缺点。

机器学习（machine learning）、深度学习（deep learning）和概率语言模型（probabilistic language models）已经迅速扩大了你可以成功应用自然语言处理的应用范围。机器学习的数据驱动方法允许通过在你想要应用的领域中提供越来越多的数据，使自然语言处理管道变得更加复杂。尽管如此，数据并不是你所需要的全部。采用更高效的机器学习方法，你通常可以超越竞争对手。本书将给你提供利用这些进步所需的理解。

图1.7中的聊天机器人管道包含了本章开始描述的大多数自然语言处理应用的所有构建块。我们还在生成的文本回应上显示了一个"反馈循环"，这样我们的回应可以使用与处理用户语句相同的算法进行处理。然后，回应的"得分"或特征可以在目标函数中组合，以评估和选择最佳的可能回应，这取决于聊天机器人对对话的计划或目标。本书专注于为聊天机器人配置这个自然语言处理管道，但你可能也能看到与文本检索或"搜索"这个最常见的自然语言处理应用的类比。我们的聊天机器人管道当然适用于《驯服文本》中重点关注的问答应用。

这个管道应用于财务预测或商业分析可能不是那么明显。但想象一下由你的管道分析部分生成的特征。这些分析或特征生成的特点可以针对你从事的特定金融或商业预测进行优化。这样，它们可以帮助你将自然语言数据纳入用于预测的机器学习管道。尽管专注于构建聊天机器人，本书为你提供了广泛的自然语言处理应用所需的工具，从搜索到财务预测。

图1.7中通常不会在搜索、预测或问答系统中使用的一个处理元素是自然语言生成（natural language generation）。这是聊天机器人的核心特征。尽管如此，文本生成步骤通常被整合到搜索引擎自然语言处理应用中，并能给这类引擎带来巨大的竞争优势。整合或总结搜索结果的能力是许多流行搜索引擎（如DuckDuckGo、Bing和Google）的制胜特点。你可以想象，对于金融预测引擎来说，能够基于从社交媒体网络和新闻源检测到的商业可行事件生成语句、推文或整篇文章是多么有价值。下一节将展示如何结合这样一个系统的层次，为自然语言处理管道的每个阶段增加复杂性和功能。

<h2 id="OsJHc">1.9 深度处理</h2>
NLP管道的各个阶段可以被视为层，类似于前馈神经网络中的层。深度学习就是通过在传统的两层机器学习模型架构（特征提取后接建模）上添加更多处理层来创建更复杂的模型和行为。在第五章中，我们解释神经网络如何通过将模型误差从输出层反向传播（backpropagating）回输入层来帮助在各层之间分散学习。但在这里，我们讨论顶层以及通过独立于其他层训练每一层可以做什么。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747464784565-dead2e01-d4ee-4e88-aff2-53cb7fec606c.png)

图1.8中的顶部四层对应于上一节中聊天机器人管道的前两个阶段（特征提取和特征分析）。例如，词性（POS）标注是在我们聊天机器人管道的分析阶段生成特征的一种方法。POS标签通常由默认的spaCy管道自动生成，该管道包括本图中的顶部四层。POS标注通常通过有限状态转换器（FST），如nltk.tag包中的方法来完成。

底部两层（实体关系和知识库）用于填充包含特定领域信息（知识）的数据库。使用所有这六层从特定语句或文档中提取的信息可以与该数据库结合使用来进行推理。推理是根据在环境中检测到的一组条件进行的逻辑推断，例如聊天机器人用户语句中包含的逻辑。该图中更深层次的这种"推理引擎"被视为人工智能的领域，机器可以对其世界进行推理并使用这些推理来做出逻辑决策。然而，聊天机器人可以仅使用上面几层的算法做出合理的决策，而无需这种知识数据库，这些决策可以结合起来产生令人惊讶的类人行为。

在接下来的几章中，我们将深入研究NLP的前几层。仅前三层就足以执行有意义的情感分析和语义搜索，以及构建模仿人类的聊天机器人。实际上，只用一层处理就有可能构建出有用且有趣的聊天机器人，直接使用文本（字符序列）作为语言模型的特征。一个仅进行字符串匹配和搜索的聊天机器人能够参与相当令人信服的对话，只要有足够的示例语句和回应。

例如，开源项目ChatterBot通过仅计算输入语句与其数据库中记录的语句之间的字符串"编辑距离"（列文斯坦距离，Levenshtein distance）来简化这个管道。如果其语句-回应对数据库包含匹配语句，则可以将相应的回复（来自先前"学习"的人类或机器对话）重用作对最新用户语句的回复。对于这个管道，只需要我们聊天机器人管道的第3步（生成）。在这个阶段中，只需要一个暴力搜索算法来找到最佳响应。使用这种简单的技术（不需要分词或特征生成），ChatterBot可以作为Gunther Cox用回收部件构建的机械机器人Salvius的对话引擎，维持一个令人信服的对话。

Will是Steven Skoczen创建的一个开源Python聊天机器人框架，采用了完全不同的方法。Will只能通过用正则表达式编程来训练响应语句——这是一种劳动密集且数据量少的NLP方法。这种基于语法的方法对问答系统和任务执行助手机器人（如Lex、Siri和Google Now）特别有效。这类系统通过采用"模糊正则表达式"（fuzzy regular expressions）和其他近似语法匹配技术来克服正则表达式的"脆弱性"。模糊正则表达式通过忽略一些最大数量的插入、删除和替换错误，在可能的语法规则（正则表达式）列表中找到最接近的语法匹配，而不是精确匹配。然而，扩展基于模式匹配的聊天机器人的行为广度和复杂性需要大量困难的人工开发工作。即使是由地球上一些最大公司（例如，Google、Amazon、Apple和Microsoft）构建和维护的最先进的基于语法的聊天机器人，在聊天机器人智商的深度和广度方面仍然处于中游水平。

使用浅层NLP可以完成许多强大的事情，几乎不需要人类监督（文本标注或策划）。通常，机器可以从其环境（它可以从Twitter或其他来源提取的词流）中永久学习。我们将在第7章向您展示如何做到这一点。



<h2 id="aCxaA">1.10 自然语言智商</h2>
就像人类的脑力一样，自然语言处理管道的能力无法通过单一的智商分数轻易衡量，需要考虑多个"智能"维度。衡量机器人系统能力的常见方式是沿着行为复杂性和所需人类监督程度这两个维度。但对于自然语言处理管道，目标是构建能够完全自动处理自然语言的系统，消除所有人类监督（一旦模型训练完成并部署）。因此，更好的智商维度应该能够捕捉自然语言处理管道的广度和深度的复杂性。

消费级产品聊天机器人或虚拟助手，如亚马逊Alexa或谷歌Allo，通常设计为拥有极其广泛的知识和能力。然而，用于响应请求的逻辑往往比较浅显，通常由一组触发短语组成，这些短语都通过单个if-then决策分支产生相同的响应。Alexa（及其底层的Lex引擎）的行为就像一个单层、扁平的(if, elif, elif, ...)语句树。谷歌Dialogflow（独立于谷歌的Allo和谷歌助手开发）具有与亚马逊Lex、亚马逊Contact Flow和Lambda类似的功能，但没有用于设计对话树的拖放式用户界面。

另一方面，谷歌翻译管道（或任何类似的机器翻译系统）依赖于特征提取器、决策树和连接关于世界知识碎片的知识图的深度树。有时，这些特征提取器、决策树和知识图是明确地编程到系统中的，如图1.9所示。另一种迅速超越这种硬编码管道的方法是数据驱动的深度学习方法。深度神经网络的特征提取器是学习而非硬编码的，但它们通常需要更多的训练数据才能达到与有意设计算法相同的性能。

在逐步构建能够在特定知识领域内对话的聊天机器人的自然语言处理管道时，你将同时使用这两种方法（神经网络和手工编码算法）。这将为你提供在你的行业或业务领域内完成自然语言处理任务所需的技能。在这个过程中，你可能会产生关于如何扩展这个自然语言处理管道功能广度的想法。图1.9将聊天机器人放在已有的自然语言处理系统中的位置。想象一下你曾经交互过的聊天机器人。你认为它们可能适合在这样的图表中的什么位置？你是否尝试过通过提出难题或智商测试之类的问题来衡量它们的智能？试着问聊天机器人一些需要常识逻辑和提出澄清问题能力的模糊问题，例如"太阳和一枚五分硬币哪个更大？"在后面的章节中，你将有机会这样做，以帮助你决定你的聊天机器人与图中的其他聊天机器人相比如何。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1747465353830-963ed30d-ceff-4cb7-87ea-8ad0c0aecc89.png)

在阅读本书的过程中，你将构建聊天机器人的各个元素。聊天机器人需要自然语言处理的所有工具才能良好运作：

+ 特征提取--通常用于生成向量空间模型
+ 信息提取--能够回答事实性问题
+ 语义搜索--能够从其文档数据库中检索相关知识
+ 自然语言生成（NLG）--用于组合新的、有意义的语句

机器学习为你提供了一条捷径，可以快速构建行为上看起来像是由一群程序员花费数年时间编程、具有数百个复杂硬编码算法和决策树分支的机器。使用现代自然语言处理管道，你可以教会机器有效响应文本中的模式，而无需陷入正则表达式的地狱；你只需要用户语句的示例以及你希望聊天机器人模仿的响应。机器学习对拼写错误和打字错误的挑剔程度远低于任何手工制作的专家系统或正则表达式，并且机器学习产生的语言模型更好、更通用、统计上更准确。

机器学习自然语言处理管道更容易"编程"。你不再需要预测用户使用的每种语言中符号的每种可能用法；你只需要用标记的示例来喂养训练管道。而且你的聊天机器人响应的质量主要取决于标记数据集的质量，使自然语言处理对你团队中的更多人变得易于使用。

自然语言处理正在革命性地改变我们交流、学习、做生意甚至思考的方式。我们正在亲眼目睹机器生成的内容侵入社会集体智慧的越来越多领域。你也将很快构建、训练和调整模拟人类对话行为的自然语言处理系统。在接下来的章节中，你将学习如何用任何你感兴趣的领域知识来训练聊天机器人或自然语言处理管道--从金融和体育到心理学和文学。如果你能找到关于它的文本语料库，你就能训练机器与该内容进行交互。

本书讲述的是如何使用机器学习来构建智能文本阅读机器，而不需要你预测人们表达事物的所有方式。每一章都在图1.7和本书封面内侧介绍的聊天机器人架构的基本自然语言处理管道基础上进行增量改进。当你学习自然语言处理的工具时，你将构建一个不仅能进行对话，还能帮助你实现商业和生活目标的自然语言处理管道。



<h2 id="GgspR">1.11 自测题</h2>
<h3 id="CojGE">1 为什么NLP被认为是AGI（类人工智能）的核心使能特征？</h3>
自然语言处理（NLP）被视为实现人工通用智能（AGI）的核心使能特征，因为它使机器能够理解、解释、生成和响应人类语言。NLP在AGI的追求中扮演着关键角色，让机器可以进行有意义的对话，与人类和其他代理协作解决复杂问题，并理解人类的社会和情感方面。它使机器能够构建丰富且细致的世界表征，进而发展出推理能力和对语言细微差别的理解，这些都是实现类人智能的基础要素。

<h3 id="vEM8K">2 如果你的训练数据包含大量反社会示例，你将如何构建亲社会的聊天机器人？</h3>
要在训练数据包含反社会示例的情况下构建亲社会的聊天机器人，我会采取以下策略：

+ 过滤训练数据中的反社会内容，减少其在训练中的影响
+ 使用积极情感引导模型，因为研究表明情感引导会影响模型的亲社会行为
+ 实施内容审核和过滤系统，确保生成内容符合道德规范
+ 使用显式亲社会示例对模型进行微调，强化正面行为
+ 设计奖励机制，优先考虑亲社会回应
+ 在训练过程中实施道德准则和价值观引导

研究表明，即使是先进的模型如ChatGPT-4在被负面情绪引导时也会产生较少的亲社会回答，因此积极地管理情感和社会行为引导至关重要。

<h3 id="hQCfm">3 聊天机器人的五个主要子系统是什么？</h3>
聊天机器人的五个主要子系统包括：

1. **解析（Parse）**-从自然语言文本中提取特征，即结构化的数值数据
2. **分析（Analyze）**-通过对文本的情感、语法和语义进行评分来生成和组合特征
3. **生成（Generate）**-使用模板、搜索和语言模型来构建可能的回应
4. **决策（Decide）**-决定哪个生成的回应最有可能将对话引向用户的会话目标
5. **数据库（Database）**-存储对话历史、用户信息和一般世界知识，用于决策子系统

<h3 id="NjGfS">4 NLP如何在搜索引擎中使用？</h3>
自然语言处理在搜索引擎中的应用包括：

+ **语境化搜索查询**：NLP使搜索引擎能够解释复杂的搜索查询，考虑单词之间的上下文和关系，从而更加细致地理解用户查询
+ **识别用户意图**：NLP分析搜索查询并提取其背后的意图，允许搜索引擎相应地定制结果
+ **超越关键词匹配**：NLP帮助搜索引擎更好地理解关键词和短语的含义，不仅仅是简单的单词匹配
+ **内容语义理解**：通过理解自然语言的上下文、意图和细微差别，搜索引擎可以提供更个性化和上下文丰富的搜索结果
+ **处理语言歧义**：使用语境建模和概率方法来处理语言中的歧义，例如确定"bank"是指金融机构还是河岸

<h3 id="jdiTr">5 编写一个正则表达式来识别你的名字及其所有拼写变体（包括昵称）。</h3>
以名字"John Smith"为例，包括常见的昵称和拼写变体：

```python
\b(John(ny|athan)?|Jon)\s+(Smith|S\.)\b
```

这个正则表达式可以匹配：

+ John Smith
+ Johnny Smith
+ Jonathan Smith
+ Jon Smith
+ John S.

可以根据个人名字的具体情况进行调整，添加更多变体或特殊拼写。

<h3 id="nYXKJ">6 编写一个正则表达式来尝试识别句子边界（通常是句号、问号或感叹号）。</h3>
识别句子边界的正则表达式：

```python
[.!?](\s+|$)
```

这个表达式匹配任何句号、问号或感叹号，后面跟着空白字符或字符串结尾。为了处理更复杂的情况（如缩写中的句号），可以扩展此表达式：

```python
(?<=[a-z])[.!?](?=\s+[A-Z]|$)
```

这个更复杂的表达式查找小写字母后面的标点符号，且后面是空格加大写字母或字符串结尾。

<h2 id="Rtn54">本章总结</h2>
+ 通过构建亲社会的NLP软件，你可以帮助让世界变得更美好。
+ 机器可以破译单词的含义和意图。
+ 智能NLP管道可以处理歧义并帮助纠正人类错误。
+ 机器可以通过仅处理未标记文本来构建关于世界的知识库。
+ 聊天机器人可以被视为语义搜索引擎，从用于训练的文档中检索最相关的响应。
+ 正则表达式不仅仅用于搜索，还有更多用途。

