> **本章涵盖内容**
>
> + 统计单词、n-gram（n元语法）和词频以分析含义
> + 使用 Zipf 定律预测单词出现概率
> + 将自然语言文本表示为向量
> + 在文本集合中使用文档频率找到相关文档
> + 使用余弦相似度估计文档对之间的相似性
>

---

在收集和统计单词（标记）并将它们归类到词干或词元后，是时候用它们做一些有趣的事情了。检测单词在简单任务中非常有用，比如关键字搜索。但如果你想做一些更高级的任务，比如分类文本或确定其主题，那么你将需要知道哪些单词对于某个特定文档和整个语料库来说最重要，并将这种“重要性”作为一个数值表示。然后，你可以使用这个“重要性”值，在一个基于每个文档中单词重要性的语料库中找到相关文档。这样可以使垃圾邮件检测器不容易被单个咒骂词或少数疑似“垃圾”词所干扰。如果你知道这些单词在文档中出现的频率，并将其与它们在其他文档中出现的频率相比，那么你可以进一步提高文档的“正面性”。在本章中，你将学习一种更细致、非二元化的单词及其在文档中使用情况的度量，称为**词频-逆文档频率（TF-IDF）**。这种方法长期以来一直是从自然语言生成特征以供商业搜索引擎和垃圾邮件过滤器使用的主流方法。

你探险的下一步是将第二章中的单词转换为连续数值，而不仅仅是代表单词计数或二进制“位向量”的整数。这种方法将单词的存在或缺失表示在一个连续空间中，允许你以更有趣的数学方式对其进行操作。你的目标是找到单词的数值表示，能够以某种方式捕捉它们所代表的单词的重要性或信息内容。你需要等到第四章才能看到如何将这些信息内容转换为代表**单词意义**的数字。

在本章中，我们探讨了三种越来越强大的表示单词及其在文档中重要性的方法：

+ **词袋（Bags of words）**——以单词计数或频率为向量
+ **n-gram词袋（Bags of n-grams）**——以单词对（bigrams）、三元组（trigrams）等的计数为向量
+ **TF-IDF向量（TF-IDF vectors）**——以更好地表示其重要性的单词分数为向量

:::success
**重要提示** TF-IDF 是**词频-逆文档频率（Term frequency–inverse document frequency）**的缩写。词频是文档中每个单词的计数，你在前几章中学过。逆文档频率的意思是将这些单词计数除以单词在出现的文档数。

:::

每种技术都可以单独使用，也可以作为自然语言处理（NLP）管道的一部分使用。这些都是**基于频率的**统计模型。在后续章节中，你将看到多种方法深入探索单词关系及其模式和非线性特征。但这些“浅层” NLP 模型在许多实际应用中都非常强大和有用，例如搜索、垃圾邮件过滤、情感分析，甚至聊天机器人。

<h2 id="X4VKD">3.1 词袋向量</h2>
让我们深入探讨将一段文本表示为机器可以处理的数值向量的挑战。在上一章中，你创建了第一个文本空间模型。你对每一个词进行了独热编码（one-hot encoding），然后用二元`OR`（或剪辑求和）将所有这些向量结合在一起，从而创建了文本的向量表示。这个二元词袋（binary bag-of-words，BOW）向量，在加载到如`pandas DataFrame`这样的数据结构时，可以很好地作为文档检索的索引。

接着，你进一步探索了一种更有用的向量表示，它统计了给定文本中每个词的出现次数（频率）。作为一种初步近似，你假设某个词出现得越多，它对该文档的意义就越重要。一个提到 _wings_（机翼）和 _rudder_（方向舵）的文档，很可能比一个提到 _cats_（猫）和 _gravity_（重力）的文档更与飞机或航空旅行相关。又或者，如果你把一些词归为表达积极情感的词，比如 _good_（好）、_best_（最好）、_joy_（喜悦）和 _fantastic_（极好），那么包含这些词次数越多的文档，其所表达的情绪也更可能是积极的。但是一个仅仅依赖这些简单规则的算法可能会产生错误或误导。

让我们看一个例子，说明统计词出现次数的有用性。我们看一篇来自维基百科关于算法偏见（algorithmic bias）文章的句子：

```python
>>> import spacy
>>> spacy.cli.download("en_core_web_sm")
>>> nlp = spacy.load("en_core_web_sm")
>>> sentence = ('It has also arisen in criminal justice, healthcare, and '
...             'hiring, compounding existing racial, economic, and gender biases')
>>> doc = nlp(sentence)
>>> tokens = [token.text for token in doc]
>>> tokens
['It', 'has', 'also', 'arisen', 'in', 'criminal', 'justice', ',', 
'healthcare', ',', 'and', 'hiring', ',', 'compounding', 
'existing', 'racial', ',', 'economic', ',', 'and', 
'gender', ',', 'biases']
```

如果这是你第一次运行`en_core_web_sm`模型，可能需要在终端中运行：

```bash
python -m spacy download en_core_web_sm
```

`spaCy`语言模型会将自然语言文本分词为一个文档对象（`Doc`类），其中包含输入文本中的所有词序列。它还会对文档进行分句（以`.sents`属性表示）。用Python中的`set()`类型，可以将这个词序列转换为文本中所有唯一词的集合。

列出文档或语料库中所有唯一词的列表，称为词汇表（vocabulary）或词典（lexicon）。在NLP管道中，创建词汇表是非常重要的一步。如果你没有识别特定词，并给它一个存储位置，管道会完全忽略它。在大多数NLP管道中，你会为不在词汇表中的词定义一个单独的标记（例如`<oov>`，表示超出词汇范围），用于存储所有未识别或忽略的词的相关信息，例如它们出现的次数。如果你不希望在词汇表中包含不寻常或造词（例如“supercalifragilistic”）这样的词，你可以把它们合并为一个单一的通用标记，并将NLP管道的处理限制在计算这些单一标记的含义。

`Python`的`Counter`类是统计序列或数组中任何对象（包括词元）出现次数的高效方法。在第2章中，你学习过`Counter`是一个特殊的字典，其中键是数组中所有唯一对象，字典值是这些对象的出现次数。

```python
>>> from collections import Counter
>>> bag_of_words = Counter(tokens)
>>> bag_of_words
Counter({',': 5, 'and': 2, 'It': 1, 'has': 1, 'also': 1, 'arisen': 1, ...})
```



`collections.Counter` 对象本质上是一个 `dict`。这意味着键实际上存储在一个无序集合（unordered collection）或 `set` 中，有时称为 `bag`。看起来这个字典好像保持了句子中单词的顺序，但这只是幻觉。你很幸运，因为你的句子中没有包含很多重复的词。最新版本的 Python（3.6 及以上）在你往字典中插入新键时会维护键的顺序。但是你马上要创建的是一个词元及其计数的字典。你需要这些词元和它们的计数来进行线性代数运算和在文档（在这里是句子）集合上的机器学习。你的 BOW 向量会记录每一个唯一词元在向量中的一致索引号，用于在向量中确定它们的位置。这样，类似 `and` 和 `or` 的词元的计数会在你的所有文档（这里是“算法偏见”维基百科文章中的句子）向量中相加。



:::success
**TIP**    在 NLP 中，字典中键的顺序无关紧要，因为向量（比如 pandas `Series`）会在内部维持一致的顺序。正如第 2 章所学，`Counter` 字典会按照你处理语料库中每一份文档的顺序，排列你的词汇表（`dict` 键）。有时候，你可能希望对词汇表按字母顺序排序，使其更易于分析。一旦你为每一个词元在向量中分配了一个维度（即槽位），请务必将该顺序记录下来，以便将来复用你的管道而无需重新处理所有文档。而如果你尝试复现别人的 NLP 流程，你会希望完全复用他们的词汇表（即标记列表），保持完全相同的顺序。否则，你将需要以完全相同的顺序、使用完全相同的软件来处理他们的数据集。

:::



对于像“算法偏见”维基百科文章句子这样较短的文本，混乱的 BOW 仍然包含了大量关于原句意图的信息。BOW 中的信息足以做一些很强大的事情，比如检测垃圾邮件、计算情绪（积极、消极或其他情绪），甚至检测隐含意图（比如讽刺）。它也许只是一个“袋子”，但它充满了含义和信息。为了让这些词更易于思考，并确保你的管道顺序一致，你想要按某种一致的顺序对它们进行排序。为了按词的计数对标记排序，`Counter` 对象提供了一个很方便的方法 `most_common`：

```python
>>> bag_of_words.most_common(3)
[(',', 5), ('and', 2), ('It', 1)]
```

真方便！`Counter.most_common` 方法会返回一个按计数排序的列表，其中每个元素是一个 2 元组，包含标记及其计数。参数 3 表示只会返回前三个标记。但这还不够，你需要一个向量表示才能对这些标记计数进行数学运算。

一个 `pandas.Series` 是一个高效的数据结构，用于存储标记计数，包括 `most_common` 方法返回的 2 元组。`Series` 的优点是，它在你使用数学运算符（如 `+`, `*`, 或 `.dot()`）时表现得像向量（NumPy 数组）。你仍然可以用正常的方括号语法（`['token']`）访问与每个标记相关的命名（标签）维度。

```python
>>> import pandas as pd
>>> most_common = dict(bag_of_words.most_common())
>>> counts = pd.Series(most_common)
```

`pandas.Series` 在打印到屏幕上时显示效果很好，这在你理解标记计数向量内容时非常有用。现在你已经创建了一个计数向量，你可以像处理其他 `pandas.Series` 那样对它进行数学运算：

```python
>>> len(counts)
18
>>> counts.sum()
23
>>> len(tokens)
23
>>> counts / counts.sum()
,        0.217391
and      0.086957
It       0.043478
has      0.043478
also     0.043478
...
```

你可以看到句子中共有 23 个词元，但词汇表中只有 18 个唯一词元。因此，每个文档向量至少需要 18 个维度，即使其他文档中没有使用这些词。这让每个词元在向量中有自己的维度（slot）。每个词元在向量中被分配了一个“槽”，对应于词汇表中的位置。向量中有些词的计数会是 0，这正是你想要的。

这也解释了为什么 `,` 和 `and` 位于 `most_common` 列表的顶部。逗号（`,`）出现了 5 次，`and` 出现了 2 次，其他所有词都只出现过一次。这说明你最常见的两个词是 `,` 和 `and`。<font style="color:#DF2A3F;">这是自然语言文本处理中很常见的问题——最常见的词往往最没有意义。</font>停用词（stop words）就是这类词——它们通常不会告诉你多少有用信息，因此你可能想要忽略它们。一个更好的做法是：根据你的文档中词频的统计信息，对标记计数进行缩放，而不是采用别人的随意停用词列表。

一个单词在给定文档中出现的次数称为它的**词频**（term frequency，TF）。你可能首先想要做的事情之一是将词元计数**归一化**（normalize，除以文档中的总词数）。这会给出词在文档中相对频率（百分比或比例），而不考虑文档长度。来看一下文本中单词 _justice_ 的相对频率，看看这种方法是否能体现出这个词在文本中的重要性：

```python
>>> counts['justice']
1
>>> counts['justice'] / counts.sum()
0.043...
```

_justice_ 这个词在句子中的**归一化词频**（normalized term frequency）约为 4%，这个比例在你处理该文档中更多句子时也不太可能提高。如果句子和文章中 _justice_ 的出现次数大致相同，那么这个归一化的 TF 分数在整个文档中会保持大致不变。

根据这个 TF，单词 _justice_ 表示该句意义的约 4%。这并不算高，考虑到这个词对句子意义至关重要。所以你需要再进行一次归一化步骤，以提高该词相对于句中其他词的权重。

为了给 _justice_ 赋予一个表明其重要性或显著性的分数，你需要获取有关该词在这句子以外的使用情况的统计信息；你需要了解 _justice_ 在其他地方的使用情况。幸运的是，对于刚入门的 NLP 工程师，维基百科提供了大量高质量、准确的多语种自然语言文本。你可以使用这些文本“教会”你的机器了解 _justice_ 在多篇文档中的重要性。为了演示这种方法的强大功能，你只需要从维基百科“算法偏见”条目中提取几个段落：

> Algorithmic bias 描述了计算机系统中系统性和可重复的错误，这些错误会造成不公平结果，例如优先考虑某一任意用户群体而非其他用户群体。偏见可能由多种因素引发，包括但不限于算法设计、数据编码、收集、选择或用于训练算法时的方式。
>

...

> 算法偏见已在选举结果、网络仇恨言论等方面被引用。它还出现在刑事司法、医疗保健和招聘中，加剧了现有的种族、经济和性别偏见。
>

...

> 在理解、研究和发现算法偏见方面存在问题，因为算法的专有性质，这些算法通常被视为商业机密。
>

——维基百科



查看这些句子，看看是否可以找出对理解文本至关重要的关键词。你的算法需要确保它包含这些词并计算它们的统计信息。如果你试图用 Python 自动（程序化）检测这些重要词元，你会如何计算其重要性分数？想想你如何利用 `Counter` 字典来帮助你的算法理解算法偏见：

```python
>>> sentence = "Algorithmic bias has been cited in cases ranging from " \
... "election outcomes to the spread of online hate speech."
>>> tokens = [tok.text for tok in nlp(sentence)]
>>> counts = Counter(tokens)
>>> dict(counts)
{'Algorithmic': 1, 'bias': 1, 'has': 1, 'been': 1, 'cited': 1, 'in': 1, 'cases': 1, 
 'ranging': 1, 'from': 1, 'election': 1, 'outcomes': 1, 'to': 1, 'the': 1, 'spread': 1, 
 'of': 1, 'online': 1, 'hate': 1, 'speech': 1, '.': 1}
```

看起来这句话中没有任何重复的词元。词频分析和 TF 向量的关键是确定单词相对于其他词的使用程度的统计量。因此我们需要将其他句子输入系统，生成基于其他句子中单词使用情况归一化后的有用词元计数。为了理解“Algorithmic Bias”，你可以花时间将整个维基百科文章输入到 Python 字符串中。你还可以从 `nlpia2` 软件包的 GitLab 下载包含维基百科文章前三段的文本文件。如果你已经克隆了 `nlpia2`，可以在本地找到 `src/nlpia2/ch03/bias_intro.txt` 文件。如果你尚未从源代码安装 `nlpia2`，可以使用 `requests` 软件包通过以下代码片段下载该文件：

```python
>>> import requests
>>> url = ('https://gitlab.com/tangibeilai/nlpia2/'
...        '-/raw/main/src/nlpia2/ch03/bias_intro.txt')
>>> response = requests.get(url)
>>> response
<Response [200]>
```

`requests` 软件包返回一个包含 HTTP 响应头（`.headers`）和正文（`.text`）的 HTTP 响应对象。`nlpia2` 软件包中的 `bias_intro.txt` 文件是维基百科“算法偏见”条目前三段的 2023 年快照：

```python
>>> bias_intro_bytes = response.content  # requests.get 返回一个包含内容字节的对象
>>> bias_intro = response.text           # .text 属性包含 HTTP 正文中的 unicode 字符串
>>> assert bias_intro_bytes.decode() == bias_intro  # bytes.decode() 将内容解码为 unicode 字符串
>>> bias_intro[:70]
'Algorithmic bias describes systematic and repeatable errors in a compu'
```



对于纯文本文档，你可以使用 `response.content` 属性，它包含了原始 HTML 页面中的字节。如果你想获取字符串，可以使用 `response.text` 属性自动解码文本字节，创建一个 unicode 字符串。

`collections` 模块中的 Python 标准库 `Counter` 类非常适合高效地统计对象序列中的任何内容。当你希望统计唯一单词和标点符号在标记列表中出现的次数时，这对于 NLP 来说非常完美：

```python
>>> tokens = [tok.text for tok in nlp(bias_intro)]
>>> counts = Counter(tokens)
>>> counts
Counter({'Algorithmic': 3, 'bias': 6, 'describes': 1, 'systematic': 2, ...})
>>> counts.most_common(5)
[(',', 35), ('of', 16), ('.', 16), ('to', 15), ('and', 14)]
```

好吧，这些统计结果稍微更有统计学意义，但仍然有许多无意义的单词和标点符号，似乎有很高的出现频率。维基百科文章显然并不是真正关注 `like`, `of`, `to`, `,` 或 `..` 这样的词。也许，关注出现频率最低的词元会比关注最常见的词更有用：

```python
>>> counts.most_common()[-4:]
[('inputs', 1), ('between', 1), ('same', 1), ('service', 1)]
```

好吧，这也没什么用。你可能希望找到诸如 _bias_, _algorithmic_, 和 _data_ 这样的词。为了找到这些词，你需要使用一种公式，在计数中找到一个平衡点，为那些“刚刚好”的词找到“Goldilocks”分数。你可以通过另一个有用的计数来做到这一点：单词出现在多少文档中，即**文档频率**（document frequency）。这就是事情开始变得有趣的地方。

如果你有一个包含许多文档的大语料库，你可以根据某个词在所有文档中使用的频率，对文档内的计数进行归一化（除法）。由于你刚开始使用词元计数向量，最好先通过将维基百科文章摘要拆分成更小的文档（句子或段落）来创建一些小文档。这样，你至少可以在一页文档中查看所有代码的结果，并确定所有计数的来源。在下一节中，你正要这样做：将“算法偏见”文章拆分成句子，运用不同方式对词典进行归一化和结构化，使它们对 NLP 更有用。

<h2 id="UGVkh">**3.2　使用 DataFrame 构造函数对文本向量化**</h2>
`Counter`字典非常适合统计文本中的词元数量——但真正强大的是向量（vector）。事实证明，只需在字典列表上调用 DataFrame 构造函数，就可以将这些字典强制转换为 DataFrame 或 Series。pandas 会负责所有记录工作，使每个唯一的词元或字典键都有自己的列。当某个文档的 Counter 字典缺少特定键（因为该文档没有包含该词或符号）时，它会生成 NaN。

一旦你把 “Algorithmic Bias” 这篇文章按行拆分，你就能体会到向量表示的威力。随后你会发现，与标准的 Python dict 相比，pandas 的 Series 在处理词元时是更为有用的数据结构。

```python
>>> docs = [nlp(s) for s in bias_intro.split('\n')
...         if s.strip()]  # 使用 spaCy 对每一行进行分词，并跳过空行
>>> counts = []
>>> for doc in docs:
...     counts.append(Counter([
...         t.text.lower() for t in doc]))      # 先用 spaCy 分词，再转小写，以改进句子分割
>>> df = pd.DataFrame(counts)
>>> df = df.fillna(0).astype(int)              # 将 NaN 替换为 0，并转为整数，便于阅读
>>> len(df)
16
>>> df.head()
   algorithmic bias describes systematic ... between same service
0              1    1         1          1 ...       0    0      0
1              0    1         0          0 ...       0    0      0
2              1    1         0          0 ...       0    0      0
3              1    1         0          1 ...       0    0      0
4              0    1         0          0 ...       0    0      0
```

由于可以查看每个维度的含义，当向量维度对应词元或字符串得分时，将向量存储在 pandas 的 DataFrame 或 Series 中就非常有用。看看本章开头那句话；它正好是维基百科文章中的第 11 句话：

```python
>>> docs[10]
It has also arisen in criminal justice, healthcare, and hiring,
compounding existing racial, economic, and gender biases.
>>> df.iloc[10]  # 索引 10 对应零基 DataFrame 的第 11 行——维基百科文章中的第 11 句
algorithmic    0
bias           0
describes      0
systematic     0
and            2
...
Name: 10, Length: 246, dtype: int64
```

现在，这个 pandas Series 就是一个向量——可以进行数学运算的对象。进行运算时，pandas 会追踪每个单词的位置，确保例如 bias 和 justice 不会被误加到一起。在此 DataFrame 中，行向量为词表中的每个单词提供了一个“维度”。事实上，`df.columns` 属性就保存着你的词表。

不过，标准英语词典里有超过 30 000 个单词。如果你开始处理多篇维基百科文章，而不仅仅是几句话，将会面对大量维度。你或许习惯了 2D 和 3D 向量，因为它们易于可视化；但距离和长度等概念在 30 000 维空间中同样适用。稍后你会学习如何改进这些高维向量。现在只需知道，向量中的每个元素用来表示文档中某个单词的计数、权重或重要性。

接下来，你将先在每个文档中找出唯一单词，再找出所有文档共同的唯一单词；在数学上，这就是各文档单词集合的并集。这个主词集称为管道的词表（vocabulary）。如果你决定记录关于每个单词的更多语言学信息，例如拼写变体或词性，可以称之为词汇表（lexicon）。

来看一下这份由三段组成的小语料库的词表。首先进行大小写折叠（case folding），即转为小写，将大写单词（例如专有名词）与对应的小写单词合并成同一词元。这将在后续阶段减少词表中的唯一单词数量，更易于观察流程：

```python
>>> docs_tokens = []
>>> for doc in docs:
...     docs_tokens.append([
...         tok.text.lower() for tok in nlp(doc.text)])  # 使用 str.lower() 进行大小写折叠
>>> len(docs_tokens[0])
27
```

现已对这 28 个文档（句子）全部完成分词，你可以把所有标记列表拼接在一起，生成包含全部标记（含重复）的总列表。该列表与原始文档的唯一区别是已被分句并分词：

```python
>>> all_doc_tokens = []
>>> for tokens in docs_tokens:
...     all_doc_tokens.extend(tokens)
>>> len(all_doc_tokens)
482
```

然后，从整段文本的标记序列创建词表或词汇表。词表是语料库中所有唯一标记的列表；就像图书馆的词典一样，词表不包含重复项。除了 dict 类型外，你知道哪些 Python 数据类型可以去重？

```python
>>> vocab = set(all_doc_tokens)  # 将标记列表转换为 set，确保每个唯一标记只保留一次
>>> vocab = sorted(vocab)        # 若不需人工查看标记列表，则无需排序
>>> len(vocab)
246
>>> len(all_doc_tokens) / len(vocab)   # 通过用词表大小除以语料总词元数，计算平均复用次数
1.959...
```

使用 `set` 数据类型能够保证任何词元都不会被重复计算。对所有词元完成大小写折叠（case folding）后，你的 498 个单词语料中只剩 248 个拼写唯一的词元。这意味着平均来看，每个词元几乎被使用两次（498 / 248）：

```python
>>> vocab   # 词汇表（lexicon）存储在变量 vocab 中
["", "'s", ',', '.', '-', '2018', ';', 'a', 'ability',
 'accurately', 'across', 'addressed', 'advanced', 'algorithm',
 'algorithmic', 'algorithms', 'also', 'an', 'analysis',
 ...
 'within', 'world', 'wrongful']
```

通常最好先遍历整个语料来构建词汇表，再回到各文档中统计词元并放到正确的词汇槽里。这样做可以按字母顺序排列词汇表，方便你大致确定每个词元计数在向量中的位置。该方法还能让你过滤掉非常常见或非常罕见的词元，从而忽略它们并保持维度较低。假设你希望跟踪这个全小写词汇表中的全部 248 个词元，就可以重新组装计数向量矩阵。



```python
>>> count_vectors = []
>>> for tokens in docs_tokens:
...     count_vectors.append(Counter(tokens))
>>> tf = pd.DataFrame(count_vectors)      # tf 是 “term frequency（词频）” 的常用缩写
>>> tf = tf.T.sort_index().T
>>> tf = tf.fillna(0).astype(int)
>>> tf
      "  's  ,  ... within  world  wrongful
0     0   0   1  ...      0      0         0
1     0   0   3  ...      0      0         0
2     0   0   5  ...      0      0         0
...
15    0   0   4  ...      1      0         0
16 rows × 246 columns
```

浏览几行计数向量，看看能否找出它们在 “Algorithmic Bias” 维基百科条目中对应的句子。仅凭这些向量，你是否能大致了解每个句子的含义？

计数向量仅用数值就能传达文档的 “主旨”。对于完全不了解单词含义的机器来说，将这些计数按词元在整个语料中的出现频率进行归一化非常有用，这正是 scikit-learn 软件包派上用场的地方。

<h3 id="bfxvp">3.2.1　更快、更好、更易用的词元计数</h3>
既然你已经知道如何手动创建计数向量，可能会想是否已有库能完成这些计数工作。幸运的是，scikit-learn（sklearn）可以满足所有机器学习-NLP 需求。<sup>3</sup> 如果你已经安装了 `nltk2`，那么通常也安装了 scikit-learn（sklearn）。如果想手动安装，可使用 `pip` 或你喜欢的包管理器：

```plain
pip install scipy scikit-learn
```

在 ipython 控制台或 Jupyter Notebook 中，可在行首加感叹号运行 Bash 命令：

```python
>>> !pip install scikit-learn   # 行首感叹号是 shell 命令的转义符
```

安装好环境后，每当需要计数词元时，就能使用 `CountVectorizer` 类。`CountVectorizer` 属于 scikit-learn 的 **转换器（Transformer）** 类，因此有 `.fit()` 方法和 `.transform()` 方法，且需按此顺序调用。`.fit()` 会遍历整个语料并记录每个文档中各词元出现的次数。`CountVectorizer` 需要这些词频计数，以便使用 `max_df`（最大文档频率）或 `min_df`（最小文档频率）参数过滤常见或罕见词元。虽然在代码清单 3.2 中无需考虑词元流行度，但当语料规模更大时就会发现其重要性。`CountVectorizer.transform()` 方法与清单 3.2 使用的 Python `Counter` 类类似。`CountVectorizer` 基于 `TransformerMixin`，从而在整个 scikit-learn 包中保持一致的 API。<sup>4</sup>

使用 `CountVectorizer` 的步骤如下（请看代码清单3.3的演示）：

1. 配置一个空的 `CountVectorizer` 实例。
2. 对语料调用 `.fit()`。
3. 对语料调用 `.transform()`，得到计数向量。



```python
>>> import numpy as np
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> np.set_printoptions(edgeitems=8)      # 告诉 numpy 在打印大数组时如何美化输出；参见 pd.options.display
>>> corpus = [doc.text for doc in docs]
>>> vectorizer = CountVectorizer()        # 在此构造函数中可配置忽略某些词元或为语料使用专用分词器
>>> vectorizer = vectorizer.fit(corpus)
>>> count_vectors = vectorizer.transform(corpus)   # .transform() 返回行向量数组，16 个文档 × 词汇表中每个词一列
>>> count_vectors
<16x240 sparse matrix of type '<class 'numpy.int64'>'
    with 376 stored elements in Compressed Sparse Row format>
```

**Wow**, 这速度真快！不过 _稀疏矩阵（sparse matrix）_ 是什么？稀疏矩阵是一种紧凑（且快速）存储大型二维数组的方法，例如计数向量数组。`numpy` 的稀疏矩阵通过跳过零计数来节省内存（RAM）。当你想为所有维基百科文章生成计数向量时，就需要稀疏矩阵——那意味着数百万篇文档和数百万个唯一词元。`CountVectorizer` 的稀疏矩阵能轻松处理此类数据，甚至更多。

清单 3.3 的稀疏矩阵似乎拥有正确的向量数量（16 行，对应 16 个句子）。根据你使用 `Counter` 字典列表的经验，可以猜到 240 列对应词汇表中的 240 个唯一词元。但是，你可能想查看稀疏矩阵中隐藏的所有词元计数。只需将稀疏矩阵转换为密集矩阵或数组即可。在真实环境下，除非只是查看一小段数据，否则不要这么做；否则可能导致电脑崩溃。但对这 16 句的小语料而言，将整个矩阵转换回熟悉的密集 NumPy 数组格式完全没问题。

**代码清单 3.4　使用 sklearn 计算词语计数向量**

```python
>>> count_vectors.toarray()   # .toarray() 方法把稀疏矩阵转换成常规 numpy 数组，用零填补空缺
array([[0, 0, 0, 0, 0, 1, ..., 0, 0, 0, 0],
       [0, 0, 0, 3, ..., 0, 0, 0, 0],
       [0, 0, 0, 2, ..., 0, 0, 0, 0],
       ...
       [0, 0, 0, 0, 1, ..., 0, 0, 0, 0],
       [0, 0, 0, 1, 0, ..., 1, 0, 0, 0]])
```

现在，你应该能明白为什么要跳过那些零值了。大多数文档只包含少量独特单词，因此你的计数向量总是充斥着作为占位符的零——代表所有未使用的单词。你的向量数组此时为 16 个文档中的每一个提供一行，为 240 个词元中的每一个提供一列。

这样，你就有了三条向量，每条对应一个文档。接下来怎么办？这些文档词频向量能够执行任何向量可以完成的酷炫操作，所以先来进一步了解向量及其所在的向量空间。<sup>6</sup>

<h3 id="dqkeL">**3.2.2　向量化（vectorizing）你的代码**</h3>
也许你在互联网上读到过“向量化代码”的说法；但这与“向量化文本”完全不同。**向量化文本（vectorizing text）** 指的是把文本转换成有意义的数值向量表示；而 **向量化代码（vectorizing code）**（见代码清单 3.5）则是借助诸如 NumPy 这类高效的编译库来加速代码、避免使用 `for` 循环。这之所以称为“向量化”，是因为你可以用向量代数的记法来消除代码中的 `for` 循环；在许多 NLP 流水线里，`for` 循环是最慢的环节。与其用 `for` 循环遍历向量或矩阵中的所有元素进行数学运算，不如直接让 NumPy 在用 C 语言编译的底层代码里完成循环。由于 pandas 在底层进行所有向量代数时也使用 NumPy，你可以把 `DataFrame` 与 NumPy 数组或 Python 浮点数混合运算，而且运行速度依旧飞快。额外的好处是，你能删除那些冗长、复杂的 `for` 循环。



```python
>>> v1 = np.arange(5)                           # range() 函数同样被视为向量化函数，可用来消除 for 循环
>>> v2 = pd.Series(reversed(range(5)))

>>> slow_answer = sum([4.2 * (x1 * x2) for x1, x2 in zip(v1, v2)])  # 把 for 循环放进列表推导再求和
>>> slow_answer                                                     # sum() 本身在 Python 内已向量化
42.0

>>> faster_answer = sum(4.2 * v1 * v2)                              # 使用乘法后直接求和
>>> faster_answer
42.0

>>> fastest_answer = 4.2 * v1.dot(v2)                               # 用点积先乘后加，一步完成
>>> fastest_answer
42.0
```

Python 的动态类型设计让这一切成为可能。当你把一个浮点数与数组或 `DataFrame` 相乘时，解释器不会因为在两种不同类型上做数学运算而报错，而是会像 Sulu<sup>7</sup> 那样领会你的意图并“立即执行”。它会用最快的方式（调用用 C 语言编译的代码，而不是 Python 的 `for` 循环）计算你想要的结果。

:::success
**TIP**　如果利用向量化消除代码中的部分 `for` 循环，你的 NLP 流水线速度可提升百倍以上；这意味着在相同时间内可以尝试百倍数量的模型。柏林社会科学中心网站提供了一篇极佳的向量化教程。<sup>8</sup> 如果你在该站点其他地方多加探索，还能找到或许是唯一值得信赖的关于 NLP 与 AI 对社会影响的数据统计来源。<sup>9</sup>

:::

如果你认为向量化只是“锦上添花”，因为它不会改善代码的算法复杂度（大 O 表示法），那就再想想吧。的确，从大 O 分析看算法不会得到改进，但你仍能获得极致的效率提升。向量化正是现代 NLP 流水线中简单向量搜索得以实现的关键。向量搜索目前炙手可热，为从 ChatGPT 到你最喜欢的应用中的语义搜索引擎提供支持。代码清单 3.6 摘自 Knowt 项目，展示在普通笔记本电脑上如何极快地完成一次点积运算。<sup>10</sup> 借助 NumPy 的内存映射文件（`numpy.memmap`），即使内存有限，也能快速执行向量化操作。遗憾的是，TF–IDF 向量化器必须常驻内存，但清单 3.6 仍展示了向量化操作可带来的性能飞跃。当你要构建个人知识管理（PKM）系统、需在毫秒级别处理超过 4 万条 Hacker Public Radio（HPR）句子时，这一点至关重要。



```python
>>> !git clone git@gitlab.com:tangibleai/community/knowt
>>> !cd knowt
>>> mmvecs = np.memmap(
...     '.knowt-data/hpr_vectors.memmap',
...     shape=(41_531, 384),   # 使用内存映射文件必须预先知道数组形状
...     dtype=np.float32,
...     mode='r')
>>> vecs = np.array(mmvecs.T.copy().tolist())
>>> variables = dict(vecs=vecs, v=v)
>>> dt_vectorized = timeit('v.dot(vecs)', globals=variables, number=20)
>>> dt_vectorized
0.106...
```

完成 320 万次乘加运算仅耗时略高于 100 毫秒。要把 384 维向量的各元素与 41 531 条句子向量的相应元素相乘，需要 41 531 × 384 ＝ 160 万次乘法；随后再执行几乎同样数量（160 万次）的加法，把每条句子向量的 384 个乘积相加。对于主频 2.8 GHz、4 核的处理器而言，理论上每秒可执行 112 亿次浮点运算（FLOPS）。在这种情况下，每秒 3200 万次运算的向量化操作效率是合理的，因为运行 Python 代码还需消耗数百次额外的开销与簿记操作。

你觉得如果用 `for` 循环为每个向量计算点积，会慢多少？下面的代码清单将对 HPR 节目注释中的 4 万多个稠密向量全部使用 `for` 循环进行遍历，供你对比。



```python
>>> def loops():
...     answers = np.zeros(shape[0])
...     for i, vec in enumerate(vecs):
...         answers[i] = sum((x1 * x2 for (x1, x2) in zip(v[0], vec)))
...     return answers
>>> variables = dict(np=np, loops=loops, vecs=vecs.T, v=v)
>>> dt_loop = timeit('loops()', globals=variables, number=20)
>>> dt_loop
0.671856558000854
>>> dt_loop / dt_vectorized
6.31581165358781
```

看起来，使用传统的 **for 循环** 进行线性代数运算（例如点积）大约会带来 6 倍的额外开销（变慢）。如果你花时间把代码向量化，就能把几乎一个数量级的计算时间节省下来，从而将这些时间投入到在更大数据集上训练模型，或使用更复杂模型上去。在某些情况下，你也许节省不了多少计算时间，但很可能会节省更多开发时间。向量化代码不仅让它更快，也让它更容易阅读和维护——即便在 Python 解释器不一定跑得更快的情况下也如此。例如，在 Knowt 项目中，我们在内存映射文件（`np.memmap`）上进行了这些速度测试（清单 3.6 和清单 3.7），结果发现 **for 循环** 实际上更快。<sup>11</sup> 因此，对于一些大型 NLP 流水线，最好自己做一次测速，看看哪种方法最适合你。

既然你已了解如何高效计算一些向量相似度（点积）运算，是时候构建一个关于向量用途的心智模型了。事实证明，自然语言文本的向量表示对你将在本书学到的所有其他语言模型都至关重要。现在正是讨论向量空间、巩固你对线性代数运算（如点积）理解的好时机。

<h3 id="umoLb">3.2.3 向量空间 TF–IDF（词频–逆文档频率）</h3>
向量——有序数字列表或坐标——是线性代数（向量代数）的基本构件。这些数字描述了空间中的位置。你也可以用向量表示方向与大小，或距离。**向量空间（vector space）** 是你可以存储任意向量表示的空间；在数学与范畴论中，它是 _所有_ 能用给定维度表示的向量的集合。例如，二维（坐标）向量会落在二维向量空间中。你朋友住址的纬度与经度就是二维向量空间中的示例。如果你曾用地图、坐标纸或图像中的像素网格，就已经在二维向量空间里工作过。

如果你用的是二维向量（如 $ latitude, longitude $），就必须保持“纬度在前，经度在后”的一致性，否则计算会出错。如果意外调换 x、y 坐标顺序，你会把对象移到新位置。若要让新的位置也适用于后续线性代数运算，你可能还得调整部分计算——因此向量不同于普通列表或数组，不应随意改变顺序。如果你发现自己在给计数向量或 TF 向量排序，很可能做错了什么。

本章（以及全书其余部分）中你将使用的所有向量和向量空间都是 **直线型（rectilinear，欧几里得）** 的，即每一维都与其他维垂直（成直角）。因此，你可以用毕达哥拉斯定理（平方和再开平方根）计算欧氏距离——两点间的直线长度。

地图或地球仪上的纬度–经度坐标怎样？那个地理坐标空间确实是曲面的，当你放大看时，看似所有坐标都在直角处相交。但你并不是“地平论者”（希望如此！），因此不会用毕达哥拉斯公式去算圣迭戈到基辅的距离，而是信赖航空公司计算跨越地球表面的曲线长度；那是一种 **曲线型（curvilinear，非直线）** 向量空间。每对纬度–经度坐标描述地球近似球面上的一点。如果再添加第三维（飞机离地高度或到地心的距离），就能获得三维坐标。<sup>12, 13</sup>

你无需担心曲线型向量空间中复杂的数学来计算距离；在 NLP 中几乎永远不必考虑向量空间曲率。你可以在二维中可视化计数向量和 TF 向量，以便直观了解某个计数向量的均值或文档在向量空间中的可能位置（如下面的代码所示）。Python 拥有可视化二维、三维向量所需的一切工具。



```python
>>> from matplotlib import pyplot as plt
>>> import seaborn as sns

>>> palette = sns.color_palette("muted")    # 其他调色板：pastel、bright、colorblind、deep、dark
>>> sns.set_theme()                         # 可能的主题：notebook、whitegrid、darkgrid

>>> vecs = pd.DataFrame([[1, 0], [2, 1]], columns=['x', 'y'])
>>> vecs['color'] = palette[:2]
>>> vecs['label'] = [f'vec{i}' for i in range(1, len(vecs)+1)]

>>> fig, ax = plt.subplots()
>>> for i, row in vecs.iterrows():
...     ax.quiver(0, 0, row['x'], row['y'], 
...                angles='xy', scale_units='xy', scale=1)
...     ax.annotate(row['label'], (row['x'], row['y']), 
...                 color=row['color'], verticalalignment='top')
>>> plt.xlim(-1, 3)
>>> plt.ylim(-1, 2)
>>> plt.xlabel('X (e.g. frequency of word "vector")')
>>> plt.ylabel('Y (e.g. frequency of word "space")')
>>> plt.show()
```

图3.1 展示了可视化两个二维向量 (0, 1) 和 (2, 1) 的一种方法。对于 TF 向量，每个向量都从 **原点** (0, 0) 开始；向量的 **尾**（tail）是它开始的地方，**头**（head，通常标尖三角形）则表示向量在空间中的目标位置。因此，如果你仅统计单词 _vector_ 与 _space_，上述两句话的 TF 向量会生成图 3.1 中的两个向量：以 “The tail of a vector …” 开头的句子只用到一次 _vector_，其坐标为 (1, 0)；以 “The head of a vector …” 开头的句子使用两次 _vector_、一次 _space_，坐标为 (2, 1)。

![图 3.1　二维向量](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748153641936-8b6c2349-b0b7-429e-aaa8-06db3869788a.png)

三维向量空间如何？你在物理课上（甚至 3D 游戏如 Minecraft）使用过三维向量空间。只要需要为词汇表中的第三个词留位置，就能使用三维向量空间；可以把 _z_ 轴——第三维——从纸面上竖起来表示新词。但轴标签用完并不会限制 TF（计数）向量的维度。对于 NLP，你可以处理 4 维、10 维、10 000 维，或任何你需要的维度——毕竟，要追踪 listing 3.2 中 246 个不同词元的计数，就需要 246 维度。

在真实世界中，你可能需要处理含数百万维度的 TF（词频计数）向量；屏幕上无法可视化所有这些维度。但第 4 章你将学习一些降维技巧，以及一些三维向量图，帮助可视化高维向量的“本质（eigen）”。<sup>14</sup>

无论你有多少维度，线性代数规律都相同。因此，你可以用本章的向量数学来衡量向量的一些有趣性质。不过，你最终会遇到 **维度灾难（curse of dimensionality）**，但在第 4 章和第 10 章会学到避免该诅咒的高级算法。维度过多是祸，因为随着维度增加，高维向量在欧氏距离上会彼此越来越远；在高维空间里找到向量变得困难，因为空间非常大，难以逐个查看向量。很多简单操作在 10 维或 20 维以上就不太可行，例如按与查询向量距离排序一个大向量列表——此时搜索引擎必须用 **近似最近邻（approximate nearest neighbor，ANN）** 算法来在合理时间内完成。<sup>15</sup> 幸运的是，本章中的高维向量是稀疏的（TF 矩阵中的大多数值为 0），因此不会受到维度灾难的影响。

对于自然语言文档的向量空间，其维度是整个语料库中不同单词或词元的数量。这个不同词数称为语料的 **词汇量大小（vocabulary size）**。在学术论文中，你可能看到用符号 $ |V| $ 来表示词汇量大小，也可以使用大写字母 K 作为变量存储该词汇量。如果想分别跟踪每个单词的计数，需要一个 K 维向量，让每个单词的计数占据向量中的一个位置。你可以用一个 K 维向量描述每个文档；如果有多个向量，就可把所有可能取值的空间称作 **向量空间**——你的向量可能落在的所有位置。要在向量空间中导航，你需要开发一种方法来衡量所有向量之间的距离。

<h2 id="vR5rx">**3.3　向量距离与相似度（Vector distance and similarity）**</h2>
向量间最常用的两种距离度量是 **欧几里得距离（Euclidean distance）** 和 **余弦距离（cosine distance）**。对应的接近度量则是 **欧几里得相似度（Euclidean similarity）** 与 **余弦相似度（cosine similarity）**。你可能比自己意识到的更了解这些度量——例如，你大概可以用 **毕达哥拉斯定理（Pythagorean theorem）** 来计算图 3.1 中标记为 _Vec1_ 的向量从尾到头的距离。这被称为该向量的**长度（length）**，或 **L2 向量范数（L2 vector norm）**：L 代表长度，2 代表毕达哥拉斯定理中使用的指数。

向量本身的长度并不如它们之间的距离重要。要计算距离，需要用一个向量减去另一个向量，并求得两者差向量的长度。在图 3.2 中，用虚线描绘的新箭头表示 _Vec1_ 与 _Vec2_ 之差。

![图 3.2 二维欧几里得距离](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748153969845-04b0b20b-6a91-4e24-ad1c-1a8d18da4d8a.png)

要用 _Vec2_ 减去 _Vec1_，可以使用 NumPy 表达式 `v2 - v1`。将 v1、v2 的二维坐标代入得  
$ (2, 1) - (1, 0) = (-1, -1) $，这就是两向量之间的差向量。该差向量的长度就是 v1 与 v2 的距离，毕达哥拉斯公式给出的距离为 0.707。

当维度较少时，欧几里得距离完全可用；但若要对百万维向量求距离，即便两向量相距很近，也会得到非常大的数值。因此，在高维向量场景下，更倾向使用 **余弦距离** 而非欧几里得距离。余弦距离的最大值为 1，所以无论维度多少，距离都不会超过 1。

**余弦距离** 与两向量之间的夹角成正比——角度越大，余弦距离越接近 1；当夹角 90° 时，距离为 1。与其用余弦距离，更简便且更有用的是计算 **余弦相似度（cosine similarity）**。余弦相似度可通过 $ 1-\text{cosine distance} $ 得到；你可以把它想象成一条向量在另一条向量上投射（或“投下阴影”）的比例。余弦相似度是两向量夹角 θ 的余弦值，用**归一化点积（normalized dot product）**计算，较欧几里得距离更易求得。余弦相似度之所以深受 NLP 工程师喜爱，原因包括：

+ 计算速度快，即便在高维向量上亦如此
+ 对单一维度的变化敏感
+ 适用于高维向量
+ 取值范围在 -1 到 1 之间

使用余弦相似度不会拖慢 NLP 流水线，因为只需计算**点积（dot product）**；事实上，你甚至不必调用余弦函数即可得到余弦相似度。只需线性代数中的点积运算，不涉及任何三角函数求值，因而非常高效（快速）。余弦相似度对每个维度独立计量，它们对向量方向的影响可以相加，即便在高维向量中亦然。由于 TF–IDF 向量可能包含成千上万，甚至数百万个维度，你需要一种在维度增多时依旧可靠的度量——这正是前述 **维度灾难（curse of dimensionality）** 所强调的。

<h3 id="XVvRJ">**3.3.1　点积（dot product）**</h3>
在 NLP 中你会频繁用到**点积**，因此了解它非常重要。如果你已经能在脑海里计算点积，可以跳过本节。

点积也称为**内积（inner product）**，因为两个向量（每个向量的元素数）或矩阵（第一个矩阵的行数与第二个矩阵的列数）的“内维度”必须相同——乘积就在这些位置上产生。点积还被称为**标量积（scalar product）**，因为它输出单个标量值，用以区别于输出向量的**叉积（cross product）**。显然，这些名称反映了在正式数学符号中表示点积（$ \cdot $）与叉积（$ \times $）时符号形状的差异。要得到标量积的标量输出，只需将一个向量的所有元素与第二个向量的对应元素相乘，然后把这些乘积相加即可。

下面这段 Python 代码可以在你大脑里的 “Python 解析器” 中运行，以确保你理解什么是点积。

```python
>>> v1 = np.array([1, 2, 3])
>>> v2 = np.array([2, 3, 4])
>>> v1.dot(v2)
20
>>> (v1 * v2).sum()                 # NumPy 数组相乘属于“向量化”操作，非常高效
20
>>> sum([x1 * x2 for x1, x2 in zip(v1, v2)])  # 除非想拖慢流水线，否则别这样迭代
20
```

:::success
**TIP**　点积等价于**矩阵乘法（matrix product）**，可通过 `np.matmul()` 函数或 `@` 运算符完成。所有向量都能转换为 $ N \times 1 $ 或 $ 1 \times N $ 矩阵，因此可将前者转置后与后者相乘：  
`v1.reshape(-1, 1).T @ v2.reshape(-1, 1)`  
结果会以 $ 1 \times 1 $ 矩阵 `array([[20]])` 返回你的标量积。

:::



这里给出教科书中的**归一化点积（normalized dot product）**形式：

$ \mathbf{A}\cdot\mathbf{B} = \lVert\mathbf{A}\rVert \lVert\mathbf{B}\rVert \cdot \cos(\theta) \tag{3.2} $

在 Python 中，可用如下代码计算**余弦相似度（cosine similarity）**：

```python
>>> A.dot(B) == (np.linalg.norm(A) * np.linalg.norm(B)) * \
...             np.cos(angle_between_A_and_B)
```

将上式对 `np.cos(angle_between_A_and_B)` （即向量 A 与 B 的余弦相似度）化简，可得计算公式：

```python
>>> cos_similarity_between_A_and_B = A.dot(B) / \
...     (np.linalg.norm(A) * np.linalg.norm(B))
```

用线性代数记号表示，即方程 3.2：

$ \cos(\theta)=\frac{\mathbf{A}\cdot\mathbf{B}}{\lVert\mathbf{A}\rVert\lVert\mathbf{B}\rVert} $

若不使用 NumPy，可改写成纯 Python：

```python
>>> import math
>>> def cosine_sim(vec1, vec2):
...     dot_prod = 0
...     for x1, x2 in zip(vec1, vec2):
...         dot_prod += x1 * x2
...
...     mag_1 = math.sqrt(sum([x1**2 for x1 in vec1]))
...     mag_2 = math.sqrt(sum([x2**2 for x2 in vec2]))
...
...     return dot_prod / (mag_1 * mag_2)
```

你需要先对两个向量做点积——逐元素相乘并求和——再分别除以两个向量的**范数（norm，长度）**。向量范数等同于其从尾到头的欧几里得距离，即元素平方和的平方根。这个**归一化点积**与余弦函数输出一样，取值范围为 $ [-1,1] $，表示两向量朝同一方向的程度。<sup>16</sup>

+ 余弦相似度为 1：归一化后向量完全同向；长度或大小可不同
+ 余弦相似度为 0：两向量在所有维度上正交
+ 余弦相似度为 -1：两向量方向完全相反
+ 对 TF 向量而言，-1 不可能出现（词频不能为负）。

在本章中，自然语言文档之间不会出现负的余弦相似度；但下一章会讨论互为反义的话题或单词，它们可能出现小于 0 甚至 -1 的相似度。

若要对 `CountVectorizer` 产生的普通 NumPy 向量计算余弦相似度，可使用 scikit-learn 内置工具：

```python
>>> from sklearn.metrics.pairwise import cosine_similarity
>>> tf = tf.fillna(0)                # 文档中缺失的词频需填 0
>>> vec1 = tf.values[1:2, :]         # 将 DataFrame（2D 数组）切片成 1×N 数组
>>> vec2 = tf.values[2:3, :]
>>> cosine_similarity(vec1, vec2)
array([[0.11785113]])                # scikit-learn 接收成对 2D 数组
```

切片 `tf` DataFrame 可能看起来奇怪，因为该函数针对大型向量数组（2D 矩阵）做了优化。上例将第一、第二行切成 $ 1\times N $ 数组，包含文本首句中的词频。

为验证为什么相似度这么低，可调用上面的 `cosine_sim` 函数（代码清单 3.11）。别忘了用 `[0]` 解引用，将 $ 1\times N $ 矩阵变成长度 N 的一维数组：

```python
>>> cosine_sim(vec1[0], vec2[0])
0.11785113019775792
```

太准确了！结果精确到 9 位有效数字，完全一致。即便你确信自己搞懂了为什么这两个计数字向量的相似度得分这么低，未来在调试结果不符合预期的流水线时，你仍可能需要追踪些问题。要进一步培养对余弦相似度的直觉，你可以修改代码清单 3.11，让它打印出匹配的词元及其计数；还可以把 `cosine_sim` 函数改写为接收 pandas 的 Series 或其他能记录维度与词元对应关系的对象，做成私人 NLP 调试工具。这类工具能像“透视眼”一样帮你看清团队的 NLP 流水线或使用中的搜索引擎为何产生意料之外的结果。解释 NLP 流水线和聊天机器人为何如此运作，是构建更智能、更具伦理性的 AI 时必不可少的技能。

这一计数字向量显示，维基百科条目 “Algorithmic Bias” 的首句与第二句仅有 11.7 %的相似度（余弦相似度 0.117）；看来第二句与第一句共享的词极少。  
若想更深入理解余弦距离，可用代码清单 3.11 检查：当输入 Counter 字典时，它会给出与 sklearn 对等 NumPy 数组相同的答案。顺便练习 **主动学习（active learning）**——先猜每对句子的余弦相似度，再运行函数验证并纠正自己，每一次迭代都会提升你对 NLP 工作的直觉。



<h2 id="p6qDF">**3.4 统计 TF–IDF 频率**</h2>
在第 2 章中，你学习了如何从语料中的词元创建 _n-gram_。现在该利用它们来获得更好的文档表示了。幸运的是，你仍可使用熟悉的工具，只需稍微调整参数即可。

首先，向语料再添加一句，这将说明为什么 _n-gram_ 向量有时比词频向量更有用：

```python
>>> import copy
>>> question = "What is algorithmic bias?"
>>> ngram_docs = copy.copy(docs)
>>> ngram_docs.append(question)
```

如果用清单 3.2 训练好的同一向量化器计算这条新句子的词频向量，会发现它与第二句话的表示完全一致：

```python
>>> question_vec = vectorizer.transform([question])
>>> question_vec
<1x240 sparse matrix of type '<class 'numpy.int64'>'
     with 3 stored elements in Compressed Sparse Row format>
```

稀疏矩阵可高效存储词元计数，但若想直观理解或调试代码，需要把向量 “致密化（densify）”。可用 `.toarray()` 方法把稀疏向量（稀疏矩阵的一行）转换为 NumPy 数组或 pandas Series：

```python
>>> question_vec.toarray()
array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ... ]])
```

---

你大概能猜到，问句中的哪个单词落在计数字向量的第 8 个位置（维度）。别忘了，这是 `CountVectorizer` 用 `.fit()` 生成词汇表后，按字母顺序排在第 8 位的单词。可以把计数字向量与词汇表配成 pandas Series，查看内部情况：

```python
>>> vocab = list(zip(*sorted((i, tok) for tok, i in
...     vectorizer.vocabulary_.items())))[1]
>>> pd.Series(question_vec.toarray()[0], index=vocab).head(8)
2018            0
ability         0
accurately      0
across          0
addressed       0
advanced        0
algorithm       0
algorithmic     1
```

接下来，计算问句向量与知识库中所有句子向量的 **余弦相似度（cosine similarity）**——搜索引擎或数据库全文检索就用这种方式查找答案：

```python
>>> cosine_similarity(count_vectors, question_vector)
array([[0.23570226],
       [0.12451456],
       [0.24743583],
       [0.4330127 ],
       [0.12909944],
       ...])
```

最相近（最相似）的向量对应语料的第 4 句，与 `question_vector` 的相似度为 0.433。查看知识库里的第 4 句，判断是否适合作为答案：

```python
>>> docs[3]
The study of algorithmic bias is most concerned with algorithms
that reflect "systematic and unfair" discrimination.
```

不错！这句话是一个不错的起点。然而，对于 _algorithmic bias_ 的定义来说，维基百科的第一句话或许更合适。思考如何改进向量化流水线，使搜索返回第一句而非第四句。

若想检验 **2-gram** 是否更有效，照清单 3.3 的步骤重新向量化，但将 `ngram_range` 超参数设为 `(1, 2)`，让向量化器统计 2-gram，而不是单词（1-gram）。**超参数（hyperparameter）** 只是函数名、参数值等；调整它们可改进 NLP 流水线。寻找最佳超参数称为 **超参数调优（hyperparameter tuning）**，可以从调高 `ngram_range` 开始：

```python
>>> ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))
>>> ngram_vectors = ngram_vectorizer.fit_transform(corpus)
>>> ngram_vectors
<16x616 sparse matrix of type '<class 'numpy.int64'>'
     with 772 stored elements in Compressed Sparse Row format>
```

观察新计数字向量的维度时，你会注意到它们明显更长——唯一 2-gram（词对）的数量永远多于唯一单词。查看对于你的问题至关重要的 _algorithmic bias_ 2-gram 的计数：

```python
>>> vocab = list(zip(*sorted((i, tok) for tok, i in
...     ngram_vectorizer.vocabulary_.items())))[1]
>>> pd.DataFrame(ngram_vectors.toarray(),
...              columns=vocab)['algorithmic bias']
0    1
1    0
2    1
3    1
4    0
```

第一句话可能更符合你的查询。不过，**袋装 n-gram（bag-of-n-grams）** 方法也有自身挑战：在大文本或大型语料中，n-gram 数量呈指数级增长，会引发前述 **维度灾难（curse of dimensionality）**。然而，正如本节所示，在某些情况下，你可能更愿意用 n-gram 而非单词计数。

<h3 id="xHnSF">**3.4.1 分析 “this”**</h3>
尽管到目前为止我们只处理过单词词元的 _n-gram（n-gram）_，但是字符 _n-gram_ 也很有用。例如，它们可以用于语言检测，或作者归属判断（在一组作者中判定谁写了当前文档）。下面我们用字符 _n-gram_ 和你在代码清单 3.3 学到的 `CountVectorizer` 类来解一道谜题。

Python 核心开发者十分有创意，他们在一个名为 **this** 的包里藏了一条有趣的彩蛋（秘密信息）。



```python
>>> from this import s as secret
>>> print(secret)

Gur Mra bs Clguba, ol Gvz Crgref
Ornhgvshy vf orggre guna htyl.
Rkcyvpvg vf orggre guna vzcyvpvg.
Fvzcyr vf orggre guna pbzcyrk.
...
Nygubhtu arire vf bsgra orggre guna *evtug* abj.
Vs gur vzcyvzragngvba vf uneq gb rkcyvna, vg'f n onq vqrn.
Vs gur vzcyvzragngvba vf rnfl gb rkcyvna, vg znl or n tbbq vqrn.
Anzrfcnprf ner bar ubaxvat terng vqrn -- lrg'f qb zber bs gubfr!
```



这些陌生单词是什么？它们用什么语言写成？H. P. Lovecraft 的粉丝或许会想到召唤死神克苏鲁时用的古老语言。<sup>17</sup> 但就算是他们，也看不懂这段文字。

要破解这段晦涩文本，你将用到刚学过的方法：**频率分析（frequency analysis，统计词元）**。不过这一次，一只小蓝鸟（或者 Mastodon）<sup>18</sup> 建议你先从 **字符词元** 而不是单词词元开始！幸运的是，`CountVectorizer` 完全支持：你可以把它配置成统计字符甚至字符三元组；若想忽略标点，scikit-learn 的向量化器同样能做到。你需要在几段不同文本上运行这个计数向量化器，以揭示 _this_ 文本（代码清单 3.14）中的隐藏模式。

将来你会感谢自己：在配置 scikit-learn 向量化器时要有意识地做决定，并把这些决定作为默认参数显式暴露在封装函数中。这样，当你想知道流水线为何忽略了下游函数需要的标点或其他词元时，Python 回溯能帮你快速定位 bug。这也能帮助你确定应用中可能的词汇表及向量表示。下面的代码清单给出了一个可复用函数，显式暴露了向量化器的 `vocabulary_` 和 `stop_words` 参数，方便集成到你的流水线。



```python
>>> from string import punctuation
>>> punc = list(punctuation) + list(' \n')
>>> def count_chars(text, tokenizer=list, token_pattern=None,
...                 stop_words=punc, **kwargs):
...     lot = [text] if isinstance(text, str) else text      # lot 表示文本列表；scikit-learn 期望字符串数组
...     vectorizer = CountVectorizer(
...         token_pattern=token_pattern,
...         stop_words=stop_words,
...         tokenizer=tokenizer,            # 默认把字符串按字符-gram 分词，tokenizer=list
...         **kwargs)
...     counts = vectorizer.fit_transform(lot)
...     counts = counts.toarray()[0]        # 取出唯一的计数字向量；.todense() 会更难处理
...     vocab = vectorizer.vocabulary_
...     index = pd.Series(vocab).sort_values().index
...     counts = pd.Series(counts, index=index)
...     return counts.sort_values()
>>> secretcounts = count_chars(secret)
>>> secretcounts
m     1
x     2
...
g    79
r    92
```

也许你还不确定这些频数有什么用；毕竟你尚未查看其他文本的字符频数。假设你对一篇随机的英文文章做同样统计，你能猜出哪些字母会位居列表首尾吗？验证猜想的办法：下载任意维基百科文章并统计其字符。下面示范如何下载维基百科 “Machine Learning” 条并查看它最常用的字符：<sup>19</sup>

```python
>>> !pip install nlpia2_wikipedia          # nlpia2_wikipedia 修复了官方包的 bug
>>> import wikipedia as wiki
>>> page = wiki.page('machine learning')
>>> mlcounts = count_chars(page.content)
>>> mlcounts
’       1
?       1
...
a    4146
e    5440
```

维基百科文章中出现次数最多的字母是 **e**，第二多的是 **a**。注意英文文档的前两名（e 和 a）与秘密文本的前两名（r 和 g）之间的差别吗？也许可视化会更直观（见图 3.3）。尝试把归一化后的字符计数并排绘制柱状图：

```python
>>> plt.subplot(2,1,1)
>>> secretcounts /= secretcounts.sum()      # 将字符计数除以总字符数，归一化为百分比
>>> secretcounts.sort_index()['a':'z'].plot(kind='bar', grid='on')
>>> plt.title('Secret Message')
>>> plt.subplot(2,1,2)
>>> mlcounts /= mlcounts.sum()              # 归一化“Machine Learning”文章的字符频率
>>> mlcounts.sort_index()['a':'z'].plot(kind='bar', grid='on')
>>> plt.title('ML Article')
>>> plt.show()
```

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748155897058-c3fe8734-684c-4158-88f3-e06f1ac4e6e1.png)

这很有意思！仔细比较两张频率直方图，你会发现最热门字母附近存在相同的“峰-谷-峰”模式，只是左右平移了。字符频率峰谷的这种 **相位偏移（phase shift）** 说明：秘密文本中最常见的字母 **r** 比英文中最常见的 **e** 右移了 13 个字母；另一个常见字母 **n** 比英文中的 **a** 也右移 13 个字母。所以，只要把字母表位置减去 13，或许就能把密文转回明文。

要验证这一偏移是否始终一致，你可以做 **谱分析（spectral analysis）**：用 `ord()` 和 `chr()` 计算两组信号最高峰位置之差：

```python
>>> peak_distance = ord('R') - ord('E')
>>> peak_distance
13
>>> chr(ord('v') - peak_distance)   # 字母 'I' 比 'V' 早 13 位
'I'
>>> chr(ord('n') - peak_distance)   # 字母 'A' 比 'N' 早 13 位
'A'
>>> chr(ord('W') - peak_distance)   # 最少见字母同样平移？
'J'
```

整数与字符映射按字母顺序排列，因此要解码密文中的 **R**，减去 13 得到英文中最常用的 **E**；解码 **V** 则会得到 **I**（第二常用）。

此时你或许已上网搜索过<sup>20, 21</sup>，发现这段秘密信息使用了 **ROT13 加密**。<sup>22</sup> ROT13 把字母在字母表中向后旋转 13 位；解码时向前旋转 13 位即可。你完全可以自己写一行代码完成编解码，也可以用 Python 内置 `codecs` 包直接揭开谜底：

```python
>>> import codecs
>>> print(codecs.decode(secret, 'rot-13'))
```

```plain
The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren’t special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you’re Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it’s a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let’s do more of those!
```

现在你已经知道 **“Python 之禅（Zen of Python）”** 了！这段箴言由 Python 语言设计者之一 Tim Peters 于 1999 年写成。此后它进入公有领域，被谱曲<sup>23</sup>，甚至被改编成恶搞版本。<sup>24</sup> “Python 之禅” 时刻提醒本书作者写出更简洁、可读、可复用的代码。希望这段代码也加深了你对“机器如何通过统计符号出现次数来破解人类语言”这一过程的理解。

<h2 id="MgXyL">**3.5 齐普夫定律（Zipf’s law）**</h2>
现在让我们进入主题——社会学。好吧，其实并不是，但你也许会喜欢这段快速的插曲：通过统计人和词语的数量，揭示日常生活中各种计数背后的另一条隐藏规律。事实证明，在自然语言中，就像自然界中的多数事物一样，只要你留心，优美的模式无处不在。

在 20 世纪初，法国速记员让-巴普蒂斯特·埃斯图普（Jean-Baptiste Estoup）注意到单词频率中的一种非凡模式，他为此煞费苦心地统计了自己能找到的所有文档（感谢计算机和 Python）。

1930 年代，美国语言学家乔治·金斯利·齐普夫（George Kingsley Zipf）试图正式化埃斯图普的观察，这种关系最终以齐普夫的名字命名。

> 给定某一自然语言语料库，任何单词的出现频率与它在频率表中的排序名次成反比。<sup>25</sup>
>
> — Wikipedia
>

更具体地说，**反比关系（inverse proportionality）****指列表中某项的计数（频率）与其在列表中的排名号码相关。显而易见，排名越高，计数越大；令人惊讶的是，当在****对数-线性（log-linear）图**上绘制时，这种关系呈线性。例如：在按流行度排序的列表中，第一项出现的次数大约是第二项的两倍、第三项的三倍。处理任何语料或文档时，一个有趣的做法是按排名（按频率）绘制单词使用频率。如果在**对数-对数（log-log）图**上发现某些离群点不落在直线上，或许值得深入调查它们。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748156400043-7ab0a377-fcd6-4563-ae30-465e9a3deb62.png)

为了展示齐普夫定律在“词”之外能延伸多远，图 3.4 展示了美国各城市人口与其人口排名之间的关系。事实证明，齐普夫定律适用于许多事物的计数。自然界充满了呈指数增长并带有**网络效应（network effects）**的系统，例如人口动态、经济产出、资源分布。<sup>26</sup> 令人惊奇的是，如此简单的齐普夫定律竟能在大量自然和人造现象中成立。诺贝尔奖得主保罗·克鲁格曼（Paul Krugman）在谈到经济模型与齐普夫定律时曾精辟地说：

> 人们通常抱怨经济学理论过于简化，给纷繁复杂的现实套上过于整洁的模型。[在齐普夫定律的世界里] 情况恰恰相反：现实本就井然有序，而模型却显得杂乱无章。
>
>   
— Paul Krugman
>

图 3.4 展示了克鲁格曼城市人口图的更新版本。

像研究城市和社交网络一样，我们也可以研究单词。首先，从 NLTK<sup>†</sup> 下载 **Brown 语料库（Brown Corpus）**：

```python
>>> import nltk
>>> nltk.download('brown')          # Brown 语料库约 3 MB
>>> from nltk.corpus import brown
>>> brown.words()[:10]              # .words() 返回分词后的字符串序列
['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']
>>> brown.tagged_words()[:5]        # 词性标注（part-of-speech tagging）示例；第 7 章和第 11 章会详细介绍
[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL')]
>>> len(brown.words())
1161192
```

> _Brown 语料库_ 是首个百万词级的英文电子语料库，1961 年由布朗大学创建。该语料库包含来自 500 个来源的文本，并按体裁（新闻、社论等）分类。<sup>28</sup>
>

拥有一百多万词元后，你就有充足的数据可供分析：

```python
>>> from collections import Counter
>>> puncs = set((',', '.', ':', ';', '--', '--', '!', '?',
...              '“', '”', '‘', '’', '(', ')', '[', ']'))
>>> word_list = (x.lower() for x in brown.words() if x not in puncs)
>>> token_counts = Counter(word_list)
>>> token_counts.most_common(10)
[('the', 69971),
 ('of', 36412),
 ('and', 28853),
 ('to', 26158),
 ('a', 23195),
 ('in', 21337),
 ('that', 10594),
 ('is', 10109),
 ('was', 9815),
 ('he', 9548)]
```

简单浏览即可发现，Brown 语料库中的词频符合齐普夫预测的对数规律：排名第 1 的单词 **the** 的出现次数大约是排名第 2 的 **of** 的两倍，是排名第 3 的 **and** 的三倍。若仍存疑，请运行 nlpia2 包中的示例代码（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py](https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py)）自行验证。

简言之，如果按出现次数为语料中的单词排序并降序列出，在足够大的样本中，列表首位单词的出现概率大约是第二位的两倍，是第四位的四倍。因此，对于大型语料，你可以用这种分布来判断某个单词在该语料任一文档中出现的统计概率。

<h2 id="vEcHT">**3.6  逆文档频率（inverse document frequency, IDF）**</h2>
现在回到文档向量。词频计数和 _n-gram_ 计数固然有用，但即便把纯粹的词频按文档长度归一化，也无法告诉你该词在当前文档中**相对于**语料库其他文档的重要性。如果能够弄清这一点，你就能开始描述语料库中的各个文档。假设你有一个包含所有人工智能（AI）图书的语料库，单词 _intelligence_ 几乎肯定在每本书（文档）里都会出现很多次，但这并不能提供任何新信息——它无法区分这些文档。相反，_neural network_ 或 _conversational engine_ 之类的词组可能在整套语料中并不常见，但一旦它们频繁出现，你就能更深刻地了解那些相关文档的性质。为此，你需要另一种工具。

**逆文档频率（inverse document frequency, IDF）** 的计算方式可能让你联想到之前为 Brown 语料库绘制齐普夫词频图时所做的工作。随着时间推移，你会发现最常出现的词语通常蕴含的信息最少。当你为 NLP 项目评估单词影响力时，也会注意到词语重要性与其频率呈指数关系，正如齐普夫分析指出的那样：语料库中第二重要的词元大概率比最重要的词元常见多个数量级。

取用你之前的 TF 计数器，并把思路扩展到按不同维度统计词元：按单个文档或整个语料计数。本节只按**文档**计数。回到维基百科的 _algorithmic bias_ 例子，再抓取一段涉及算法性种族歧视的内容，把它当作“偏见语料库（Bias Corpus）”中的第二篇文档。

> _Algorithms have been criticized as a method for obscuring racial prejudices in decision-making. Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime. This could potentially mean that a system amplifies the original biases in the data._
>

> _…_
>

> _A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on “creditworthiness,” which is rooted in the US fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities._ <sup>29</sup>
>

首先，计算“偏见语料库”中这两篇文档的总词数：

```python
>>> DATA_DIR = ('https://gitlab.com/tangibleai/nlpia/'
...             '/-/raw/master/src/nlpia/data')
>>> url = DATA_DIR + '/bias_discrimination.txt'
>>> bias_discrimination = requests.get(url).content.decode()
>>> intro_tokens = [t.text for t in nlp(bias_intro.lower())]
>>> disc_tokens  = [t.text for t in nlp(bias_discrimination.lower())]
>>> intro_total  = len(intro_tokens)
>>> intro_total
484
>>> disc_total   = len(disc_tokens)
>>> disc_total
9550
```

现在手头已有两篇关于 _bias_ 的分词文档，查看单词 **bias** 在每篇文档中的 TF，并把结果分别存入两个字典：

```python
>>> intro_tf = {}
>>> disc_tf  = {}
>>> intro_counts = Counter(intro_tokens)
>>> intro_tf['bias'] = intro_counts['bias'] / intro_total
>>> disc_counts = Counter(disc_tokens)
>>> disc_tf['bias']  = disc_counts['bias']  / disc_total
>>> print('Term Frequency of "bias" in intro is:{:.4f}'
...       .format(intro_tf['bias']))
Term Frequency of "bias" in intro is:0.0167
>>> print('Term Frequency of "bias" in discrimination chapter is:{:.4f}'
...       .format(disc_tf['bias']))
Term Frequency of "bias" in discrimination chapter is: 0.0022
```

数字相差 8 倍；难道引言部分就比后续章节更关注 _bias_？并非如此，还需更深入。先看看其它词，例如 **and** 的得分：

```python
>>> intro_tf['and'] = intro_counts['and'] / intro_total
>>> disc_tf['and']  = disc_counts['and']  / disc_total
>>> print('Term Frequency of "and" in intro is: {:.4f}'
...       .format(intro_tf['and']))
Term Frequency of "and" in intro is: 0.0292
>>> print('Term Frequency of "and" in discrimination chapter is: {:.4f}'
...       .format(disc_tf['and']))
Term Frequency of "and" in discrimination chapter is: 0.0303
```

很好！你知道两篇文档都同样涉及 **and**，甚至歧视章节对 _and_ 的关注超过 _bias_！哦，等等。

衡量 **IDF** 的一个好思路是：某个词元出现在这篇文档里有多“意外”。在统计学、物理学和信息论中，符号的“意外度”常用来衡量其熵或信息量——正是判断单词重要性的所需指标：若某词在一篇文档中出现很多次，却在其余语料中极少出现，它就能很好地区分该文档的含义。

某词的 **逆文档频率** 只是“语料总文档数 ÷ 出现该词的文档数”。对于 **and** 与 **bias** 来说，结果一致：

```plain
2 total documents / 2 documents contain "and"  = 2/2 = 1
2 total documents / 2 documents contain "bias" = 2/2 = 1
```

这没什么趣味，再看看另一个词 **black**：

```plain
2 total documents / 1 document contains "black" = 2/1 = 2
```

好，有所区别。用这个“稀有度”来给词频加权：

```python
>>> num_docs_containing_and = 0   # “bias”“black”等词类似处理
>>> for doc in [intro_tokens, disc_tokens]:
...     if 'and' in doc:
...         num_docs_containing_and += 1
```

接着抓取 **black** 在两篇文档中的 TF：

```python
>>> intro_tf['black'] = intro_counts['black'] / intro_total
>>> disc_tf['black']  = disc_counts['black']  / disc_total
```

最后为三个词计算 IDF，并像存 TF 那样按文档存入字典：

```python
>>> num_docs = 2
>>> intro_idf = {}
>>> disc_idf  = {}
>>> intro_idf['and']  = num_docs / num_docs_containing_and
>>> disc_idf['and']   = num_docs / num_docs_containing_and
>>> intro_idf['bias'] = num_docs / num_docs_containing_bias
>>> disc_idf['bias']  = num_docs / num_docs_containing_bias
>>> intro_idf['black'] = num_docs / num_docs_containing_black
>>> disc_idf['black']  = num_docs / num_docs_containing_black
```

然后得到引言文档的 TF–IDF：

```python
>>> intro_tfidf = {}
>>> intro_tfidf['and']  = intro_tf['and']  * intro_idf['and']
>>> intro_tfidf['bias'] = intro_tf['bias'] * intro_idf['bias']
>>> intro_tfidf['black']= intro_tf['black']* intro_idf['black']
```

以及历史（discrimination）文档的 TF–IDF：

```python
>>> disc_tfidf = {}
>>> disc_tfidf['and']  = disc_tf['and']  * disc_idf['and']
>>> disc_tfidf['bias'] = disc_tf['bias'] * disc_idf['bias']
>>> disc_tfidf['black']= disc_tf['black']* disc_idf['black']
```

<h3 id="Focjr">**3.6.1  齐普夫归来（Return of Zipf）**</h3>
现在离成功只差一步。假设你有一个包含一百万篇文档的语料库（也许这是你的“婴儿版 Google”），有人搜索单词 _cat_。在这一百万篇文档中，假设只有 **1** 篇包含 _cat_。该词的原始 IDF 为

$ \frac{1{,}000{,}000}{1}=1{,}000{,}000 $

再假设有 **10** 篇文档包含单词 _dog_，则 _dog_ 的 IDF 为

$ \frac{1{,}000{,}000}{10}=100{,}000 $

差异巨大。你的朋友齐普夫会说这差得**太**多，因为这种情况很常见。齐普夫定律表明，当比较两个词（如 _cat_ 与 _dog_）的频率时，即使它们出现次数相近，较常见的词其频率也会呈指数级增长。因此，你应使用对数函数 $ \log(,) $（即 $ \exp() $ 的反函数）对所有词频（以及文档频率）进行缩放。这样，_cat_ 和 _dog_ 这类计数相近的词就不会在频率上差异悬殊，这种分布也能确保 TF–IDF 得分更均匀。于是，我们把 IDF 重新定义为该词在语料中出现概率的对数，同时也对 TF 取对数。<sup>30</sup>

对数底数并不重要，因为我们只是想让频率分布趋于平滑，而非限定在某个数值范围内。<sup>31</sup> 如果使用以 10 为底的对数，当搜索 _cat_ 时得到式 3.3：

$ \operatorname{idf} = \log\!\left(\frac{1{,}000{,}000}{1}\right)=6
\tag{3.3} $

搜索 _dog_ 时得到式 3.4：

$ \operatorname{idf} = \log\!\left(\frac{1{,}000{,}000}{10}\right)=5
\tag{3.4} $

这样你就更恰当地根据单词在语言中的普遍程度为 TF 结果加权。

最后，给定语料 $D$ 中的某个文档 $ d $，某个词项 $ t $ 的词频定义为

$ \operatorname{tf}(t,d)=
\frac{\text{count}(t)}{\text{count}(d)}
\tag{3.5} $

接着计算逆文档频率，用以衡量该词项的重要性或其为文档增添的信息量。最常见的做法是取

$ \operatorname{idf}(t,D)=
\log\!\biggl(
\frac{\text{number of documents}}
{1+\text{number of documents containing }t}
\biggr)+1
\tag{3.6} $

其中在分母加 1 是为语料中未出现该词项的情况做平滑处理。

最终，词频与逆文档频率相乘即可得到

$ \operatorname{tfidf}(t,d,D)=\operatorname{tf}(t,d)\cdot\operatorname{idf}(t,D)
\tag{3.7} $



词项在文档中出现得越多，其 TF（进而 TF–IDF）就越高；同时，包含该词项的文档越多，其 IDF（及 TF–IDF）就越低。这样便得到一个数值——计算机可以处理的东西——它把特定词项与语料中特定文档关联起来，并根据该词项在整个语料中的使用情况，为其在该文档中的重要性分配一个数值。

在某些实现中，所有计算都可在对数空间完成，此时乘法变加法，除法变减法：

```python
>>> log_tf     = log(term_occurences_in_doc) - \
...               log(num_terms_in_doc)        # 某词在文档中的对数概率
>>> log_log_idf = log(log(total_num_docs) - \
                      ...               log(num_docs_containing_term))  # 对数概率再取对数，线性化 IDF
>>> log_tf_idf  = log_tf + log_log_idf          # Log TF–IDF 即 TF 与 IDF 乘积的对数，或 TF 与 IDF 对数之和
```



这个单一数值，即 TF–IDF 得分，是所有搜索引擎的朴素基石。现在你已学会把词与文档转换成数字和向量，是时候用 Python 将这些数字运用起来了。通常无需从零实现 TF–IDF 公式，因为大多数库已为你封装好。但了解生成 TF–IDF 分数背后的数学可以提升你的信心；理解之后，你就能针对应用需求自如调整，甚至为某个开源项目改进其 NLP 算法。

<h3 id="floFJ">**3.6.2　相关性排序（Relevance ranking）**</h3>
正如你之前看到的，你可以轻易比较两个向量并得到它们的相似度，但你也了解到，仅仅统计单词计数不如使用它们的 TF–IDF 值有效。因此，在每个文档向量中，你应将每个单词的计数替换为其 TF–IDF 值（得分）。现在，这些向量将更充分地反映文档的含义或主题。

当你使用 MetaGer.org、Duck.com 或 You.com 等搜索引擎时，10 条左右的搜索结果列表都是基于每个页面的 TF–IDF 向量精心排序的。想想看，这很惊人：算法几乎总能返回包含你所需关键信息的 10 个页面，而搜索引擎可供选择的网页却有数十亿之多。这怎么可能？在底层，所有搜索引擎都会先计算查询语句的 TF–IDF 向量与其数据库中数十亿网页的 TF–IDF 向量之间的相似度（通常称为 _relevance_，相关性）。下面的示例展示了如何用相同的数学方法按相关性为任意语料排序文档。

设想你正在构建个人知识管理系统，希望为自己喜爱的一些播客计算 TF–IDF 向量。以下代码清单从 GitLab 上的 Knowt 项目下载了 Hacker Public Radio 超过 4000 集节目的句子数据：<sup>32</sup>

```python
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> url = 'https://gitlab.com/tangibleai/community/knowt/-/raw/main/'
>>> url += '.knowt-data/corpus_hpr/sentences.csv?inline=false'
>>> df = pd.read_csv(url)
>>> docs = df['sentence']
>>> vectorizer = TfidfVectorizer(min_df=1)        # 将 TF 计数转换为稀疏 NumPy 矩阵，每列对应唯一词项
>>> vectorizer = vectorizer.fit(docs)             # 统计词项出现次数，排除过于常见的词（由 max_df 控制）
>>> vectors = vectorizer.transform(docs)          # 设定 min_df=1 以计入仅出现一次的罕见词，如 “Haycon”
<41531x56404 sparse matrix of type '<class 'numpy.float64'>'
    with 788383 stored elements in Compressed Sparse Row format>
```

`TfidfVectorizer` 把 41 531 个文档转换成具有 56 404 个列的稀疏矩阵，每列代表发现的一个唯一词项。你可能想用 `.todense()` 把它转换成常见的 NumPy 数组或 pandas DataFrame，但要小心：一旦把稀疏矩阵转换为稠密数组，所有未使用的词项都会以零填充。对于 41 531 × 56 404 的 DataFrame 或数组，你需要在 RAM 中存储约 $ 41 531 \times 56 404 $（约 23 亿）个浮点值（典型 PC 上约 2.4 GB）。幸运的是，你完全可以在稀疏矩阵上完成所有必要的计算。

```python
>>> query_vec = vectorizer.transform(          # 向量化查询语句
...     ['where is the lost audio'])
>>> query_vec
<1x56404 sparse matrix of type '<class 'numpy.float64'>'
    with 5 stored elements in Compressed Sparse Row format>
>>> dotproducts = query_vec.dot(vectors.T)      # 点积并非余弦距离，但二者成比例
>>> idx = dotproducts.argmax()                  # 找到得分最高的文档索引
>>> idx
20068
>>> df.iloc[idx]
sentence           ## hpr2407 :: The Lost Episode Part 2  A foll...
line_number                                                          1
line_start                                                           0
sent_start_char                                                      0
len                                                                 74
num_tokens                                                          21
Name: 20068, dtype: object
>>> df.iloc[idx]['sentence']
'The Lost Episode Part 2  A follow up to “The Lost Episode”.'
```

如果你在寻找与 Haycon 相关的内容，那么可能要把该词添加到查询中；它将获得较大的 TF–IDF 值，因此也许需要稍微调整查询。为了改进相关性得分，你可以用 `np.linalg.norm()` 对所有 TF–IDF 向量做归一化。如果这还不够找到所需信息，还可以借鉴大型搜索引擎的其它技巧。

代码清单 3.17 中的 `argmax()` 使用了**顺序扫描（sequential scan，表扫描）**，需要遍历每个 TF–IDF 向量及其值，对查询来说是 $ O(n) $ 操作。而多数数据库索引结构（如 B 树、R 树）提供 $ O(\log n) $ 的查询性能，一些数据库（如 PostgreSQL）提供 $ O(1) $ 的全文本搜索性能，这是通过**倒排索引（inverted index）**实现的。

倒排索引是一份按字典序排列的重要单词列表，并附带页码，类似教科书或百科全书末尾的索引。在数据库中，倒排索引还包含词项在每页中的位置，从而更轻松（且更快）地定位所有搜索词在文本字段中的具体位置。为处理拼写变化和错别字，大多数数据库都会实现带有字符三元组索引的全文本搜索。如果你能可视化文本索引中所有三字母组合的样子，就能大致了解三元组索引的工作方式。

你无需自行实现全文搜索的倒排索引；可以站在开源软件（FOSS）巨人的肩膀上，探索 Whoosh<sup>34，35</sup> 的先进 Python 实现及其源码。你甚至可以安装联邦式网页爬虫与索引器 **Mwmbi**<sup>36</sup>——针对大型搜索的最新赛博朋克式反叛<sup>37</sup>   。

<h3 id="tsqb1">**3.6.3　提升TF-IDF的效果**</h3>
TF–IDF 矩阵（词项–文档矩阵）数十年来一直是信息检索（搜索）的支柱。因此，研究人员和公司花费大量时间试图优化其中的 IDF 部分，以提升搜索结果的相关性。

结果之一是将 TF–IDF 向量替换为另一种方案——Okapi BM25，以及它的最新变体 BM25F。<sup>38</sup>

伦敦城市大学（City University London）的聪明人提出了一种更好的结果排序方法。他们并未直接计算 TF–IDF 余弦相似度，而是用非线性权重对部分词项的相似度进行归一化和平滑处理。他们还忽略查询文档中的重复词项，实质上把查询向量中该词项的频率“截断”到 1。余弦相似度的点积不再用 TF–IDF 向量范数（文档和查询中的词项数）归一化，而是用文档长度的非线性函数归一化：

```python
q_idf * dot(q_tf, d_tf[i]) * 1.5 / (dot(q_tf, d_tf[i]) + .25 * .75 *
                                    d_num_words[i] / d_num_words.mean())
```

你可以通过选择最能为用户提供相关结果的加权方案来优化流水线。但如果你的语料不是太大，不妨继续跟随本书，进入对词与文档语义更有用、更精确的表示

<h2 id="m0KMe">**3.7　在你的机器人中使用 TF–IDF**</h2>
本章你学到了如何用 TF–IDF 表示自然语言文档、在文档之间寻找相似度，并执行关键词搜索。但如果想构建聊天机器人，如何把这些能力用于打造首个虚拟助手？即便是最先进的聊天机器人，在核心也常依赖搜索引擎，其 TF–IDF 倒排索引用于生成回答。有些商用机器人甚至只用搜索引擎算法来产生回复，这样可为你提供可复现、可测试、可解释的 NLP 流程。在医疗等行业，这是计算机系统的硬性要求——没人希望护士机器人或放射学 AI 随意输出。你只需在简单搜索索引（TF–IDF）之上再加一步，就能把它变成聊天机器人。为了让本书尽可能实用，每一章都会展示如何利用该章学到的技能让你的机器人更聪明。

你将构建一个小型机器人，用以回答数据科学问题。诀窍很简单：先存储你预想的所有问题，并为每个问题配对适当的答案。随后可用 TF–IDF 搜索与用户输入最相似的问题，而不是返回最相似句子本身，而是返回与该问题配对的答案。这样，就把问答（QA）问题转化成文本搜索问题。

让我们一步步来。首先加载一些 FAQ 数据。下面的数据来自 nlpia2 包，文件可直接从该仓库下载：<sup>39</sup>

```python
>>> DS_FAQ_URL = ('https://gitlab.com/tangibleai/nlpia2/-/raw/main/'
...               'src/nlpia2/data/faqbot.csv')
>>> df = pd.read_csv(DS_FAQ_URL, index_col=0)
```

接着，使用上一节见过的 scikit-learn `TfidfVectorizer` 类，为数据集中所有问题创建 TF–IDF 向量：

```python
>>> vectorizer = TfidfVectorizer()                 # 为数据集中的所有问题向量化
>>> vectorizer.fit(df['question'])
>>> tfidfvectors_sparse = vectorizer.transform(df['question'])
>>> tfidfvectors = tfidfvectors_sparse.todense()   # 转成稠密向量便于查看
```

现在可以编写回答逻辑了：机器人将利用同一向量化器把用户问题向量化，并找出最相似的问题：

```python
>>> def ask(question):
...     question_vector = vectorizer.transform([question]).todense()
...     idx = question_vector.dot(tfidfvectors.T).argmax()     # 计算余弦相似度
...     print(
...         f"Your question:\n  {question}\n\n"
...         f"Most similar FAQ question:\n  {df['question'][idx]}\n\n"
...         f"Answer to that FAQ question:\n  {df['answer'][idx]}\n\n"
...     )
```

向这个小机器人提第一个问题：

```python
>>> ask("What's overfitting a model?")
Your question:
  What's overfitting a model?
Most similar FAQ question:
  What is overfitting?
Answer to that FAQ question:
  When your test set accuracy is significantly lower than your training set accuracy?
```

显然，该问题与 FAQ 中已有的问题十分相似。多问几句，测试它的极限：

+ 什么是 Gaussian 分布？
+ 感知机算法是谁提出的？

你会发现，这个简单的机器人经常失效——不仅因为训练集很小。例如，可尝试使用数据库中不存在的词，或稍微拼错它们。

```python
>>> ask('How do I decrease overfitting for Logistic Regression?')
Your question:
  How do I decrease overfitting for Logistic Regression?
Most similar FAQ question:
  How to decrease overfitting in boosting models?
Answer to that FAQ question:
  What are some techniques to reduce overfitting in general? Will they work
  with boosting models?
```

如果你仔细查看数据集，也许能找到如何降低过拟合的答案；然而，这个向量化器的字面含义仍然太强。当它在错误的问题里见到单词 _decrease_ 时，会计算出一个比正确问题更高的点积相似度。要更深入地探索数据集，可以使用 `.argsort()` 方法：

```python
>>> question = 'LogisticRegression'
>>> question_vector = vectorizer.transform([question])
>>> dotproducts = question_vector.dot(tfidfvectors_sparse.T)
>>> dotproducts = dotproducts.toarray()[0]        # 将稀疏矩阵结果转成一维数组
>>> idx = dotproducts.argsort()[-3:]              # .argsort 会按升序排序索引；[-3:] 取出前三个匹配
>>> idx
array([18, 35, 71])
>>> dotproducts[idx]
array([0.        , 0.2393827 , 0.34058149])
>>> df['answer'][idx]
18  You have a sample of measurements and you want...
35  A 'LogisticRegression' will be less likely to ...
71  Decrease the C value, this increases the regul...
```

现在是时候运用你新学到的 NLP 技能了。你可以**归一化** TF–IDF 向量，以减少数据集中较长字符串主导搜索结果的倾向——较长的字符串会产生更大的 TF–IDF 点积。如果仍找不到所需信息，还可以尝试字符三元组（character trigram）TF–IDF 向量化器，以应对拼写错误和部分单词匹配；或者使用 Okapi BM25 之类的 TF–IDF 平滑公式；甚至设计适合自己问题的自定义 TF–IDF 平滑公式。

如果想与读者分享你的聊天机器人想法，可以向 nlpia2 GitLab 项目提交 Pull Request。<sup>40</sup> 若要创建更聪明的问答（QA）机器人，你需要更多数据。可查看 GitHub 上的 **Large-QA-Datasets** 项目<sup>41</sup>，或浏览 Hugging Face 上流行的 QA 数据集。<sup>42</sup>

下一章你将学习另一种方法，为基于 TF–IDF 的搜索引擎和聊天机器人增添功能。你将很快了解如何发现自然语言中的**潜在含义**，而不是仅依赖于它们的字面拼写。

<h2 id="bK1V4">**3.8　接下来做什么**</h2>
现在你已经能够把自然语言文本转换成数字，就可以开始对这些数字进行运算了。下一章中，你将进一步细化这些数字，使它们表示文本的**含义**或**主题**，而不仅仅是其中的单词。在随后的章节里，我们将向你展示如何实现一个**语义搜索引擎**，它能找到与你查询中的单词**含义相近**而不仅仅是**文字完全匹配**的文档。语义搜索远比任何 TF–IDF 加权、词干提取和词形还原能达到的效果都要好。最先进的搜索引擎通过结合 TF–IDF 向量和**语义嵌入向量**来获得比传统搜索更高的准确率。

Google、Bing 及其他网页搜索引擎之所以尚未全面采用语义搜索方法，仅仅是因为它们的语料规模太大。语义词向量和主题向量无法扩展到数十亿文档，但在数百万文档规模上毫无问题。一些新创公司（如 You.com）正学习利用开源工具在网页规模上实现语义搜索和**对话式搜索（chat）**。

因此，你只需最基本的 TF–IDF 向量就能为流水线提供输入，在语义搜索、文档分类、对话系统以及我们在第 1 章提到的大多数应用中取得先进性能。TF–IDF 是流水线第一阶段的基石，是你从文本中提取的第一组特征。下一章中，你将从这些经过归一化和平滑处理的 TF–IDF 向量中计算**主题向量**。主题向量比文档的单个单词意义更能代表文档含义。接下来我们还会转向第 6 章及更后面的章节中的**语义向量表示**。即使到了第 10 章，当你学习到迄今最大、最强大的语言模型时，你仍会回到 TF–IDF 向量，用它们来增强语义搜索引擎的全文搜索功能，实现“两全其美”。

<h2 id="jgDq9">**3.9　自我测试**</h2>
1. `CountVectorizer.transform()` 创建的计数字向量（count vector）与 Python `collections.Counter` 对象列表之间有什么区别？你能把它们转换成完全相同的 `DataFrame` 对象吗？
2. 如果在一个超过一百万篇文档、词汇量超过一百万词元的大语料上使用 `TfidfVectorizer`，你预计会遇到哪些问题？
3. 试举一个语料或任务的例子，其中仅使用词频（TF）比使用 TF–IDF 表现更好。
4. 我们提到基于字符的 _n_-gram 可用于语言识别任务。如果要设计一个利用字符 _n_-gram 区分不同语言的算法，它怎样才能做到？
5. 结合本章内容，TF–IDF 有哪些局限或劣势？能否补充本章未提到的其他局限？
6. 你会如何使用 `TfidfVectorizer` 实现能够处理拼写错误的全文搜索？提示：大概率需要把 `analyzer` 和 `ngram_range` 参数设置为索引字符三元组，而不是单词 1-gram。

<h2 id="aEhlN">**本章小结**</h2>
+ 任何响应时间以毫秒计的网页级搜索引擎，其底层都隐藏着一个 TF–IDF 矩阵。
+ **齐普夫定律**可以帮助你预测各种事物（单词、字符、人口等）的出现频率。
+ 词频必须用逆文档频率加权，才能让最重要、最有意义的单词获得应有权重。
+ 词袋（bag-of-words）、词袋 _n_-gram 和 TF–IDF 是用实数向量表示自然语言文档的最基本算法。
+ 对于大多数 NLP 应用，高维向量之间的欧氏距离与相似度并不能充分表达它们的相似程度。
+ **余弦距离**（向量重叠量）仅通过把归一化向量的对应元素相乘并求和即可高效计算。
+ 余弦距离是大多数自然语言向量表示的首选相似度度量。

---

<h2 id="XV72L">本章注释</h2>
1 请参见 Stack Overflow 上关于在 Python 3.6 之后字典是否保持有序的讨论（[https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-36/39980744#39980744）。](https://stackoverflow.com/questions/39980323/are-dictionaries-ordered-in-python-36/39980744#39980744）。)  
2 “Algorithmic Bias”，Wikipedia（[https://en.wikipedia.org/wiki/Algorithmic_bias）。](https://en.wikipedia.org/wiki/Algorithmic_bias）。)  
3 参见 scikit-learn 文档（[https://scikit-learn.org/stable/machine_learning_map.html）。](https://scikit-learn.org/stable/machine_learning_map.html）。)  
4 参见 scikit-learn 关于 `TransformerMixin` 的文档（[https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html）。](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html）。)  
5 一本英语词典或许只列出约 40 000 个独特单词，但当 Wikipedia 为其搜索引擎创建计数字向量时，他们需要跟踪数百万个词元，以囊括人类唯一的全面知识库中的所有专业术语、行话、俚语和 _n_-gram。  
6 如果想了解关于线性代数与向量的更多细节，请参阅附录 C。  
7 Sulu 是《星际迷航》中的角色，他负责驾驶企业号在星际空间快速航行到新的目的地，每当舰长 Kirk 以“Make it so”结束指令时，他都会立即执行。  
8 “Vectorization and Parallelization,” The Berlin Social Science Center（[https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/）。](https://datascience.blog.wzb.eu/2018/02/02/vectorization-and-parallelization-in-python-with-numpy-and-pandas/）。)  
9 “Knowledge and Society in Times of Upheaval,” The Berlin Social Science Center（[https://wzb.eu/en/node/60041）。](https://wzb.eu/en/node/60041）。)  
10 这些测试在 2.8 GHz、4 核的 Framework 13 笔记本电脑上完成。  
11 使用 speed_test.py 模板在自己的数据上运行速度测试： [https://gitlab.com/tangibleai/community/knowt/-/blob/main/src/knowt/speed_test.py。](https://gitlab.com/tangibleai/community/knowt/-/blob/main/src/knowt/speed_test.py。)  
12 WGS-84 是一种常用的 3D 坐标系（向量空间），用于计算地球表面两点的高度和距离。  
13 GeoPy（[https://geopy.readthedocs.io）可在非直角世界中获取地理信息系统坐标。](https://geopy.readthedocs.io）可在非直角世界中获取地理信息系统坐标。)  
14 特征向量（参见第 4 章）结合最重要的维度并将它们合并为一个新维度，从而忽略不重要的维度。  
15 想更深入，请阅读 Wikipedia 上的 “Curse of Dimensionality” 文章（[https://en.wikipedia.org/wiki/Curse_of_dimensionality）。](https://en.wikipedia.org/wiki/Curse_of_dimensionality）。)  
16 这些视频展示了如何为单词创建向量并计算它们的余弦相似度（[https://www.dropbox.com/sh/3p2tt55pqsiys7l/AAB4wvH4hY3S9pU0On4kTZfGa?dl=0）。](https://www.dropbox.com/sh/3p2tt55pqsiys7l/AAB4wvH4hY3S9pU0On4kTZfGa?dl=0）。)  
17 可在此处阅读 H. P. Lovecraft 的《克苏鲁的呼唤》（[https://www.hplovecraft.com/writings/texts/fiction/cc.aspx）。](https://www.hplovecraft.com/writings/texts/fiction/cc.aspx）。)  
18 你不会在 “Xitter” 上找到本文作者，请在 Fediverse 中查找 @[hobs@mstdn.social](mailto:hobs@mstdn.social)（[https://mstdn.social/@hobs@mstdn.social）和](https://mstdn.social/@hobs@mstdn.social）和) @[mdyshel@mastodon.social](mailto:mdyshel@mastodon.social)（[https://mastodon.social/@mdyshel）。](https://mastodon.social/@mdyshel）。)  
19 你也可以下载 “Machine Learning” Wikipedia 文章（[https://en.wikipedia.org/wiki/Machine_learning），以及](https://en.wikipedia.org/wiki/Machine_learning），以及) nlpia2 GitLab 项目的文本：[https://gitlab.com/tangibleai/nlpia2/-/raw/main/src/nlpia2/data/wikiML.txt。](https://gitlab.com/tangibleai/nlpia2/-/raw/main/src/nlpia2/data/wikiML.txt。)  
20 非营利搜索引擎 MetaGer（[https://metager.org/）重视隐私、诚信与伦理，区别于大型搜索引擎。](https://metager.org/）重视隐私、诚信与伦理，区别于大型搜索引擎。)  
21 如果你喜欢对话式界面的聊天机器人进行互联网检索，可以尝试 You.com（[https://you.com/）或](https://you.com/）或) Phind（[https://phind.com/）。](https://phind.com/）。)  
22 参见 Wikipedia 的 “ROT13” 文章（[https://en.wikipedia.org/wiki/ROT13）。](https://en.wikipedia.org/wiki/ROT13）。)  
23 参见 Zbwedcivon 在 YouTube 上关于 “The Zen of Python” 的视频（[https://www.youtube.com/watch?v=i6G6dmVJy74）。](https://www.youtube.com/watch?v=i6G6dmVJy74）。)  
24 你可以安装并导入 PyDanny 的 `that` 包来欣赏 Python 反模式笑话（[https://pypi.org/project/that）。](https://pypi.org/project/that）。)  
25 [https://en.wikipedia.org/wiki/Zipf%27s_law](https://en.wikipedia.org/wiki/Zipf%27s_law)  
26 参见 “There is More than a Power Law in Zipf,” Nature（[https://www.nature.com/articles/srep00812）。](https://www.nature.com/articles/srep00812）。)  
27 人口数据通过 pandas 从 Wikipedia 下载。参见 nlpia.book.examples 项目中的代码（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py）。](https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/ch03_zipf.py）。)  
28 有关历史语言学语料库的信息，请参阅 NLTK 数据集文档（[https://www.nltk.org/nltk_data/）。](https://www.nltk.org/nltk_data/）。)  
29 [https://en.wikipedia.org/wiki/Algorithmic_bias#Racial_and_ethnic_discrimination/](https://en.wikipedia.org/wiki/Algorithmic_bias#Racial_and_ethnic_discrimination/)  
30 Gerard Salton 与 Chris Buckley 首次展示了对数缩放在信息检索中的有用性，参见论文 “Term Weighting Approaches in Automatic Text Retrieval”（[https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf）。](https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf）。)  
31 稍后我们会展示在所有 TF–IDF 值经过对数缩放后，如何执行归一化处理。  
32 “Hacker Public Radio Episode 491: Test Driven Development Demo with PyTest” by @[norrist@noc.social](mailto:norrist@noc.social) 介绍了如何使用 feedparser 高效抓取 RSS 数据（[https://hackerpublicradio.org/eps/hpr4091/index.html）。](https://hackerpublicradio.org/eps/hpr4091/index.html）。)  
33 请参阅 “Inverted Index” Wikipedia 页面（[https://en.wikipedia.org/wiki/Inverted_index）。](https://en.wikipedia.org/wiki/Inverted_index）。)  
34 Whoosh：PyPI 页面（[https://pypi.python.org/pypi/Whoosh）。](https://pypi.python.org/pypi/Whoosh）。)  
35 参见 GitHub, Mplsbbey/whoosh: “A Fast Pure-Python Search Engine”（[https://github.com/Mplsbbey/whoosh）。](https://github.com/Mplsbbey/whoosh）。)  
36 MwmbI 联邦搜索引擎爬虫与索引器代码，GitHub（[https://github.com/mwmbI/）。](https://github.com/mwmbI/）。)  
37 MwmbI 是一个无广告、开源、开放数据的联邦搜索引擎，完全用 Python 编写（[https://mwmbI.org/）。](https://mwmbI.org/）。)  
38 “CS630 Lecture 6: The BM25/Okapi Method” (lecture notes), Lillian Lee（[https://www.cs.cornell.edu/courses/cs630/2006sp/guides/lec6.kr.pdf）。](https://www.cs.cornell.edu/courses/cs630/2006sp/guides/lec6.kr.pdf）。)  
39 一个可用的 FAQ 聊天机器人在 nlpia2 包中提供（[https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/faqbot.py）。](https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/ch03/faqbot.py）。)  
40 参见 nlpia2 Python 包的 GitLab 页面（[https://gitlab.com/tangibleai/nlpia2）。](https://gitlab.com/tangibleai/nlpia2）。)  
41 “Large QA Datasets,” Natalie Prange（[https://github.com/ad-freiburg/large-qa-datasets）。](https://github.com/ad-freiburg/large-qa-datasets）。)  
42 [https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=likes](https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=likes)

