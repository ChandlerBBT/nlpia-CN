:::color1
**本章内容**

+ 理解词嵌入(word embeddings)或词向量(word vectors)
+ 用向量表示意义
+ 定制词嵌入以创建领域特定的词嵌入
+ 使用词嵌入进行推理
+ 可视化词语意义

:::

词嵌入(word embeddings)或许是你自然语言处理(NLP)工具箱中最易上手且最常用的工具。它们可以为你的NLP流程提供对词语的整体认知。在本章中，你将学习如何将词嵌入应用于真实场景。更重要的是，你还会了解在何种情况下不宜使用词嵌入。希望这些示例能帮助你在商业和个人生活中构想出新颖有趣的应用。

你可以将词向量(word vectors)稍作类比，想象成角色扮演游戏(RPG)中角色或 Dota 2 英雄的属性列表。现在设想这些角色卡或档案中没有任何文字描述。你需要以一致的顺序保存所有数字属性，这样你才能知道每个数字代表什么。这就是词向量的工作原理。数字并未贴上含义标签，而是被放在向量中固定的槽(slot)或位置。这样，当你将两个词向量相加、相减或相乘时，一者中的 strength（力量）槽就会与另一者中的 strength 槽对齐——对于 agility（敏捷）、intelligence（智力）、alignment（阵营）或 philosophy（哲学）等属性也是如此。

深思熟虑的角色扮演游戏通常会鼓励对哲学和人格的更深层思考，例如 chaotic good（混乱善良）或 lawful evil（守序邪恶）。幸运的是，Hobson 的地下城(Dungeon)主持人为他打开了眼界，让他意识到诸如 good（善）和 evil（恶）等词语所暗示的二元对立的虚假性，这也帮助他欣赏到词语和词向量的模糊性。你将在本章学到的词向量能够容纳几乎任何文本和任何语言中可量化的词语属性，而且这些向量属性或特征相互交织，能够轻松处理 lawful evil（守序邪恶）、benevolent dictator（仁慈独裁者）和 altruistic spite（利他怨恨）等复杂概念。

学习词嵌入通常被归类为表征学习(representation learning)算法<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">1</font>**</sup>。任何词嵌入的目标都是构建对词语“特征(character)”的紧凑数值表示。这些数值表示能够让计算机以有意义的方式处理你的词语（或你的 Dota 2 角色卡）。

<h2 id="dD62S">6.1 你的大脑“处理”词语</h2>
词嵌入是用来表示含义的向量，而你的大脑则是存储含义的所在。你的大脑“处理”词语——它会受到词语的影响。正如化学物质影响大脑一样，词语亦如此。“This is your brain on drugs”是 80 年代反毒电视广告中的一句流行口号，画面是一对鸡蛋在煎锅中滋滋作响<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">2</font>**</sup>。

幸运的是，词语比化学物质温和且更具积极影响。图 6.1 所示的“词语中的大脑”示意图看起来与鸡蛋在煎锅中滋滋作响大不相同。该草图为你提供了一种想象方式：当你阅读一句话时，你的大脑中对应的神经元会被激发并产生思维。你的大脑通过向与词语关联的相邻神经元发送信号来连接这些词语的含义。词嵌入是这些词语之间连接的向量表示，因此它们也是你大脑中神经元连接网络节点嵌入(node embeddings)的一种粗糙表现<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">3</font>**</sup>。

![图6.1 大脑中的词嵌入](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748178871301-67dfb840-3de0-49ac-bfc0-33e7c6bfb041.png)

你可以将词嵌入看作一种向量表示，反映你在思考某一词语时，大脑中神经元的激活模式。每当你思考一个词语时，思绪就在你的大脑中引发一系列电荷波动和化学反应，这些反应始于与你所思考词语相关的神经元。大脑中的神经元会像在池塘中掷下石子激起的涟漪一样成波状触发，但这些电信号只沿着某些神经元流动，而非全部。

随着你阅读本段文字，你正在你的神经元中激发一次次闪光，就如图 6.1 中的示意一样。实际上，研究人员已经发现，当你思考词语时，大脑中神经元活动模式与人工神经网络中用于生成词嵌入的权重模式之间存在令人惊讶的相似性<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">4,5,6</font>**</sup>。

用于创建词嵌入 (word embeddings) 的算法是一种自监督机器学习算法 (self-supervised machine learning algorithm)。这意味着你无需为算法提供字典或同义词库；你只需大量文本，算法便会从文本中词语之间的关联中学习。本章后面，你只需收集一堆维基百科文章作为训练集。随着人类创造的文本数据量越来越大，基于这些数据训练出的嵌入质量也随之提升。

这里再给你一个“词语中的大脑”思考示例。词语不仅影响你的思维方式，还会影响你的沟通方式。而你有点像集体意识——社会大脑中的一个神经元。单词 sorta 对我们而言是一个特别强大的神经连接模式，因为我们在 Daniel Dennett 的《Intuition Pumps》一书中学到了它的含义<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">7</font>**</sup>。

在其著作中，Daniel Dennett 讨论了“无理解的能力”(“competence without comprehension”)这一强大概念。达尔文使用该概念来解释语言理解大脑如何通过简单机制从单细胞生物进化而来。在他那个时代，复杂生物体（如有机体）无需智能设计者即可存在的想法具有革命性意义。艾伦·图灵基于相同概念，展示了复杂计算如何拆解为简单机械操作，并由不理解算术概念的机器执行。这也为理解词嵌入提供了另一种思路——计算机无需“理解”一个词，就能对该词执行复杂操作，如语义搜索或语言生成。由此我们不禁要问：词嵌入究竟有何用处？

<h2 id="dKCpS">6.2 应用</h2>
好了，那么这些超棒的词嵌入(word embeddings)能派上什么用场？只要需要让机器理解词语或短 n-gram（n-gram）的场景，都可以使用词嵌入：

+ 用于职位、网页等的语义搜索(semantic search)
+ 找回“就在嘴边”的词语提示(tip-of-your-tongue word finders)
+ 重写标题或句子
+ 情感塑造(sentiment shaping)
+ 回答词语类比问题(answering word analogy questions)
+ 用词语和名称进行推理(reasoning with words and names)

在学术界，研究人员利用词嵌入解决 200 多种自然语言处理问题<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">8</font>**</sup>：

+ 词性标注(part-of-speech tagging)
+ 命名实体识别(named entity recognition)
+ 类比查询(analogy querying)
+ 相似度查询(similarity querying)
+ 音译(transliteration)
+ 依存句法分析(dependency parsing)

如今，词嵌入背后的相同原理已被用于嵌入整个句子和段落，使这种嵌入方法更加实用和强大。

<h3 id="IwJaO">6.2.1 意义搜索(Search for meaning)</h3>
在二十年前，搜索引擎会根据你输入词语在网页中的词频–逆文档频率(tf–idf, term frequency–inverse document frequency)得分来找所有匹配的词项，并且优秀的搜索引擎会用同义词扩展你的搜索词。有时，搜索引擎甚至会改写你的词语来猜测你真正想要查找的内容。例如，当你搜索 sailing cat 时，它可能会将 cat 改为 catamaran 以帮助你消歧；在结果排序时，搜索引擎甚至会把 positive sum game 改为 nonzero sum game 以将你引导到正确的维基百科页面。

后来，信息检索研究者发现在潜在语义分析(latent semantic analysis, LSA)上更为有效的方法——词嵌入(word embeddings)。这些新的词嵌入(向量, vectors)使搜索引擎能够直接匹配查询语句的“含义”与网页，而无需猜测用户意图。你的搜索词嵌入提供了一个针对搜索意图(intent)的直接数值表示，该表示基于互联网上这些词语平均含义的向量。

:::color4
**重要**  
词嵌入（有时称为词向量）是对词义（包括字面和隐含含义）的高维数值向量表示。在由词嵌入定义的空间中，语义上更相近的词，其向量距离也更接近。

:::

搜索引擎无需再基于硬编码规则进行同义词替换、词干提取、词形还原、大小写归一或消歧义。它们直接根据其索引中所有网页的文本创建词嵌入。不幸的是，主流搜索引擎决定把这一新能力用来将词嵌入与产品和广告匹配，而非真实词语。AdWords、iAd 等服务的词嵌入会根据营销商出价的多少来加权，以此从你的原始搜索意图中“劫持”你。换言之，大厂让企业轻而易举地操纵搜索引擎，训练你成为它们的“消费僵尸”。

如果你使用更“诚实”的搜索引擎，例如 Startpage<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">9</font>**</sup>、DISROOT<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">10</font>**</sup> 或 Wolfram Alpha<sup><font style="color:#DF2A3F;background-color:#FBDE28;">11</font></sup>，你会发现它们更可能给出你真正想要的结果。而且，如果你有某些暗网或私人页面和文档，想用作组织或个人生活的知识库，也可以自建一个基于尖端 NLP 的搜索引擎，比如 Elastic Search<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">12</font>**</sup>、Meilisearch<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">13</font>**</sup>、Searx<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">14</font>**</sup>、Apache Solr<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">15</font>**</sup>、Apache Lucene<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">16</font>**</sup>、Qwant<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">17</font>**</sup> 或 Sphinx<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">18</font>**</sup>。即便是 PostgreSQL，也能在全文搜索精度上超越主流搜索引擎。这些语义搜索引擎在底层使用向量检索来查询词嵌入和文档嵌入（向量）数据库。你会惊讶于使用“诚实可靠”搜索引擎时，世界在你眼中会变得多么清晰。

开源的 Python 工具（如 NBoost 和 PyNNDescent）让你能够将词嵌入与最喜爱的 TF–IDF 搜索算法集成<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">19</font>**</sup>；或者，如果你想可扩展地搜索自定义的嵌入和向量，可以使用近似最近邻算法来索引任意你喜欢的向量<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">20</font>**</sup>。

词嵌入（word embeddings）的美妙之处在于，所有你习惯的向量代数计算（vector algebra math），例如计算距离（distance），同样适用于词嵌入——只是此时的“距离”代表的是词语在语义（meaning）上的差异，而非物理距离。而且，这些新嵌入比你在 TF–IDF 向量中习惯的成千上万维（thousands of dimensions）要更加紧凑且富含语义。

你可以利用这种语义距离在词语数据库中搜索所有与你心中职位名称相近的职位头衔。这可能会揭示一些你从未想到的额外职位名称。或者，你的搜索引擎可以设计为向搜索查询中自动补充相关词语，以确保相关职位名称被检索出来。这就类似于一个能够理解词义的自动完成（autocomplete）搜索框，这种技术称为语义搜索（semantic search）：

下面展示如何用词嵌入进行语义搜索（semantic search）——

```bash
# 安装 nessvec
pip install nessvec
```

```python
from nessvec.indexers import Index
index = Index(num_vecs=100_000)     # FastText 词汇表中 1,000,000 个嵌入词向量中的 100,000 个
index.get_nearest("Engineer").round(2)
```

```latex
Engineer     0.00
engineer     0.23
Engineers    0.27
Engineering  0.30
Architect    0.35
engineers    0.36
Technician   0.36
Programmer   0.39
Consultant   0.39
Scientist    0.39
```

你可以看到，查找词嵌入的最近邻有点像在词库（thesaurus）中查词，但它是一个比本地书店或在线词典更“模糊”（fuzzy）且更完整的词库。很快你就会看到如何将该“词库”定制到任意领域。例如，你可以仅用英国（UK）、印度（India）或澳大利亚（Australia）的职位发布进行训练，取决于你感兴趣的地区；或者你可以让它更好地适应硅谷（Silicon Valley）的科技岗位，而不是纽约（New York）的金融和银行岗位。如果你希望它能处理更长的职位名称（如 _software developer_ 或 _NLP engineer_），还可以在训练时加入 2-gram 和 3-gram。

词嵌入的另一个优点是它们具有模糊性（fuzzy）。你可能已经注意到，_engineer_ 的若干近邻是你在词库中不太可能见到的词，而且你可以根据需要不断扩展该列表。因此，如果你想查找软件工程师（software engineer）而非架构师（architect），可以调用 `get_nearest()` 列表，再针对另一个词执行检索，比如 _Programmer_：

```python
>>> index.get_nearest("Developer").round(2)
Developer    -0.00
developer     0.25
Developers    0.25
Programmer    0.33
Software      0.35
developers    0.37
Designer      0.38
Architect     0.39
Publisher     0.39
Development   0.40
```

有趣的是，_Developer_ 这一头衔常常与 _Publisher_ 相关联。在与 Manning Publications 的开发编辑、开发经理，甚至技术开发编辑共事之前，我们或许从未猜到其中原因。就在今天，这些“开发者”才促使我们加紧编写本章内容！

<h3 id="nhm9s">6.2.2 组合词嵌入(Combining word embeddings)</h3>
 词嵌入的另一个优点在于，你可以随心所欲地组合它们，来创造新词！你可以将多个词语的含义相加，尝试找到一个单词，能够同时捕捉到你相加的这两个词语的意义：

```python
>>> chief = (index.data[index.vocab["Chief"]]
...          + index.data[index.vocab["Engineer"]])
>>> index.get_nearest(chief)
```

```latex
Engineer    0.110178
Chief       0.128640
Officer     0.310105
Commander   0.315710
engineer    0.329355
Architect   0.350434
Scientist   0.356390
Assistant   0.356841
Deputy      0.363417
Engineers   0.363686
```

如果你立志成为一名 chief engineer，那么 scientist、architect 和 deputy 等职位很可能也会在你的职业道路上出现。

```python
>>> chief = (index.data[index.vocab["Chief"]]
...          + index.data[index.vocab["Engineer"]])
>>> index.get_nearest(chief)
```

---

提到本章开头的“就在嘴边的词”提示应用，你是否曾试图在只知对某位名人有大致印象的情况下搜索她的名字，比如：

> She invented something to do with physics in Europe in the early 20th century.
>

如果你将这句话输入 Google 或 Bing，可能得不到你想要的直接答案（Marie Curie）。Google 搜索很可能只会给出著名物理学家（包括男性和女性）的列表，你需要翻阅几页才能找到目标。一旦你找到了 Marie Curie，Google 或 Bing 会记录这一行为，下次再搜索科学家时就可能提供更准确的结果。（至少在我们为本书做研究时，确实如此；我们不得不使用私密浏览窗口，以确保搜索结果与你的体验一致。）

利用词嵌入，你可以搜索能够同时组合 “woman”、“Europe”、“physics”、“scientist” 和 “famous” 这些词义的词或名称，从而找到最接近你要找的标记（token）Marie Curie。实现这一点的做法是将这些词的向量相加：

```python
>>> answer_vector = (wv['woman']
...                  + wv['Europe']
...                  + wv['physics']
...                  + wv['scientist'])
```

在本章中，我们将展示如何精确执行此查询。你甚至可以看到如何通过词嵌入向量运算来剔除单词内部的一些性别偏差：

```python
>>> answer_vector = (wv['woman']
...                  + wv['Europe']
...                  + wv['physics']
...                  + wv['scientist']
...                  - wv['male']
...                  - 2 * wv['man'])
```

借助词嵌入，你可以“从 woman 中拿出 man”！

<h3 id="Uqebz">6.2.3 类比问题 (analogy questions)</h3>
如果您可以将您的问题重新表述为一个类比问题，该怎么办？想象您的查询类似于：  
“对核物理而言，谁相当于路易·巴斯德之于病菌？”  
不过，Google Search、Bing 甚至 DuckDuckGo 在这方面帮不上太大忙<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">21</font>**</sup>。但通过词嵌入 (word embeddings)，解决方案简单得多：从“Louis_Pasteur”中减去“germs”，然后再加上一些“physics”：

```python
>>> answer_vector = wv['Louis_Pasteur'] - wv['germs'] + wv['physics']
```

您可能在标准化考试（如 SAT、ACT 或 GRE）的英语类比题部分见过类似的问题。有时，它们会以正式的数学符号形式出现，例如：

```plain
MARIE CURIE : 科学 :: ？ : 音乐
```

这样是否更容易猜出这些词语的向量运算？一种可能的写法是：

```python
>>> wv['Marie_Curie'] - wv['science'] + wv['music']
```

您可以将这种思路应用到人和职业以外的事物——例如，运动队和城市<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">22</font>**</sup>——  
“The Timbers are to Portland as what is to Seattle?”  
在标准化考试的形式中，就是：

```plain
TIMBERS : PORTLAND :: ？ : SEATTLE
```

与前面的例子类似，您可以用这样的数学表达来解答：

```python
wv['Timbers'] - wv['Portland'] + wv['Seattle'] = ?
```

理想情况下，这样的向量运算会给出：

```python
wv['Seattle_Sounders']
```

但更常见的是，标准化考试会使用英语词汇并提出简单些的问题，例如：

```plain
WALK : LEGS :: ？ : MOUTH
```

对于词嵌入来说，这类“舌尖上的问题”都不在话下。词嵌入可以用来回答即使无法以搜索查询或类比形式提出的模糊问题，也能帮助您在词汇中迅速想起任何单词，只要该单词的向量存在即可。（ 对于 Google 预训练的 Word2Vec 模型，只要您的词不是在 2013 年之后才被创造出来，就几乎肯定包含在 Google 用来训练该模型的 1000 亿词新闻语料库中。）即使对于那些无法以搜索查询或类比形式提出的问题，词嵌入依然表现良好。您可以在 6.3 节中了解与嵌入相关的一些数学内容。  

<h3 id="g2EFc">6.2.4 Word2Vec 创新 (Word2Vec innovation)</h3>
我们脑海和大脑神经的连接中，会将经常一起出现的词语堆积在一起，最终定义这些词语的含义。正如幼儿，您会听到人们谈论足球、消防车、计算机和书籍，并逐渐弄清它们各自的含义。令人惊讶的是，您的机器并不需要有机体或大脑，就能像幼儿一样理解词语。

一个孩子在现实世界或图画书中看到某个物体几次后，就能学会一个词。孩子不需要阅读字典或同义词典，就像机器也能在没有字典、同义词典或任何监督学习数据集的情况下自行学习。机器甚至不需要“看到”物体或图片。通过您设置的文本解析方式和数据集，机器完全以自监督方式学习。您所需要的，只有大量文本。

在前面的章节中，您可以忽略一个词的近邻上下文，只需统计其在同一文档中的出现次数即可。如果您将文档做得非常非常短，这些共现计数就能用来表示词语本身的含义。这正是 Tomas Mikolov 和他的 Word2Vec 自然语言处理算法的关键创新。John Rupert Firth 将“词由其所处环境定义”（“a word is characterized by the company it keeps”）的概念推广开来<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">23</font>**</sup>。但要让词嵌入真正发挥作用，需要 Tomas Mikolov 专注于非常小的“词语邻域”以及 21 世纪计算机的计算能力和海量可读文本。您不需要字典或同义词典来训练词嵌入；您只需要大量文本。

这正是本章要做的：像幼儿一样教会机器成为“海绵”，帮助机器弄清词语的含义，而无需明确标注词典定义。您所需要的，只是一堆从任意书籍或网页随机抽取的句子。将这些句子分词并分段（前面章节已学），您的 NLP 管道在读取每批新句子时就会越来越聪明。

在第 2 和第 3 章中，您从隔离的邻居中提取词语，只关注其在每个文档中是否出现。您忽略了邻居对词语含义的影响以及这些关系如何作用于整体语句意义，而我们的词袋 (BOW) 概念将每个文档中的所有词语汇总成一个统计袋。本章中，您将为每个词创建仅包含少数词（通常少于 10 个标记）的更小 BOW，并确保这些邻域有边界，以防止词义跨句蔓延。此过程将帮助您的词嵌入语言模型专注于最密切相关的词语。

词嵌入 (word embeddings) 可以帮助您识别同义词、反义词，或仅仅属于同一类别（如人、动物、地点、植物、名称或概念）的词。在第 4 章中，我们可以使用语义分析实现这一点，但您的更紧凑的词语邻域将在词嵌入上体现为更高的准确性。LSA（潜在语义分析）对词语、n-gram 和文档的处理并未捕捉所有词的字面含义，更遑论隐含意义；LSA 过大的词袋使得词语的某些内涵变得模糊不清。

高密度和适度高维度（但不要过高）的词嵌入既是它们强大之处，也决定了它们的局限。因此，在您的流水线中，将它们与稀疏的超高维 TF–IDF 向量或离散的 BOW 向量结合使用时，将最能体现它们的价值。

<h3 id="PYixg">6.2.5 人工智能依赖于嵌入 (Artificial intelligence relies on embeddings)</h3>
词嵌入在自然语言理解准确性方面迈出了一大步，同时也在对人工通用智能 (AGI) 的希望中实现了突破。您认为自己能分辨出来自机器的智能与非智能消息吗？情况可能没有您想象得那么明显。即便是大科技公司最顶尖的团队，也被 2023 年最先进聊天机器人——Bing 和 Bard——的“无心”回答所蒙蔽。更简单、更真实的对话工具（如 You.com 和 Phind.com）及其聊天界面，在大多数互联网检索任务上均能超越大型搜索引擎。

哲学家 Douglas Hofstadter 指出，在衡量智能时应关注以下几个方面<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">24</font>**</sup>：

+ 灵活性
+ 应对歧义
+ 忽略无关细节
+ 寻找相似性和类比
+ 生成新想法

您很快就会看到词嵌入如何在您的软件中实现这些智能特质。例如，词嵌入通过赋予词语模糊性和细微差别，使得机器人能够灵活应对，而 TF–IDF 向量则无法做到。在之前版本的聊天机器人中，如果您想让机器人在回应常见问候时表现出灵活性，就必须枚举所有可能的“hi”的说法。

但有了词嵌入，您可以用一个向量同时识别“hi”、“hello”和“yo”的含义。您甚至可以为机器人可能遇到的所有概念创建嵌入，只需提供尽可能多的文本即可。无需再手工构建词汇表。

:::color4
WARNING    如同词嵌入一样，智能本身也是一个高维概念。这使得人工通用智能（AGI）成为一个难以捉摸的目标。务必小心，不要让您的用户或上司认为您的聊天机器人具备普遍智能，即使它看起来已实现了霍夫斯塔特（Hofstadter）所说的所有“基本要素”。

:::



<h2 id="ZGcxw">6.3 Word2Vec</h2>
2012 年，微软的实习生 Tomas Mikolov 找到了一种将词义嵌入到向量空间的方法。词嵌入（word embeddings）或词向量（word vectors）通常具有 100 到 500 个维度，具体取决于用于训练的语料库信息量的广度。Mikolov 训练了一个神经网络，以预测每个目标词附近会出现哪些词。Mikolov 使用了单隐藏层网络，因此几乎任何线性机器学习模型也都能胜任。逻辑回归（logistic regression）、截断奇异值分解（truncated SVD）、线性判别分析（linear discriminant analysis）或朴素贝叶斯（Naive Bayes）都能很好地工作，并已被他人成功用于复现 Mikolov 的结果。2013 年，Mikolov 与团队在 Google 发布了用于创建这些词向量的软件，称之为 Word2Vec<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">25</font>**</sup>。

Word2Vec 语言模型仅通过处理大量未经标注的文本语料，就能学习词义。无需人为标注 Word2Vec 词表中的词语，也无需告知算法 Marie Curie 是科学家、Timbers 是足球队、Seattle 是城市或 Portland 同时属于俄勒冈州和缅因州的城市；同样，无需告知其足球是运动、团队是人群、城市既是地点也是社区。Word2Vec 都能自主学习这些知识，且远不止于此！您所需要的，只是一份足够大的语料，其中出现 Marie Curie、Timbers 和 Portland 与“science”、“soccer”或“cities”相关的上下文。

Word2Vec 的这种无监督特性正是其强大之处。我们的世界充斥着未经标注、未分类、无结构的自然语言文本。

无监督学习（unsupervised learning）和监督学习（supervised learning）是机器学习中两个根本不同的方法。在监督学习中，必须由人或一个人团队为数据贴上目标变量的正确标签。相比之下，无监督学习使机器能够直接从数据中学习，而无需任何人为干预。训练数据不需要经过人为组织、结构化或标注。您可以在附录 D 中了解更多关于监督学习和无监督学习的内容。

像 Word2Vec 这样的无监督学习算法非常适合用于自然语言文本。与其试图训练神经网络直接学习目标词语的含义（基于这些含义的标签），不如让网络去预测句子中目标词的邻近词。因此，从某种意义上说，您确实有标签：即您试图预测的邻近词。但由于这些标签来自数据集本身，无需人工标注，Word2Vec 的训练算法毫无疑问是一种无监督学习算法。

预测本身并不是 Word2Vec 发挥作用的原因——它只是达到目的的一种手段。您真正关心的是内部表示（即向量），Word2Vec 会逐步构建该向量，以帮助它生成这些预测。该表示将比第 4 章中的潜在语义分析（LSA）和潜在狄利克雷分配（latent Dirichlet allocation，LDiA）产生的词–主题向量更好地捕捉目标词的含义（其语义）。

:::color4
**注意 (Note)**  
尝试使用更低维内部表示重构输入的模型称为**自编码器 (autoencoders)**。这听起来或许有些奇怪：此过程类似让机器复述您刚问的问题，但它不能将问题逐字记录下来。机器必须将您的问题压缩成简写，并对所有提问都使用相同的简写算法（函数）。机器就在此过程中学习到一种新的简写（向量）表示。

如果想进一步了解用于创建高维对象（如单词）压缩表示的无监督深度学习模型，请搜索“autoencoder”<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">26</font>**</sup>。它们也是入门神经网络的常用方法，因为几乎可应用于任意数据集。

:::

Word2Vec 会学习到您可能未曾联想到的词义属性。您可曾想过，每个词都携带一定的地理信息、情感（positivity）和性别倾向？如果语料中某个词具有诸如“placeness”、“peopleness”、“conceptness”或“femaleness”之类的特性，则所有其他词在向量表示中也会获得对应的分数。当 Word2Vec 学习词向量时，词义会在邻近词之间相互渗染。

语料库中的所有词都将由数值向量表示，类似于第 4 章讨论的词-主题向量。但这一次，主题具有更具体、更精确的含义。在潜在语义分析（LSA）中，词语只需在同一文档中出现，就能让它们的含义相互渗染并汇入其词-主题向量；而在 Word2Vec 中，词语必须彼此靠近，通常相隔不超过五个词且位于同一句中。此外，还可通过对词向量主题权重进行加减运算，创建新的有意义的词向量！

理解词向量的一个有益思路是，将其视为一系列权重或分数。每个权重或分数对应词义的一个特定维度，以下示例即为演示。

**Listing 6.1 计算 nessvector**

```python
>>> from nessvec.examples.ch06.nessvectors import *
>>> nessvector('Marie_Curie').round(2)
placeness    -0.46
peopleness    0.35    # 发挥创意，为您感兴趣的 nessvec 维度命名，例如 “trumpness” 或 “ghandiness”。要不要来个 nessvec PR？
animalness    0.17
conceptness  -0.32
femaleness    0.26    # 除非您有充足的运行内存和大量时间，否则不要导入此模块。预训练的 Word2Vec 模型体量巨大。
```

你可以为 Word2Vec 词汇表中的任意单词或 n-gram 计算 ness 向量（nessvector），可使用 nlpia 提供的工具（[https://gitlab.com/tangibleai/nessvec/-/blob/main/src/nessvec/examples/ch06/nessvectors.py）。这种方法适用于并可对任何带](https://gitlab.com/tangibleai/nessvec/-/blob/main/src/nessvec/examples/ch06/nessvectors.py）。这种方法适用于并可对任何带) “ness” 后缀的词进行评分。



你能想到的所有成分，都可以被嵌入，包括 peopleness、animalness、placeness、thingness，甚至 conceptness。一个词嵌入会把所有这些得分组合成一个稠密的浮点向量（没有零值）。

Mikolov 在思考如何用向量数值化表示单词时开发了 词向量（Word2Vec）算法。他对你在第 4 章做过的、准确度较低的情感数学并不满意；他想做的是 类比推理（analogical reasoning），就像你在上一节用那些类比问题所做的那样。这个概念听起来也许很高大上，但实际上，它只是意味着你可以对词向量做数学运算，并且当你把结果向量转换回单词时，答案是有意义的。

<h3 id="PKJD8">6.3.1 类比推理</h3>
词向量（Word2Vec）首次公开是在 2013 年的 ACL 会议上<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">27</font>**</sup>。那场报告题为“连续空间词表示中的语言规律性（Linguistic Regularities in Continuous Space Word Representations）”，介绍了一种令人惊讶的高精度语言模型。与等价的 潜在语义分析（LSA）模型相比，Word2Vec 嵌入在回答本章前面讨论的类比问题时，准确率提高了四倍（45% 对 11%）<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">28</font>**</sup>。这种准确率提升如此之大，以至于 Mikolov 的初稿被 国际学习表征会议（ICLR）拒稿<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">29</font>**</sup>。评审认为模型性能好得令人难以置信。Mikolov 团队花了近一年才公开源码并被 计算语言学协会（ACL）接收。突然之间，借助词向量，下列问题可以通过 向量代数（vector algebra）求解（见图 6.2）：

```plain
Portland Timbers + Seattle – Portland = ?
```

![](已给出插图)![图6.2 Word2Vec的几何运算](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748235913561-ba8ed735-7e92-477f-8bc1-3681d191a23c.png)

词向量（Word2Vec）语言模型“知道”单词 _Portland_ 与 _Portland Timbers_ 之间的距离与 _Seattle_ 与 _Seattle Sounders_ 之间的大致相同，而且两个词对的向量位移方向也大致一致。这意味着 `word2vec` 模块可用于回答你的球队类比问题。你可以把 _Portland_ 与 _Seattle_ 的差向量加到表示 _Portland Timbers_ 的向量上，接近 _Seattle Sounders_ 的向量。该玩具问题的运算如公式 6.1 所示：

:::color2
公式 6.1  足球队类比问题的计算结果

$ \begin{bmatrix}
0.0168\\
0.007\\
0.247\\
\vdots
\end{bmatrix}
+
\begin{bmatrix}
0.093\\
-0.028\\
-0.214\\
\vdots
\end{bmatrix}
-
\begin{bmatrix}
0.104\\
0.0883\\
-0.318\\
\vdots
\end{bmatrix}
=
\begin{bmatrix}
0.006\\
-0.109\\
0.352\\
\vdots
\end{bmatrix} $

:::

在加减词向量之后，得到的向量几乎不会与词向量词表中的某个向量完全相等。Word2Vec 词向量通常有数百维，每维都是连续实数。然而，你词表中与结果向量最近的那个向量往往就是你的 自然语言处理（NLP）问题的答案。与该近邻向量关联的英文单词，就是关于球队和城市问题的自然语言答案。

Word2Vec 允许你把自然语言的词频向量转换到维度更低的 Word2Vec 向量空间。在这个低维空间里，你可以完成数学运算，再把结果转换回自然语言空间。你可以想象，这一能力对聊天机器人、搜索引擎、问答系统或信息抽取算法有多么有用。

:::warning
**NOTE**    Mikolov 及其同事 2013 年的初版论文仅能达到 40% 的回答准确率，但当时已显著超越其他任何语义推理方法。自发表以来，随着在超大规模语料上的训练，Word2Vec 的性能不断提升——参考实现训练于包含 1,000 亿词的 Google News 语料库。本书中我们将一直使用这一预训练模型。

:::

研究团队还发现，单数词与复数词之间的差向量通常大小相近、方向一致。公式 6.2 展示了如何计算这一 复数化向量：

:::color2
公式 6.2  单数形与复数形之间的距离

$ \vec{x}_{\text{coffee}} - \vec{x}_{\text{coffees}}
\;\approx\;
\vec{x}_{\text{cup}} - \vec{x}_{\text{cups}}
\;\approx\;
\vec{x}_{\text{cookie}} - \vec{x}_{\text{cookies}} $

:::

当你用 _coffees_ 的向量减去 _coffee_ 的向量，得到的向量代表单词 _coffee_ 的复数化。在复数化对名词含义有一致影响的语言中，_coffee_ 的复数化向量应与 _cup_、_cookie_ 或其他名词的复数化向量相似。当 Tomas Mikolov（以及整个 NLP 领域）首次计算这一复数化向量，并发现它在数千个名词上都一致时，这是一个真正的“欧几里得时刻”<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">30</font>**</sup>。不久之后，Mikolov 及其他研究者发现，大多数词类比都可以用同样的方法计算。这是第一次可以用直观的向量数学来解决类比推理任务。将标记和单词嵌入向量空间的能力成为过去十年 NLP 与 AI 爆炸式发展的基石计算技巧。即便如今规模最大的语言模型，也依赖与 2013 年 Tomas Mikolov 最早创建的那些词向量相似的嵌入。

<h4 id="ehV6d">更多使用词向量的理由</h4>
词向量不仅对推理和类比问题有用，对你使用自然语言向量空间模型的其他所有任务同样重要。从模式匹配到建模再到可视化，如果你懂得如何使用本章的词向量，你的 NLP 流水线的准确性和实用性都会提升。

例如，本章后面我们会展示如何在二维语义地图（semantic map）上可视化词向量，如图 6.3 所示。你可以把它想成旅游胜地的卡通地图，或公交站海报上的那种印象派地图。在这些卡通地图中，语义上和地理上接近的事物会被挤在一起。对于卡通地图，艺术家会调整各种地点图标的比例和位置，以契合当地氛围。使用词向量，机器也能体会单词和地名应当相距多远。因此，机器能够生成类似图 6.3 的印象派语义地图<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">31</font>**</sup>。

![](已给出插图)![图6.3 10个美国城市投影到二维图上的词向量](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748236089062-4e71d4e3-8b01-4bc6-a47f-609da325491a.png)

如果你熟悉这些美国城市，你可能会意识到这不是一张准确的地理地图，但它却是一张相当不错的语义地图。我们有时会混淆德州的两座大城市——休斯顿和达拉斯——它们的词向量几乎完全相同。而加州几个大城市的词向量则形成了一个漂亮的文化三角。

词向量对聊天机器人和搜索引擎同样大有裨益。对于这些应用，词向量可以克服基于模式和关键字匹配的僵化与脆弱性。字符级的匹配模式无法分辨 _tell me about a Denver omelet_ 与 _tell me about the Denver Nuggets_ 之间的差异，而基于词向量的模式可以。基于词向量的模式很可能区分食物（omelet）和篮球队（Nuggets），并针对用户的不同提问给出恰当回应。

<h3 id="bNGik">6.3.2 学习词嵌入</h3>
词嵌入是表示单词含义（语义）的向量；然而，词义本身难以捉摸且模糊不清。孤立的单词往往语义含糊。以下因素都会影响一个词的含义：

▪ 思想的发出者是谁  
▪ 目标受众是谁  
▪ 词语所处的上下文（何时、何地）  
▪ 假定的领域知识或背景知识  
▪ 词语想要表达的具体语义

你的大脑对某个词的理解通常与他人不同，而且这种理解会随时间演变。你会在与其他概念建立新联系时学到关于该词的新知识；当你学习新的概念和词汇时，也会根据它们在大脑中留下的印象，与这些新词建立新的联系。嵌入用于表示由新词在你大脑中产生的神经元连接模式，这些向量往往拥有数百个维度。

想象一个小女孩说：“我妈妈是一名医生。”<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">32</font>**</sup> 设想 _doctor_ 对她意味着什么，再思考随着她成长，这个词在她的自然语言理解（NLU）算法中的意义如何演变。随着时间推移，她会学会区分医学博士（MD）和哲学博士（PhD）。再想象几年后，当她开始考虑申请医学院或博士项目时，这个词对她将意味着什么。并设想这个词对她的母亲——那位医生——的含义；最后，设想对没有医疗服务可用的人来说 _doctor_ 的含义。

创建有用的单词数值表示并非易事。你想嵌入到向量中的含义不仅取决于“谁”的语义，还取决于你希望机器在“何时、何地”处理并理解这种语义。以 GloVe、Word2Vec 及其他早期词嵌入为例，其目标在于表示单词的“平均”或最常见含义。研究人员关注的是类比问题及其他评估人类和机器词义理解的基准测试。例如，本章之前的代码片段就使用了预训练的 fastText 词嵌入。



:::success
**提示**  
预训练的词向量可用于 Wikipedia、DBpedia、X（原 Twitter）以及 Freebase 语料库<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">33</font>**</sup>。这些模型是开展词向量应用的绝佳起点：  
▪ Google 提供了基于英文 Google News 文章训练的预训练 Word2Vec 模型<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">34</font>**</sup>

▪ Facebook 发布了适用于 294 种语言的词向量模型 fastText<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">35</font>**</sup>

:::

一旦确定了词嵌入的目标受众或用户，只需收集这些单词的用例示例。Word2Vec、GloVe 和 fastText 均属于无监督学习算法；你只需要一份与自己及用户感兴趣领域相关的原始文本。例如，如果你主要关注医生，可以用医学期刊文本来训练嵌入；若想获得更通用的词义理解，机器学习工程师常用 Wikipedia 和在线新闻文章，因为 Wikipedia 代表了我们对世界万物的集体认知。

有了语料库，如何为词嵌入语言模型创建训练集？早期主要有两种方法：

▪ 连续词袋模型（Continuous bag of words，CBOW）  
▪ 连续跳字模型（Continuous skip-gram）

连续词袋模型（continuous bag-of-words，CBOW）方法根据邻近的上下文词（输入词）预测目标词（输出词）。它与第 3 章中学习的词袋向量（BOW vectors）的唯一区别在于，CBOW 是针对文档中不断滑动的词窗口创建的。这意味着你将拥有与所有文档中词序列中的词数几乎相等的 CBOW 向量，而 BOW 向量在每篇文档中仅对应一个向量。这为词嵌入训练集提供了更多可利用的信息，因此能够生成更精确的嵌入向量。采用 CBOW 方法，你会从原始文档中可提取的每一个可能短语生成大量微型合成文档。

**跳字模型（skip-gram）**  
采用 skip-gram 方法，同样会生成大量合成文档，但预测目标和特征反向：使用 CBOW 的目标词去预测 CBOW 的特征词。Skip-gram 模型根据句子中目标词前后的词来预测被跳过的词。尽管看似把词对颠倒，但很快你会发现其结果在数学上几乎等价。图 6.4 与图 6.5 展示了如何分别设置神经网络以学习 CBOW 和 skip-gram 词向量。

![](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748236460511-3fd9f4bc-688a-48eb-b2ad-9da67226453a.png)

你可以看到，两种神经网络方法为跳字模型（skip-gram）和连续词袋模型（CBOW）都产生了相同数量的训练样本。在跳字模型训练中，你要预测处于上下文词“邻域”中的一个词。设想你的语料库包含 Bayard Rustin<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">36</font>**</sup> 和 Larry Dane Brimner<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">37</font>**</sup> 反对个人主义的这句睿智之言：

:::warning
_We are all one. And if we don't know it, we will find out the hard way._  
——Bayard Rustin

:::

:::warning
**重要**  跳字模型是一个 2-gram 或 gram 对，其中每个 gram 都位于另一个 gram 的“邻域”内。按惯例，gram 可以是你的分词器设计要预测的任何文本块——通常是单词。

:::

对于连续跳字模型训练方法，跳字对是通过在词序列中跳过 0～4 个词来构造的。当使用 Word2Vec 跳字方法训练词嵌入时，跳字对中的第一个词称为 **上下文词**（context word）——即 Word2Vec 神经网络的输入；第二个词称为 **目标词**（target word）——语言模型和嵌入向量要预测的输出。在图 6.6 中，你可以看到跳字模型生成词嵌入时的神经网络架构。

![图6.6 跳字模型的输入输出样例](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748236762208-944eff0b-ce38-4a5c-a64a-57e4003812f0.png)



<h4 id="fjS58">什么是 Softmax？</h4>
Softmax 函数常用作神经网络输出层的激活函数，当网络目标是解决分类问题时尤为常见。Softmax 会将输出压缩到 0 和 1 之间，并保证所有输出节点的和恒等于 1，从而可以把输出视为概率。

对于 $ k $ 个输出节点中的每一个，Softmax 的输出值可用归一化指数函数计算：

:::color2
（公式 6.3 Softmax）

$ \sigma(z)_j=\frac{e^{z_j}}{\sum_{i=1}^{k} e^{z_i}} $

:::

三神经元输出层的输出向量形如下面的三维列向量：

:::color2
（公式 6.4 示例 3D 向量）

$ v=\begin{bmatrix}
0.5\\
0.9\\
0.2
\end{bmatrix} $

:::

经过 Softmax 压缩后的向量为：

:::color2
（公式 6.5 Softmax 后的示例向量）

$ \sigma(v)=\begin{bmatrix}
0.309\\
0.461\\
0.229
\end{bmatrix} $

:::

这些值（保留三位有效数字）的总和约等于 1.0，符合概率分布的性质。

图 6.7 展示了神经网络对前两个上下文词的数值化输入与输出。在示例中，输入词为 _Monet_，网络的期望输出词视训练对不同而为 _Claude_ 或 _painted_。图中给出了 Word2Vec 神经网络在跳字模型下、针对标记对 _Monet_ 与 _Claude_ 训练时的一些理想化示例数值：左侧是一份 _Monet_ 的 one-hot 编码输入向量，其余位置（如标记 _1806_ 或 _Claude_）均为 0；右侧是连续稠密的 Softmax 输出向量，其中目标标记 _Claude_ 对应的输出值接近 1。

![图6.7 跳字模型训练过程的神经网络样例](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748236862562-e3197224-f5bd-47ae-9f96-40fe7147dac1.png)



<h3 id="HrqIC">6.3.3 无需词典即可学习词义</h3>
在本例中训练 Word2Vec 并不需要使用 wiktionary.org 那样的词典来显式定义单词含义。你只需让 Word2Vec 读取包含完整语句的文本即可。本节使用 PyTorch 中 torchtext 包自带的 WikiText2 语料库：

```python
>>> import torchtext
>>> dsets = torchtext.datasets.WikiText2()
>>> num_texts = 10000
>>> filepath = DATA_DIR / f'WikiText2-{num_texts}.txt'
>>> with open(filepath, 'wt') as fout:
...     fout.writelines(list(dsets[0])[:num_texts])
```

为了让过程更直观，你可以查看刚刚用 WikiText2 数据集创建、包含约 10 000 个段落的文本文件：

```bash
>>> !tail -n 3 ~/nessvec-data/WikiText2-10000.txt
```

> 当玛姬离开兹韦格医生的办公室时，她说：  
“每当风穿过树叶呼啸时，”
>

> 我会想，洛文斯坦，洛文斯坦……。  
这是对《潮汐王子》的引用； 是洛文斯坦医生。 
>

> == 反响 ==
>

第 99 998 段恰好包含缩写 Dr.，对应单词 _doctor_。你可以用它来练习 “Mommy is a doctor” 这一直觉实验。很快你就会发现，词向量能否真正学会 _doctor_ 的含义，或者它会不会把表示街道的 Dr.（drive）搞混。

值得庆幸的是，WikiText2 数据集已经完成了分词。单词以单个空格 (“ ” ) 分隔，因此流程无需判断 Dr. 是否位于句末。如果文本未分词，你的 NLP 流程就得在每句末尾去掉标点。甚至标题分隔符 “==” 也被拆成两个单独标记 “=” 与 “=”，段落之间则以换行符 (“\n”) 分隔。维基百科标题（如 == 反响 ==）同样会被当作段落，段落间的空行也会保留。

你可以使用句边界检测器或句子分段器（如 spaCy）把段落拆分成句子，防止训练用的词对跨句扩散。在训练词向量时遵守句子边界可提升嵌入准确性，但是否需要这额外提升由你决定。

流程还能处理的大型基础设施之一是大语料的内存管理。如果要在数百万段落上训练词向量，就需要使用在磁盘上管理文本的数据集对象，只把所需部分加载到 RAM 或 GPU。Hugging Face Hub 的 datasets 包可以完成这项工作：

```python
>>> import datasets
>>> dset = datasets.load_dataset('text', data_files=str(filepath))
>>> dset
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 10000
    })
})
```

但你仍需告诉 Word2Vec 什么才算“词”。这是你在 Word2Vec 数据集中唯一需要关注的“监督”信息；只需用第 2 章最简单的分词器就能得到不错的效果。对于这种以空格分隔的文本，可以调用 `str.split()`，再配合 `str.lower()` 做大小写折叠，将词汇表规模减半。出人意料的是，仅凭这些就足以让词向量学到单词的含义与内涵，从而解决 SAT 类比题甚至推断现实中的事物与人物。

```python
def tokenize_row(row):
    row['all_tokens'] = row['text'].lower().split()
    return row
```

现在，你可以在包含 WikiText2 数据行（每行都有一个 `text` 键）的 `torchtext` 数据集上应用自定义分词器：

```python
>>> dset = dset.map(tokenize_row)
>>> dset
DatasetDict({
    train: Dataset({
        features: ['text', 'tokens'],
        num_rows: 10000
    })
})
```

为了让神经网络能够进行 one-hot 编码与解码，你需要先为数据集计算词汇表：

```python
>>> vocab = list(set(
...     tok
...     for row in dset['train']['tokens']
...     for tok in row))
>>> vocab[:4]
['cast', 'kaifeng', 'recovered', 'doctorate']

>>> id2tok = dict(enumerate(vocab))
>>> list(id2tok.items())[:4]
[(0, 'cast'), (1, 'kaifeng'), (2, 'recovered'), (3, 'doctorate')]

>>> tok2id = {tok: i for (i, tok) in id2tok.items()}
>>> list(tok2id.items())[:4]
[('cast', 0), ('kaifeng', 1), ('recovered', 2), ('doctorate', 3)]
```

剩余的特征工程步骤是通过对标记序列进行 **窗口化** 来生成跳字对（skip-gram pairs），然后在这些窗口内配对：

```python
WINDOW_WIDTH = 10

def windowizer(row, wsize=WINDOW_WIDTH):
    """将句子（str）转换为滑动窗口内的跳字对。"""
    doc = row['tokens']
    out = []
    for i, wd in enumerate(doc):
        target = tok2id[wd]
        window = [
            i + j
            for j in range(-wsize, wsize + 1, 1)
            if (i + j >= 0) & (i + j < len(doc)) & (j != 0)
        ]
        out += [(target, tok2id[doc[w]]) for w in window]
    row['moving_window'] = out
    return row
```

 当你将 窗口化器（windowizer） 应用于数据集后，其中会新增一个 window 键，用于存储标记窗口：

```python
>>> dset = dset.map(windowizer)
>>> dset
DatasetDict({
    train: Dataset({
        features: ['text', 'tokens', 'window'],
        num_rows: 10000
    })
})
```

跳字对生成器函数：

```python
def skip_grams(tokens, window_width=WINDOW_WIDTH):
    pairs = []
    for i, wd in enumerate(tokens):
        target = tok2id[wd]
        window = [
            i + j
            for j in range(-window_width, window_width + 1, 1)
            if (i + j >= 0) & (i + j < len(tokens)) & (j != 0)
        ]
        pairs.extend([(target, tok2id[tokens[w]]) for w in window])
    # HuggingFace 的 datasets 将每个文本元素视为字典
    return pairs
```

你的神经网络只需要窗口化数据中的跳字对：

```python
from torch.utils.data import Dataset

class Word2VecDataset(Dataset):
    def __init__(self, dataset, vocab_size, wsize=WINDOW_WIDTH):
        self.dataset = dataset
        self.vocab_size = vocab_size
        self.data = [i for s in dataset['moving_window'] for i in s]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

`DataLoader` 将帮助你管理内存，使流程几乎可在任何规模的语料（甚至整个维基百科）上复用：

```python
from torch.utils.data import DataLoader

dataloader = {}
for k in dset.keys():
    dataloader = {
        k: DataLoader(
            Word2VecDataset(dset[k], vocab_size=len(vocab)),
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=CPU_CORES - 1
        )
    }
```

你需要一个 one-hot 编码器把词对转换为 one-hot 向量对：

```python
def one_hot_encode(input_id, size):
    vec = torch.zeros(size).float()
    vec[input_id] = 1.0
    return vec
```

为了揭开示例背后的“魔法”，我们将从零开始训练网络（与第 5 章相同）。你会看到 Word2Vec 网络几乎与上一章的单层网络一致：

```python
from torch import nn
EMBED_DIM = 100  # 100 维度较小但可用；300 更为常见

class Word2Vec(nn.Module):
    def __init__(self, vocab_size=len(vocab), embedding_size=EMBED_DIM):
        super().__init__()                       # 仅在实例化 Word2Vec 对象时初始化网络层
        self.embed = nn.Embedding(vocab_size, embedding_size)
        self.expand = nn.Linear(embedding_size, vocab_size, bias=False)

    def forward(self, input):
        hidden = self.embed(input)               # 隐藏层将词频统计嵌入为低维向量
        logits = self.expand(hidden)             # 输出层将 100D 隐藏向量解码预测 one-hot
        return logits
```

 一旦实例化 Word2Vec 模型，你就可以为词汇表中超过 20,000 个单词创建 100 维嵌入向量:

```python
>>> model = Word2Vec()
>>> model
Word2Vec(
  (embed): Embedding(20641, 100)
  (expand): Linear(in_features=100, out_features=20641, bias=False)
)
```

如果有 GPU，可将模型发送到 GPU 加速训练：

```python
import torch
if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')
device
```

大多数现代 CPU 可在 15 分钟内完成训练，不必担心没有 GPU：

```python
>>> model.to(device)
Word2Vec(
  (embed): Embedding(20641, 100)
  (expand): Linear(in_features=100, out_features=20641, bias=False)
)
```

现在进入有趣的部分！只需阅读大量文本，Word2Vec 就能迅速学会 _Dr._ 及数千个标记的含义。你可以沏杯茶或巧克力，或进行十分钟冥想，让笔记本电脑思考单词的意义。首先定义训练参数：

```python
from tqdm import tqdm  # 用于显示进度条
EPOCHS = 10
LEARNING_RATE = 5e-4

loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)

running_loss = []
pbar = tqdm(range(EPOCHS * len(dataloader['train'])))
for epoch in range(EPOCHS):
    epoch_loss = 0
    for sample_num, (center, context) in enumerate(dataloader['train']):
        if sample_num % len(dataloader['train']) == 2:
            print(center, context)
            # center: tensor([...])
            # context: tensor([...])
        center, context = center.to(device), context.to(device)
        optimizer.zero_grad()
        logits = model(input=center)
        loss = loss_fn(logits, context)
        if sample_num % 10000:
            pbar.set_description(f'loss[{sample_num}] = {loss.item():.3f}')
        epoch_loss += loss.item()
        loss.backward()
        optimizer.step()
        pbar.update(1)
    epoch_loss /= len(dataloader['train'])
    running_loss.append(epoch_loss)

save_model(model, loss)
```

<h3 id="aqQPt">6.3.4 使用 gensim.word2vec 模块</h3>
如果上一节听起来过于复杂，不必担心。多家公司都提供了预训练词向量模型，各主流编程语言的 NLP 库也能高效地使用这些预训练模型。本节将介绍如何利用词向量的“魔力”。在示例中，我们使用流行的 **gensim 库（gensim）**。

要下载模型，你可以搜索“Google News 预训练 word2vec 模型”<sup>38</sup>。将谷歌原始二进制文件下载到本地路径后，可通过 gensim 进行加载：

```python
>>> from gensim.models.keyedvectors import KeyedVectors
>>> word_vectors = KeyedVectors.load_word2vec_format(
...     '/path/to/GoogleNews-vectors-negative300.bin.gz', binary=True)
```

词向量模型对内存要求较高。如果内存有限，或不想等待模型完全载入，可通过 `limit` 参数减少加载进内存的词数。下面的示例仅加载 Google News 语料中最常见的 200 000 个词：

```python
>>> from gensim.models.keyedvectors import KeyedVectors
>>> from nlpia2.loaders import get_data
>>> word_vectors = get_data('w2v', limit=200000)
```

→ 这样可将占用内存控制在只加载 200 000 个（而非 200 万个）Word2Vec 向量的规模。

请注意，限制词汇表会导致当文本中出现未加载词汇时，NLP 流水线表现下降。因此，一般只在开发阶段限制模型大小。若想复现本章其余示例的结果，应使用完整的 Word2Vec 模型。

`gensim.KeyedVectors.most_similar()` 方法可高效地找到给定词向量的最近邻。参数 `positive` 接收需要相加的向量列表（与本章开头的足球队示例类似），`negative` 则用于做“减法”或排除无关词；`topn` 决定返回多少个相关词。

与传统同义词词典不同，Word2Vec 的“同义”是一种连续分数（距离）。由于 Word2Vec 本身是连续向量空间模型，每个维度都是连续值且维度高，得以捕捉单词含义的完整范围，这也是它能处理类比甚至 _zeugma_（轭语，单词中多义并置）的原因——后者需要具备常识与推理能力<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">39</font>**</sup>。

词嵌入足以让机器对 SAT 类比题有基本理解：

```python
>>> word_vectors.most_similar(positive=['cooking', 'potatoes'], topn=5)
[('cook', 0.6973530650138855),
 ('oven_roasting', 0.6754535068258667),
 ('slow_cooker', 0.6742023170295715),
 ('sweet_potatoes', 0.6600279808044434),
 ('stir_fry_vegetables', 0.6548759341239929)]

>>> word_vectors.most_similar(positive=['germany', 'france'], topn=1)
[('europe', 0.7223039699554443)]
```

gensim 还提供 `doesnt_match` 方法，用于找出与列表其余元素最不相关的词：

```python
>>> word_vectors.doesnt_match("potatoes milk cake computer".split())
'computer'
```

 为了确定列表中最不相关的术语，该方法会返回与列表中所有其他术语距离最大的那一项。  如果要执行向量运算（例如著名的 _king + woman – man = queen ，这正是当初让 Mikolov 及其导师兴奋不已的示例  _），可在 `most_similar` 中使用 `negative` 参数：

```python
>>> word_vectors.most_similar(positive=['king', 'woman'],
...     negative=['man'], topn=2)
[('queen', 0.7118192315101624), ('monarch', 0.6189674139022827)]
```

 gensim 库还允许你计算两个术语之间的相似度。如果你想比较两个单词并求它们的余弦相似度（cosine similarity），请使用 `.similarity()` 方法：  

```python
>>> word_vectors.similarity('princess', 'queen')
0.7070531598370459
```

 如果你想编写自己的函数并直接操作原始词向量，可以通过 Python 的方括号语法 `[]` 或 `KeyedVector` 实例上的 `get()` 方法来访问它们。你可以将加载后的模型对象当作字典，目标单词就是字典键。返回数组中的每个浮点数都代表向量的一个维度。在 Google 的词向量模型中，得到的 NumPy 数组形状为 1 × 300：  

```python
>>> word_vectors['phone']
array([-0.01446533, -0.12792969, -0.11572266, -0.22167969, -0.07373047,
       -0.05981445, -0.10009766, -0.06884766,  0.14941406,  0.10107422,
       ...
)
```

 如果你想弄清这些数字究竟意味着什么，确实可以办到，但这会耗费大量精力。你需要研究一些同义词，看看它们在数组的 300 个数字中共同具备哪些数值。或者，你也可以像本章开头那样，找到这些数字的 线性组合（linear combination），从而构造出诸如 空间性（placeness） 和 女性性（femaleness） 等维度。  

<h3 id="qkfiw">6.3.5 生成自定义词向量表示</h3>
在某些情况下，你可能需要训练领域专用的词向量模型。如果 NLP 流水线处理的文本在用词方式上与 2006 年前 Google News 语料明显不同，自定义模型通常能提高准确率。不过，你需要大量文档才能达到 Google 与 Mikolov 的效果。如果目标词在 Google News 中极少出现，或文本在受限领域（如医学文本、会议记录）中以独特方式使用词语，那么专用模型尤为有益。下文展示如何训练自己的 Word2Vec 模型。训练前，需利用第 2 章介绍的工具对语料做预处理；依旧使用 gensim。

<h4 id="L5AIO">**预处理步骤**</h4>
首先把文档切分为句子，再把句子切分为标记（tokens）。`gensim.word2vec` 期望的输入是句子列表，列表里的每个句子又是 token 列表，以免相邻句子的无关词共现影响学习。示例结构如下：

```python
>>> token_list
[
    ['to', 'provide', 'early', 'intervention/early', 'childhood', 'special',
     'education', 'services', 'to', 'eligible', 'children', 'and', 'their',
     'families'],
    ['essential', 'job', 'functions'],
    ['participate', 'as', 'a', 'transdisciplinary', 'team', 'member', 'to',
     'complete', 'educational', 'assessments', 'for']
    ...
]
```

切分句子并将句子转换为 token，你可沿用第 2 章的多种策略，再补充一个工具 **DetectorMorse（DetectorMorse）**。DetectorMorse 是在 _Wall Street Journal_ 多年句子数据上预训练的句子分段器，某些场景下比 NLTK 和 gensim 自带的分段器更准确<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">40</font>**</sup>。如果语料与 WSJ 风格相近，DetectorMorse 可提供当下最高精度；也可在自己的大规模句子数据上重新训练。完成 token 列表（每句一个列表）后，即可开始训练 Word2Vec 模型。

<h4 id="RVivp">训练领域专用的 Word2Vec 模块</h4>
首先加载 word2vec 模块：

```python
from gensim.models.word2vec import Word2Vec
```

下面的代码清单展示了设置 Word2Vec 训练核心参数的方法。

****

```python
num_features   = 300  # 词向量维度数量
min_word_count = 3    # Word2Vec 纳入训练的最小词频；语料小就降低，语料大就提高
num_workers    = 2    # 训练使用的 CPU 核心数；multiprocessing.cpu_count() 可自动扩展
window_size    = 6    # 上下文窗口大小
subsampling    = 1e-3 # 高频词下采样率
```

现在可以开始训练了。

```python
model = Word2Vec(
    token_list,
    workers=num_workers,
    size=num_features,
    min_count=min_word_count,
    window=window_size,
    sample=subsampling)
```

训练所需时间取决于语料规模和 CPU 性能。小语料几分钟即可完成；若要训练全面的词模型，语料可能包含数百万句子，需要为语料中的每个单词收集多种用法示例。处理更大语料（如 Wikipedia）时，应预期更长训练时间和更高内存占用。

此外，word2vec 模型本身也可能占用大量内存。但通常我们只关心隐藏层的权重矩阵。训练结束后，可冻结模型并丢弃多余信息，将内存占用减少约一半。以下命令会删除预测词共现所用的输出层权重：

```python
model.init_sims(replace=True)
```

`init_sims` 方法将冻结模型，存储隐藏层的权重并丢弃用于预测词语共现的输出层权重。输出层权重并不是大多数 Word2Vec 应用中使用的向量的一部分。但一旦丢弃输出层的权重，模型将无法再进行训练。你可以使用以下命令保存已训练好的模型，以便后续使用：  

```python
model_name = "my_domain_specific_word2vec_model"
model.save(model_name)
```

如果想测试新训练的模型，可按上一节的方法加载并使用：

```python
from gensim.models.word2vec import Word2Vec
model_name = "my_domain_specific_word2vec_model"
model = Word2Vec.load(model_name)
model.most_similar('radiology')
```



<h2 id="HGKGF">**6.4 Word2Vec替代方案**</h2>
Word2Vec是一个突破性的进展，但它依赖于必须通过反向传播（backpropagation）训练的神经网络模型。自从Mikolov首次推广词嵌入（word embeddings）以来，研究人员提出了越来越精确高效的方式，将词语的意义嵌入到向量空间中：

+ Word2Vec
+ GloVe
+ fastText

斯坦福NLP研究人员<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">41</font>**</sup>，由Jeffrey Pennington领导，开始研究Word2Vec为何表现如此出色并找出其优化的损失函数。他们首先统计了词语共现，并将其记录在一个平方矩阵中。他们发现可以对该共现矩阵执行奇异值分解（SVD，Singular Value Decomposition）<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">42</font>**</sup>，将其分解为与Word2Vec产生的两个权重矩阵相同的形式。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">43</font>**</sup>关键在于用相同的方式对共现矩阵进行归一化。但在某些情况下，Word2Vec模型未能收敛到与斯坦福研究人员使用SVD方法所达到的相同全局最优解。正是这种对整个语料库（corpus）中词语共现（co-occurrences）的全局向量的直接优化，赋予了GloVe其名称。

<h3 id="GgGaw">**6.4.1 GloVe**</h3>
GloVe能够生成与Word2Vec输入权重矩阵和输出权重矩阵等价的矩阵，生成的语言模型与Word2Vec具有相同的准确度，但速度更快。这意味着通过使用文本数据进行计算，可以更高效地训练，并且可以在更小的语料库上训练且仍能收敛。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">44</font>**</sup>由于SVD算法已经被精炼了几十年，GloVe在调试和算法优化方面具有领先优势。Word2Vec依赖于反向传播来更新形成词嵌入的权重。与GloVe中用于SVD的优化算法相比，神经网络反向传播效率较低。

尽管Word2Vec最初推广了基于词向量的语义推理（semantic reasoning）概念，在训练新的词向量模型时，最佳选择很可能是GloVe。使用GloVe，你更可能找到向量表示的全局最优解，获得更准确的结果，并且spaCy将其用作默认的嵌入算法，因此当你运行以下代码时，结果是使用GloVe在后台计算的！

```python
>>> import spacy
>>> nlp = spacy.load("en_core_web_sm")
>>> text = "This is an example sentence."
>>> doc = nlp(text)
>>> for token in doc:
...     print(token.text, token.vector)
```

GloVe具有以下优势：

+ 训练速度更快
+ 更佳的内存/CPU效率（可以处理更大的文档）
+ 数据利用率更高（对较小语料库更友好）
+ 相同训练量下结果更准确

<h3 id="jHI8u">**6.4.2 fastText**</h3>
Facebook的研究人员在Word2Vec的基础上进一步发展，<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">45</font>**</sup>通过为模型训练引入新方法，创建了新算法，命名为fastText。该算法预测周围n字符的子词（subword）片段，而不仅仅是像Word2Vec那样使用周围词语。例如，单词 whisper 将生成以下2-和3-字符片段：  
['wh', 'whi', 'hi', 'his', 'is', 'isp', 'sp', 'spe', 'pe', 'per', 'er']

fastText会为每个n字符片段（称为子词）训练向量表示，包含完整词、拼写错误词、部分词，甚至单个字符。该方法的优势在于它能更好地处理罕见词或新词，比原始Word2Vec方法更出色。

fastText的分词器将为较长词语的两部分创建向量，如果较长词在语料库中出现次数远少于其组成部分。例如，如果语料库中只提到Superwoman一两次，但super和woman成千上万次，则fastText会将super和woman的向量相加生成Superwoman的向量。这样，fastText需要分配的OOV（超出词汇表）向量数量会更少。在你的NLU（自然语言理解）管道中，OOV词看起来像未知词（unknown word），在完全陌生的语言中会有相同的效果。而Word2Vec只会“记住”它以前见过的词，fastText因其子词方法更灵活。

作为fastText发布的一部分，Facebook发布了预训练的fastText模型，覆盖294种语言。在Facebook研究的GitHub页面<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">46</font>**</sup>上，你可以找到从Abkhazian到Zulu的模型。该模型集合甚至包括罕见语言，如Saterland Frisian，这只由少数德国人使用。Facebook提供的预训练fastText模型仅在可用的维基百科语料库上训练，因此其词汇量和准确度在不同语言中会有所不同。

我们在nessvec包中包含了用于创建OOV词新向量的fastText逻辑。我们还对fastText管道进行了增强，使用Peter Norvig著名而优雅的拼写纠错算法<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">47</font>**</sup>处理拼写错误和输入错误。这将为你带来双重好处：一个可理解的训练算法以及一个在现实世界中需要使用预训练向量时的稳健推理或预测模型。

<h4 id="v0vSs">**使用预训练模型增强你的NLP模型表现**</h4>
利用全球最强大的公司提供的开源预训练嵌入，增强你的NLP管道。预训练fastText向量几乎覆盖每种可用语言。如果你想查看所有用于词嵌入的选项，请访问fastText模型库。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">48</font>**</sup>对于多语种功能，你可以找到Common Crawl版本的fastText嵌入支持的157种语言的组合模型。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">49</font>**</sup>如果愿意，你可以下载每种语言的所有嵌入版本，或者仅下载最受欢迎的100万个词嵌入。

:::color4
**WARNING**  
bin+text wiki.en.zip文件<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">50</font>**</sup>大小为9.6 GB。text-only wiki.en.vec文件<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">51</font>**</sup>大小为6.1 GB。如果使用nessvec包而非gensim，它会下载600 MB的wiki-news-300d-1M.vec.zip文件。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">52</font>**</sup>该wiki-news-300d-1M.vec.zip文件包含从维基百科和新闻网页中提取的100万个最受欢迎词汇（不区分大小写）的300维向量。

:::

nessvec包会为你的所有预训练向量创建内存映射DataFrame（.hdf5文件），防止因加载所有向量而导致内存耗尽。它会在需要时仅加载所需向量：

```python
>>> from nessvec.files import load_fasttext
>>> df = load_fasttext()
>>> df.head().round(2)
    0 1 2 ... 297 298 299
, 0.11 0.01 0.00 ... 0.00 0.12 -0.04
the 0.09 0.02 -0.06 ... 0.16 -0.03 -0.03
. 0.00 0.00 -0.02 ... 0.21 0.07 -0.05
and -0.03 0.01 -0.02 ... 0.10 0.09 0.01
of -0.01 -0.03 -0.03 ... 0.12 0.01 0.02
>>> df.loc['prosocial']
0 0.0004
1 -0.0328
2 -0.1185
...
297 0.1010
298 -0.1323
299 0.2874
Name: prosocial, Length: 300, dtype: float64
```

:::color4
**NOTE** 为了增强你的词嵌入管道，可以使用Bloom embeddings。Bloom embeddings并不是用于创建词嵌入的新算法，而是用于在高维向量中存储和检索时提供更快速、更准确索引方法。Bloom embedding表中的向量可以表示由两个或多个词组合在一起的意义。其技巧在于从向量中减去不需要的词，从而无需重建原始嵌入。幸运的是，spaCy在其v2.0语言模型中实现了这种效率。这就是spaCy如何创建数百万单词的词嵌入，同时只存储20,000个唯一向量的方法。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">53</font>**</sup>

:::



<h3 id="pJ2pi">6.4.3 Word2Vec 与 LSA</h3>
你也许想知道，词嵌入（word embeddings）与第 4 章中的 LSA 词-主题向量（LSA word-topic vectors）相比如何。LSA 词-主题向量是你对 TF–IDF 向量进行主成分分析（principal component analysis，PCA）后得到的词嵌入。同样，LSA 也能生成文档-主题向量（document-topic vectors），你曾将其用作整个文档的嵌入。LSA 文档-主题向量是该文档中所有词-主题向量求和的结果。如果你想得到一个与文档-主题向量对应的整篇文档的词向量，只需把该文档的所有词向量相加——这与 Doc2Vec 文档向量的工作方式非常接近。

若你的 LSA 主题向量矩阵规模为 $ Nwords×NtopicsN_{\text{words}} \times N_{\text{topics}} $，则 LSA 词向量就是该 LSA 矩阵的行向量。这些行向量与 Word2Vec 一样，用大约 200 到 300 个实数来表达词义。LSA 词-主题向量在寻找相关和无关词语方面，与 Word2Vec 向量一样有用。如同在 GloVe 讨论中所学，Word2Vec 向量也可通过与 LSA 相同的 SVD 算法生成，但 Word2Vec 通过在文档之间创建滑动窗口来更充分地利用相同数量的单词。借助这种方法，同一个单词在窗口滑动之前最多可被重复利用五次。

那么，增量训练或在线训练呢？LSA 和 Word2Vec 算法都允许向语料中添加新文档，并调整已有词向量，以反映新文档中的共现情况，但只能更新词汇表中已存在的 “桶”（bins）。如果要加入全新的单词，就会改变词汇表大小，相应的一元热（one-hot）向量也会变化；若要将新词纳入模型，就必须重新开始训练。

LSA 的训练速度快于 Word2Vec，而且在处理长文档时，它在区分和聚类这些文档方面表现更佳。事实上，斯坦福研究者正是利用这种更快的基于 PCA 的方法来训练 GloVe 向量。你可以使用 nessvec 包比较三种最流行的词嵌入<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">54</font>**</sup>。

Word2Vec 的 “杀手级应用” 是其实现的语义推理能力。LSA 词-主题向量也能进行类似推理，但通常不够准确。如果想让 LSA 模型在准确性和动态性上接近 Word2Vec，你必须将文档拆成句子，然后只用短语进行训练。借助 Word2Vec，你可以回答诸如 _Harry Potter + University = Hogwarts_ 之类的问题。如需查看领域专用 word2vec 模型的生动示例，可参考 Niel Chah 针对《哈利·波特》《指环王》等史诗作品词汇训练的模型<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">55</font>**</sup>。

LSA 的优势包括：  
▪ 更快的训练  
▪ 在长文档之间具备更好的区分能力

另一方面，Word2Vec 和 GloVe 的优势包括：  
▪ 在大型语料上的利用效率更高  
▪ 进行词语语义推理（如解答类比问题）时更准确

<h3 id="vM4qq">6.4.4 静态词嵌入 vs. 上下文化词嵌入</h3>
现实中你会遇到两类词嵌入：**静态词嵌入（static word embeddings）** 和 **上下文化词嵌入（contextualized word embeddings）**。  
静态词嵌入可独立用于单个单词或 _n_-gram；训练完成后，向量将保持固定。这就是你在类比推理及其他词向量推理任务中使用的词嵌入类型。此处你会训练语言模型来生成静态词嵌入，单词的上下文仅用于训练模型。一旦词嵌入训练完毕，你就不会再利用上下文去调整它们——此时你是在**使用**（而非学习）已训练好的词嵌入。这意味着一个单词的不同词义都被压缩进同一个静态向量。例如，Word2Vec 会为 _World Bank_ 里的 _bank_ 和 _riverbank_ 里的 _bank_ 返回相同的嵌入。迄今为止见到的所有嵌入——Word2Vec、GloVe 和 fastText——都是静态嵌入。

相比之下，**上下文化词嵌入** 会根据前后词嵌入及其顺序进行更新或细化；一个词相对于其他词出现的顺序，会影响其上下文化嵌入。这意味着对于 bigram _not happy_，上下文化嵌入得到的向量会比静态嵌入更接近 _unhappy_ 的向量。

上下文化嵌入在诸如语义搜索等多种应用场景中更为有用。其重大突破源于双向 Transformer 神经网络的引入，例如 **BERT（bidirectional encoder representations for transformers）**，本书第 9 章将深入讨论。BERT 嵌入优于 Word2Vec 与 GloVe，因为 BERT 不仅考虑词左右两侧的上下文，还考虑句中词序，因此成为众多 NLP 应用的热门选择。

<h3 id="f6FWh">6.4.5 可视化词关系</h3>
语义词关系威力强大，其可视化往往能带来有趣发现。本节演示如何将词向量投影到 2D。

首先，从 Google News 语料预训练的 Google Word2Vec 模型中加载所有词向量。顾名思义，该语料包含大量 _Portland_、_Oregon_ 以及其他城市和州名的提及。为简单起见，你将使用 **nlpia** 包，从而快速开始玩转 Word2Vec 向量。

```python
>>> from nessvec.indexers import Index
>>> index = Index()             # 下载预训练 fastText 嵌入向量到 ~/…/nessvec-data/
>>> vecs = index.vecs
>>> vecs.shape
(3000000, 300)
```

:::color4
**WARNING** Google News Word2Vec 模型体量庞大，包含 300 万个词，每个词 300 维；完整模型需占用约 3 GB RAM。

:::

在 gensim 中，`KeyedVectors` 对象现在保存着一个 300 万行的 Word2Vec 向量表。我们从 Google 提供的文件中加载了这些向量；该文件存储的是基于 Google News 文章大语料训练的 Word2Vec 模型。毫无疑问，其中包含大量州名与城市名。下面的代码清单展示了词汇表中从第 1 000 000 个词开始的少量示例。

```python
>>> import pandas as pd
>>> vocab = pd.Series(wv.vocab)
>>> vocab.iloc[1000000:1000060]
Illington_Fund      Vocab(count:447860, index:2552140)
Illingworth         Vocab(count:290516, index:94834)
Illingworth_Halifax Vocab(count:1984281, index:1015719)
Illini              Vocab(count:2984391, index:15609)
IlliniBoard.com     Vocab(count:1480147, index:1518593)
Illini_Bluffs       Vocab(count:2636947, index:363053)
```

请注意，复合词和常见的 _n-gram_ 会使用下划线字符（“_”）连接。另请注意，键值映射中的 value 是 `gensim.Vocab` 对象，它不仅包含某个词在词表中的索引位置（便于检索 Word2Vec 向量），还记录该词在 Google News 语料中的出现次数。

如前所示，若要获取某个词的 300 维向量，可在 `KeyedVectors` 对象上使用方括号语法检索任意单词或 _n-gram_：

```python
>>> wv['Illini']
array([ 0.15625   ,  0.18652344,  0.33203125,  0.55859375,  0.03637695,
       -0.09375   , -0.05029297,  0.16796875, -0.0625    ,  0.09912109,
       -0.0921748 ,  0.39257812,  0.05395508,  0.35351562, -0.02270508,
       ...])
```

之所以选择按字典序排名的第一百万个“单词”，是因为最前面的几千个“单词”实际上是大量出现于 Google News 语料中的标点符号序列（例如 `#/#/#/#` 等）。恰好 **Illini** 出现在你的列表中。下面的代码清单将展示 **Illini** 向量与 **Illinois** 向量之间的距离。

```python
>>> import numpy as np
>>> np.linalg.norm(wv['Illinois'] - wv['Illini'])      # 欧氏距离
3.3653798
>>> cos_similarity = np.dot(wv['Illinois'], wv['Illini']) / (
...     np.linalg.norm(wv['Illinois']) *\
...     np.linalg.norm(wv['Illini']))                  # 余弦相似度 = 归一化点积
>>> cos_similarity
0.5501352
>>> 1 - cos_similarity                                 # 余弦距离
0.4498648
```

这些距离表明 **Illini** 与 **Illinois** 的语义只算中等接近。

接下来，我们要检索所有美国城市的 Word2Vec 向量，以便使用它们之间的距离在二维语义平面上作图。怎样在 `KeyedVectors` 对象的词汇表中找到所有城市和州名呢？可以像前面的示例那样利用余弦距离，找出与 **state** 或 **city** 这两个词接近的所有向量。

不过，与其遍历 300 万词及其向量，不如加载一个包含全球城市和州（地区）列表的数据集。

```python
>>> from nlpia.data.loaders import get_data
>>> cities = get_data('cities')
>>> cities.head(1).T
```

（输出包含 `geonameid`、`name`、`latitude`、`longitude`、`population` 等多列）

该 GeoCities 数据集包含经纬度、人口等丰富信息，可用于将地理距离与 Word2Vec 距离进行有趣的可视化或对比。此处我们只想把 Word2Vec 距离映射到二维平面并观察效果，先聚焦美国境内的城市。

```python
>>> us = cities[(cities.country_code == 'US') &\
...             (cities.admin_code.notnull())].copy()
>>> states = pd.read_csv(
...     'http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv')
>>> states = dict(zip(states.Abbreviation, states.State))
>>> us['city'] = us.name.copy()
>>> us['st'] = us.admin_code.copy()
>>> us['state'] = us.st.map(states)
>>> us[us.columns[-3:]].head()
```

（输出示例：`city` | `st` | `state` Bay Minette | AL | Alabama …）

现在，每个城市除了州缩写外还有州全称。接着检查这些州名和城市名中哪些存在于 Word2Vec 词汇表：

```python
>>> vocab = pd.concat([us.city, us.st, us.state])
>>> vocab = np.array([word for word in vocab if word in wv])
>>> vocab[:10]
```

即便只看美国城市，也会遇到同名大城市，例如 Portland, Oregon 和 Portland, Maine。为了让城市向量融合所在州的语义，可将城市向量与对应州向量相加——这正是“类比推理”的魔法。

下面的代码清单演示了一种做法：把州的 Word2Vec 向量加到城市向量上，并把所有新向量存入一个大型 `DataFrame`。州名若未出现在词汇表中，就退而求其次使用州缩写。

```python
>>> city_plus_state = []
>>> for c, state, st in zip(us.city, us.state, us.st):
...     if c not in vocab:
...         continue
...     row = []
...     if state in vocab:
...         row.extend(wv[c] + wv[state])
...     else:
...         row.extend(wv[c] + wv[st])
...     city_plus_state.append(row)
>>> us_300d = pd.DataFrame(city_plus_state)
```

具体关系代表的属性（如地理邻近度、文化或经济相似度）取决于你的语料。关系特征深受训练语料影响，并将忠实反映该语料。

:::warning
**词向量存在偏见！**  
词向量基于训练语料学习词语之间的关系，它们代表了撰写这些文档和网页作者对那些词语的平均含义。这意味着词嵌入带有所有创作这些网页的人所持有的偏见和刻板印象。如果你的语料是金融领域，那么 _bank_ 的词向量主要会与存款业务相关；如果语料主要是地质学，_bank_ 的词向量则会与河流和河岸相关；如果语料主要来自母系社会，女性是银行家、男性在河里洗衣，那么词向量就会体现这种性别偏见。

下面的示例展示了在 Google News 文章上训练的词向量模型的性别偏见。如果你计算 _man_ 与 _nurse_ 的距离，并将其与 _woman_ 与 _nurse_ 的距离进行比较，就能看到这种偏见：

>>> word_model.distance('man', 'nurse')  
0.7453  
>>> word_model.distance('woman', 'nurse')  
0.5586

识别并补偿此类偏见，对于在充满偏见的世界中训练模型的任何 NLP 从业者都是一大挑战。

:::



这些新闻文章作为训练语料，共享一个共同元素——城市的语义相似性。在文中，语义相近的地点似乎可以互换；因此，词向量模型学到它们是相似的。如果你在不同语料上训练，词语关系可能会不同。

在规模和文化上相似的城市会被聚类得很近，例如 San Diego 与 San Jose，尽管它们在地理上相距甚远。类似地，度假胜地（如 Honolulu 和 Tahoe）也可能被聚在一起。

幸运的是，你可以像之前那样用常规代数，将城市向量与州名及州缩写的向量相加。如第 4 章所述，你可以使用 PCA 将 300 维向量降到人类可理解的 2D。PCA 既能让你看到这些 300D 向量在 2D 投影中的“影子”，又能保证该投影是数据的最佳视角，使各向量尽可能分散。PCA 就像一位优秀摄影师，会从所有可能角度寻找最优拍摄角度。向量相加后甚至不必归一化长度，因为 PCA 会自动处理。

我们已在 **nlpia** 包中保存了这些“增广”的城市词向量，你可加载后用于自己的应用。下面的代码使用 PCA 将它们投影到 2D 平面。

```python
>>> from sklearn.decomposition import PCA
>>> pca = PCA(n_components=2)          # PCA 生成的 2D 向量仅用于可视化
>>> us_300D = get_data('cities_us_wordvectors')
>>> us_2D = pca.fit_transform(us_300D.iloc[:, :300])  # DataFrame 最后一列保存城市名
```

![图6.8 使用PCA将三千维词向量投影到二维图](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748239119133-9f062302-0759-4799-b9e9-56df468656d3.png)

Figure 6.8 展示了这些 300D 美国城市词向量在 2D 平面上的投影；该图的气泡图看起来有点像通勤列车路线示意图。气泡的位置并不准确表示真实地理位置，但能让你感受到城市间的大致相对位置及如何迁移。使用 Word2Vec 向量可以自动绘制这类地图，因为当模型在诸如 _New Orleans is west of the Mississippi River_ 或 _New Orleans is in the south_ 这样的短语上训练时，地理位置被嵌入到了词向量中——这就是 _southness_ 与 _westness_ 如何嵌入 _New Orleans, Louisiana_ 词向量的原因。

:::warning
**NOTE**     语义距离较低（距离值接近零）表示词语之间的相似度较高。语义（或“意义”）距离取决于用于训练的文档中这些词的邻近关系。如果两个词的Word2Vec向量在词向量空间中彼此接近，那么它们在上下文中往往是以相似的方式使用（即在它们附近出现的词也很相似）。例如，“San Francisco”（旧金山）接近“California”（加利福尼亚），因为它们在句子中经常同时出现，且它们周围使用的词语分布也相似。相反，如果两个词之间的距离较大，这表示它们的上下文和意义共享的可能性较低（即它们在语义上是相异的），比如“cars”（汽车）和“peanuts”（花生）。  

:::

 如果你想探索图6.8中显示的城市地图，或者尝试绘制你自己的向量，代码清单6.12向你展示了如何使用一个可以处理DataFrame的Plotly包装器来生成地理-语义（geo-semantic）地图。Plotly包装器期望接收一个DataFrame，其中每一行代表一个样本，每一列则包含你想要绘制的特征。这些特征可以是分类特征（例如时区）和连续的实值特征（例如城市人口）。生成的图表是交互式的，对于探索多种机器学习数据非常有用，尤其是涉及到复杂事物（如词语和文档）的向量表示。  

```python
>>> import seaborn
>>> from matplotlib import pyplot as plt
>>> from nlpia2.plots import offline_plotly_scatter_bubble
>>> df = get_data('cities_us_wordvectors_pca2_meta')
>>> html = offline_plotly_scatter_bubble(
...     df.sort_values('population', ascending=False)[:350].copy()\
...         .sort_values('population'),
...     filename='plotly_scatter_bubble.html',
...     x='x', y='y',
...     size_col='population', text_col='name', category_col='timezone',
...     xscale=None, yscale=None,  # “log” 或 None
...     layout={}, marker={'sizeref': 3000})
{'sizemode': 'area', 'sizeref': 3000}
```

为了将 300 维词向量转换为二维表示，需要使用降维技术；我们采用了主成分分析（PCA）。减少输入向量所含信息的范围，可降低 300D 压缩到 2D 时的信息损失；因此，你已把词向量限定为与城市相关的部分。这类似于在计算词频-逆文档频率（TF-IDF）或词袋（Bag-of-Words，BOW）向量时限定语料的领域或主题。

若想获得信息量更大的多样化向量，通常需要使用非线性嵌入算法，如 t 分布随机邻域嵌入（t-SNE）。我们将在后续章节讨论 t-SNE 及其他神经网络技术；掌握本章的词向量嵌入算法后，你会更容易理解 t-SNE。

<h3 id="NaPKM">6.4.6  建立连接</h3>
在本节中，我们将构建一种称为**图**<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">57</font>**</sup>的数据结构。图结构非常适合表示数据中的关系：其核心可视为由实体（节点或顶点）通过关系（边）相连而成。社交网络就是图结构极佳的应用场景。本节采用**无向图**，即关系没有方向；例如 Facebook 中两人的好友关系，需要双方互为好友才能成立。相对地，**有向图**包含单向关系，Twitter 中的“关注/被关注”即为一例：你可以关注某人而对方无需回关，因此可拥有粉丝而无需互相关注。

为了可视化本章中各个思想和概念之间的关系，可以创建一个无向图，在语义相似的句子之间建立连接（边）。你将使用一个力导向布局引擎，将所有相似的概念（节点）聚拢成簇。但首先，你需要为每个句子生成某种嵌入表示。句子通常表达单一的思想，那么如何利用词嵌入（word embeddings）来创建句子的嵌入呢？

你可以应用之前章节学到的词嵌入方法来创建句子嵌入。具体而言，就是将句子中每个单词的嵌入向量取平均，从而得到每个句子的一个 300 维嵌入向量。

<h4 id="B8KSL">从 NLPIA2 手稿中提取自然语言</h4>
你可在 nlpia2 项目 src/nlpia2/data/manuscript 目录（[https://gitlab.com/tangibleai/nlpia2/）下载本书各章节的](https://gitlab.com/tangibleai/nlpia2/）下载本书各章节的) ADOC 文件，如代码清单 6.13 所示。以下示例使用第 6 章手稿。若你自己撰写书籍或文档，请勿照此操作——在待处理文本中不断测试和编辑代码的递归循环会令人崩溃。但现在，你可享受这番“头疼”成果，处理眼前的文字。

```python
>>> import requests
>>> repo = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main'
>>> name = 'Chapter-06_Reasoning-with-word-embeddings-word-vectors.adoc'
>>> url = f'{repo}/src/nlpia2/data/{name}'
>>> adoc_text = requests.get(url)
```

接下来需将文本保存为 ADOC 文件，以便使用命令行工具将其渲染为 HTML。

```python
>>> from pathlib import Path
>>> path = Path.cwd() / name
>>> with path.open('w') as fout:
...     fout.write(adoc_text)
```

随后，应把 ADOC 文本渲染为 HTML，以便更轻松地将自然语言文本与格式字符等“非自然”文本分离，如代码清单 6.15 所示。可使用 Python 包 Asciidoc3 将任意 AsciiDoc （ADOC）文件转换为 HTML。

```python
>>> import subprocess
>>> subprocess.run(args=[ #Asciidoc3 应用程序可将 ADOC 文件渲染为 HTML
...     'asciidoc3', '-a', '-n', '-a', 'icons', path.name])
```

现在你已得到 HTML 文本文件，接下来即可使用 Beautiful Soup 包提取文本。

```python
>>> if os.path.exists(chapt6_html) and os.path.getsize(chapt6_html) > 0:
... chapter6_html = open(chapt6_html, 'r').read()
... bsoup = BeautifulSoup(chapter6_html, 'html.parser')
... text = bsoup.get_text()  #BeautifulSoup.get_text()从HTML提取自然语言
```

现在你已经拥有本章的文本，可以运行 spaCy 的小型英语语言模型来获取句子嵌入向量。SpaCy 将自动对 Doc 对象中的词元向量取平均，从而得到句子的嵌入<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">58</font>**</sup>。除了获取句子向量外，你还需要提取每个句子中的**名词短语（noun phrases）**<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">59,60</font>**</sup>，用作句子向量的标签。

```python
>>> import spacy
>>> nlp = spacy.load('en_core_web_md')
>>> config = {'punct_chars': None}
>>> nlp.add_pipe('sentencizer', config=config)
>>> doc = nlp(text)
>>> sentences = []
>>> noun_phrases = []
>>> for sent in doc.sents:
...     sent_noun_chunks = list(sent.noun_chunks)
...     if sent_noun_chunks:
...         sentences.append(sent)
...         noun_phrases.append(max(sent_noun_chunks))
>>> sent_vecs = []
>>> for sent in sentences:
...     sent_vecs.append(sent.vector)
```

得到句子向量和名词短语之后，你应当对句子向量进行归一化（normalize）<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">61</font>**</sup>，使每个向量的长度（或称 2-范数（2-norm））为 1。计算 2-范数的方法与直角三角形对角线长度的计算方法相同，即每个维度的长度的平方相加，再对总和取平方根：

```python
>>> import numpy as np
>>> vector = np.array([1, 2, 3, 4])
>>> np.sqrt(sum(vector**2))
5.47...
>>> np.linalg.norm(vector)
5.47...
```

将数据归一化到 300 维向量空间中，可确保所有向量处于相同的尺度，同时保留它们之间的差异性<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">62</font>**</sup>。

```python
>>> import numpy as np
>>> for i, sent_vec in enumerate(sent_vecs):
...     sent_vecs[i] = sent_vec / np.linalg.norm(sent_vec)
```

句子向量归一化之后，可以计算这些向量之间的相似度。对一组对象列表中所有可能对象对进行成对的相似度计算，将产生一个称为**相似度矩阵（similarity matrix）或关联矩阵（affinity matrix）**的方阵<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">63</font>**</sup>，如代码清单 6.18 所示。如果你对每个向量与其他向量进行点积运算，实际上你所计算的就是之前章节熟悉的余弦相似度。

```python
>>> np_array_sent_vecs_norm = np.array(sent_vecs)
>>> similarity_matrix = np_array_sent_vecs_norm.dot(
...     np_array_sent_vecs_norm.T)
```

通过对归一化后的句子嵌入矩阵（N×300维）与其自身转置矩阵进行点积运算，即可计算出相似度矩阵。这样生成一个 N×N 的矩阵，每一行和每一列对应本章的一个句子。由于乘法运算的交换律，矩阵的上半三角区域的值与下半三角区域的值完全相同。因此，无论向量之间的相似度计算方向如何，矩阵值都是一样的。

得到相似度矩阵后，你可以创建一个无向图，以句子向量之间的相似度作为边，在相似句子之间建立连接。代码清单 6.19 中的代码使用了 NetworkX<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">64</font>**</sup>库创建无向图数据结构。内部数据存储使用了嵌套字典（nested dictionaries），即字典的字典的字典……依此类推<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">65</font>**</sup>。类似链表，嵌套字典结构能够高效地查询稀疏数据。你计算的相似度矩阵是一个密集矩阵，接下来需要将其变为稀疏矩阵，因为你不希望在图中每个句子都与其他句子相连。你需要去掉相似度低（句子距离远）的句子对之间的连接。

```python
>>> import re
>>> import networkx as nx
>>> similarity_matrix = np.triu(similarity_matrix, k=1) #np.triu 将矩阵的下三角（k=1表示包括对角线）变为零。这种方式使你可以只对上半部分进行阈值检查
>>> iterator = np.nditer(similarity_matrix,
...     flags=['multi_index'], order='C')
>>> node_labels = dict()
>>> G = nx.Graph()
>>> pattern = re.compile(
...     r'[\w\s]*[\'\"?]?[\w\s]+\–?[\w\s]*[\'\"?]?[\w\s]*'
... ) #此处的正则表达式用于清理节点标签字典中不必要的数值或符号，以确保节点标签清晰明确
>>> for edge in iterator:
...     key = 0
...     value = ''
...     if edge > 0.95:                       # 该阈值是任意选择的；对这份数据来说，这是一个不错的分割点
...         key = iterator.multi_index[0]
...         value = str(noun_phrases[iterator.multi_index[0]])
...         if (pattern.fullmatch(value)
...             and (value.lower().rstrip() != 'figure')):
...             node_labels[key] = value
...         G.add_node(iterator.multi_index[0])
...         G.add_edge(iterator.multi_index[0],
...                     iterator.multi_index[1], weight=edge)
```

如下一段代码清单所示，你现在可以使用 matplotlib.pyplot 来可视化刚刚组装好的崭新网络图。

```python
>>> import matplotlib.pyplot as plt
>>> plt.subplot(1, 1, 1)                    # 初始化一个填满窗口的单子图
>>> pos = nx.spring_layout(G, k=0.15, seed=42)  # k 为弹簧常数；数值越大，吸引力越强，节点最终会更靠近
>>> nx.draw_networkx(
...     G,
...     pos=pos,                            # pos 保存节点在二维平面中的 (x, y) 坐标
...     with_labels=True,
...     labels=node_labels,
...     font_weight='bold')
>>> plt.show()
```

![图6.9 用词向量把一个个概念连接起来](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748262827185-cb344f0a-bc46-4af5-a8ad-bec378703d6b.png)

![图6.10 第六章的无向图](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748262865999-283d2448-1b3e-4c2e-9ee1-635aeda10221.png)

最后，在图 6.9 和图 6.10 中，你可以看到你的**无向图**把本书中的自然语言概念聚成了不同簇！力导向图中的弹簧根据与其他概念的连接，把相似概念拉到一起。

每个**节点**表示本章某个句子的平均词嵌入，**边**（线条）表示含义相近的句子之间的连接。观察该图，你会发现中央有一个大型句子簇，拥有最多的连接；更外围还能看到一些较小的主题簇，例如“体育”和“城市”。

中心的稠密簇应包含本章核心思想及其相互关系。放大后可见，这些片段大多围绕“用单词和数字来表示词语”这一主题展开，因为这正是本章讨论的内容。本章最后提供了一些练习，帮助你巩固本节所学。

<h3 id="YL18E">6.4.7  非自然词（Unnatural words）</h3>
词嵌入（word embeddings），例如 Word2Vec，不仅适用于英语单词，也适用于任何**符号序列**——只要符号的顺序与邻近关系能够代表其含义。如果符号具备语义，嵌入便可能有用。

你也许已经想到，词嵌入同样适用于非英语语言。例如，它们对**中文汉字和日文汉字（Kanji）这类表意文字，甚至刻在古埃及陵墓中的象形文字，都大有裨益。对于试图隐藏词义的语言，嵌入与向量推理同样有效：你可以对大量以猪拉丁语（Pig Latin）或其他儿童——甚至罗马皇帝——发明的语言记录进行向量推理。凯撒密码（Caesar ciphers）**<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">66</font>**</sup>**（如 ROT13）以及替换密码（substitution ciphers）**<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">67</font>**</sup> 都难逃 Word2Vec 的向量推理攻击。你甚至不需要解码环（见图 6.11）；只需准备一大批消息或 n-gram，让 Word2Vec 嵌入器分析它们的共现关系即可。

![图6.11 解码环](https://cdn.nlark.com/yuque/0/2025/png/1310472/1748262988494-0acb5d66-4a97-4761-b27d-bc6739bfff0c.png)

Word2Vec 甚至被用来从“非自然词”或 **ID 号**中挖掘信息与关系，例如大学课程号（如 CS-101）、产品型号（如 Koala E7270 或 Galaga Pro）、甚至是序列号、电话号码和邮政编码。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">68</font>**</sup> 若想最大化挖掘这类 ID 之间的关联，需要多样化的句子来承载这些 ID。若 ID 的字符位置本身也隐含意义，把 ID 切分成最小语义单元（类似自然语言中的词或音节）会更有效。

<h2 id="ILbLr">6.5  自我测试</h2>
1. 使用预训练词嵌入，仅凭自然语言简介计算 **Dota 2** 英雄的力量、敏捷和智力。<sup>**<font style="color:#DF2A3F;background-color:#FBDE28;">69</font>**</sup>
2. 将本书另一章节（或任何你想更好理解的文本）中的概念联系可视化成图。
3. 尝试把本书所有章节的词嵌入图形可视化结合在一起。
4. 举例说明词向量如何支持霍夫施塔特（Hofstadter）“八种智能”中的至少两种要素。
5. 派生（fork）**nessvec** 仓库，制作你自己的可视化或 nessvector “角色卡”（character sheets），为你喜欢的词或名人绘制其正念（mindfulness）、道德感（ethicalness）、仁慈（kindness）或影响力（impactfulness）等多维特征。
6. 使用 PCA 和词嵌入，为与你所在地相近的若干城市或事物创建二维地图。尝试将二元词组（bigrams）既作为一个点，也作为两个独立点来绘制。地理词的位置是否与其真实地理位置相关？非地理词又如何？

<h2 id="XmqTF">本章小结</h2>
+ 词向量与向量化推理可解决一些颇为微妙的问题，例如类比题和非同义词关系。
+ 若要保持词向量的时效性并提升其与当下事件和概念的相关度，可使用 gensim 或 PyTorch 重新训练并微调嵌入。
+ **nessvec** 是一个有趣的新工具，可帮助你迅速找到“就在嘴边”的那个词，或可视化词语的“角色卡”。
+ 词嵌入能够揭示人名、地名、公司名甚至职业名称中的隐藏含义。
+ 对城市与国家词向量进行 PCA 投影，可揭示地理距离遥远地区间的文化相似度。
+ 将潜在语义分析向量升级为更强大的词向量的关键，是在生成 n-gram 时遵守句子边界。
+ 仅凭预训练词嵌入，机器就能轻松通过标准化测试中的单词类比题。

---

<h2 id="ycP63">本章注释</h2>
1  参见“表示学习（Representation Learning）”，Papers With Code ([https://paperswithcode.com/area/methodology/representation-learning)。](https://paperswithcode.com/area/methodology/representation-learning%29。)  
2  “This Is Your Brain on Drugs”，维基百科 ([https://en.wikipedia.org/wiki/This_Is_Your_Brain_on_Drugs)。](https://en.wikipedia.org/wiki/This_Is_Your_Brain_on_Drugs%29。)  
3  参见“回顾：节点嵌入（Node Embeddings）”，Ted Kye，圣迭戈机器学习读书会 ([https://github.com/SanDiegoMachineLearning/bookclub/blob/master/graph/graphml-05-GNN1.pdf)。](https://github.com/SanDiegoMachineLearning/bookclub/blob/master/graph/graphml-05-GNN1.pdf%29。)  
4  “语言-大脑编码实验的稳健评估（Robust Evaluation of Language-Brain Encoding Experiments）”，Lisa Beinborn、Samira Abnar 和 Rochelle Choenni ([https://arxiv.org/abs/1904.02547)。](https://arxiv.org/abs/1904.02547%29。)  
5  NLP Highlights，“将人类认知模式关联到 NLP 模型（Linking Human Cognitive Patterns to NLP Models）” ([https://soundcloud.com/nlp-highlights/130-linking-human-cognitive-patterns-to-nlp-models-with-lisa-beinborn)。](https://soundcloud.com/nlp-highlights/130-linking-human-cognitive-patterns-to-nlp-models-with-lisa-beinborn%29。)  
6  [https://beinborn.eu/。](https://beinborn.eu/。)  
7  参见 Daniel Dennett《直觉泵及其他思维工具（Intuition Pumps and Other Tools for Thinking）》（Brilliance Audio 2014）第 96 页。  
8  参见“词嵌入（Word Embeddings）”，Papers With Code ([https://paperswithcode.com/task/word-embeddings)。](https://paperswithcode.com/task/word-embeddings%29。)  
9  参见注重隐私保护的搜索引擎 Startpage ([https://www.startpage.com/)。](https://www.startpage.com/%29。)  
10  参见非营利搜索引擎 Disroot ([https://search.disroot.org)。](https://search.disroot.org%29。)  
11  Wolfram Alpha 使用最先进的 NLP ([https://wolframalpha.com/)。](https://wolframalpha.com/%29。)  
12  参见 Elasticsearch 后端源码 ([https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch)) 及前端 SearchKit 演示 ([https://demo.searchkit.co/type/all?query=prosoc](https://demo.searchkit.co/type/all?query=prosoc) ial%20AI)。  
13  参见 Meilisearch 源码与自托管 Docker 镜像 ([https://github.com/meilisearch/meilisearch](https://github.com/meilisearch/meilisearch)) 以及托管服务 ([https://www.meilisearch.com/)。](https://www.meilisearch.com/%29。)  
14  参见 Searx Git 仓库 ([https://github.com/searx/searx)。](https://github.com/searx/searx%29。)  
15  参见 Apache Solr 主页及 Java 源码 ([https://solr.apache.org/)。](https://solr.apache.org/%29。)  
16  参见 Apache Lucene 主页 ([https://lucene.apache.org/)。](https://lucene.apache.org/%29。)  
17  欧洲搜索引擎 Qwant 遵循更严格的法规以防操纵与欺骗 ([https://www.qwant.com/)。](https://www.qwant.com/%29。)  
18  参见 Sphinx 主页及 C 源码 ([http://sphinxsearch.org/)。](http://sphinxsearch.org/%29。)  
19  “如何在 3 分钟内构建语义搜索引擎（How to Build a Semantic Search Engine in 3 minutes）”，Cole Thienes 与 Jack Pertschuk ([http://nmg.bz/yvjG)。](http://nmg.bz/yvjG%29。)  
20  参见 PyNNDescent Python 包 ([https://pypi.org/project/pynndescent/)。](https://pypi.org/project/pynndescent/%29。)  
21  如果不信，可以全部亲自试试。

22  对不熟悉美国体育的读者说明：波特兰伐木者队和西雅图海湾者队均为美国职业足球大联盟球队。  
23  参见“John Rupert Firth”，维基百科 ([https://en.wikipedia.org/wiki/John_Rupert_Firth)。](https://en.wikipedia.org/wiki/John_Rupert_Firth%29。)  
24  参见 Douglas R. Hofstadter《哥德尔、埃舍尔、巴赫：集异璧之大成（Gödel, Escher, Bach: an Eternal Golden Braid）》（Basic Books 1999）第 26 页。  
25  “在向量空间中高效估计词表示（Efficient Estimation of Word Representations in Vector Space）”，Mikolov、Chen、Corrado、Dean ([https://arxiv.org/pdf/1301.3781.pdf)。](https://arxiv.org/pdf/1301.3781.pdf%29。)  
26  参见“无监督特征学习与深度学习教程（Unsupervised Feature Learning and Deep Learning Tutorial）”，斯坦福大学 ([http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)。](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/%29。)  
27  参见“连续空间词表示中的语言规律（Linguistic Regularities in Continuous Space Word Representations）”，Tomas Mikolov、Wen-tau Yih、Geoffrey Zweig ([https://www.aclweb.org/anthology/N13-1090)。](https://www.aclweb.org/anthology/N13-1090%29。)  
28  参见 Radim Řehůřek 对 Tomas Mikolov 的访谈：Rare Technologies RRP 播客档案第 1 集 ([https://rare-technologies.com/rrp#episode_1_tomas_mikolov_on_ai)。](https://rare-technologies.com/rrp#episode_1_tomas_mikolov_on_ai%29。)  
29  参见“ICLR2013”，Open Review ([https://openreview.net/forum?id=dpCdOWtqXd60&noteId=C8Vn84fgSQ8qa)。](https://openreview.net/forum?id=dpCdOWtqXd60&noteId=C8Vn84fgSQ8qa%29。)  
30  “词与短语的分布式表示及其组合性（Distributed Representations of Words and Phrases and their Compositionality）”，Tomas Mikolov 等 ([https://arxiv.org/pdf/1310.4546)。](https://arxiv.org/pdf/1310.4546%29。)  
31  生成这些交互式 2D 词图的代码可在此获取：[http://nmg.bz/M5G7。](http://nmg.bz/M5G7。)  
32  参见 Daniel C. Dennett《直觉泵及其他思维工具》第 15 章“关于意义或内容的思考工具”（Norton & Company 2014）。  
33  参见 3Top：word2vec-api ([https://github.com/3Top/word2vec-api#wheres-to-get-a-pretrained-model)。](https://github.com/3Top/word2vec-api#wheres-to-get-a-pretrained-model%29。)  
34  原始 Google 300D Word2Vec 模型见 GitHub：[https://github.com/mmihaltz/word2vec-GoogleNews-vectors。](https://github.com/mmihaltz/word2vec-GoogleNews-vectors。)  
35  参见 facebookresearch：fastText ([https://github.com/facebookresearch/fastText)。](https://github.com/facebookresearch/fastText%29。)  
36  参见“Bayard Rustin”，维基百科 ([https://en.wikipedia.org/wiki/Bayard_Rustin)，关于这位民权人士。](https://en.wikipedia.org/wiki/Bayard_Rustin%29，关于这位民权人士。)  
37  参见“Larry Dane Brimner” ([https://en.wikipedia.org/wiki/Larry_Dane_Brimner)，他撰写了超过](https://en.wikipedia.org/wiki/Larry_Dane_Brimner%29，他撰写了超过) 150 本儿童读物。  
38  Google 在 Google Drive 托管了 Mikolov 训练的原始模型：[https://github.com/mmihaltz/word2vec-GoogleNews-vectors。](https://github.com/mmihaltz/word2vec-GoogleNews-vectors。)  
39  Douglas Hofstadter 与 Emmanuel Sander《表面与本质：类比如何点燃思维（Surfaces and Essences: Analogy as the Fuel and Fire of Thinking）》（Basic Books 2013）。  
40  参见“Detector Morse”，Kyle Gorman 与 OHSU ([https://github.com/cslu-nlp/DetectorMorse)。](https://github.com/cslu-nlp/DetectorMorse%29。)  
41  斯坦福 GloVe 项目 ([https://nlp.stanford.edu/projects/glove/)。](https://nlp.stanford.edu/projects/glove/%29。)  
42  有关 SVD 的更多细节见第 4 章与附录 C。  
43  “全局向量（GloVe）词表示”，Jeffrey Pennington、Richard Socher、Christopher D. Manning ([https://nlp.stanford.edu/pubs/glove.pdf)。](https://nlp.stanford.edu/pubs/glove.pdf%29。)  
44  参见 Gensim 关于 Word2Vec 与 GloVe 性能对比：[https://rare-technologies.com/making-sense-of-Word2Vec/#glove_vs_word2vec。](https://rare-technologies.com/making-sense-of-Word2Vec/#glove_vs_word2vec。)  
45  “用子词信息丰富词向量（Enriching Word Vectors with Subword Information）”，Bojanowski 等 ([https://arxiv.org/pdf/1607.04606.pdf)。](https://arxiv.org/pdf/1607.04606.pdf%29。)

46  参见 facebookresearch：FastText 预训练向量文档，GitHub ([https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md)。](https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md%29。)  
47  参见 Peter Norvig“如何编写拼写校正器（How to Write a Spelling Corrector）” ([https://norvig.com/spell-correct.html)。](https://norvig.com/spell-correct.html%29。)  
48  [https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md](https://github.com/facebookresearch/fastText/blob/main/docs/pretrained-vectors.md)  
49  [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)  
50  [https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip.v](https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.zip.v)  
51  [https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec](https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec)  
52  [https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip](https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip)  
53  参见 spaCy 中型语言模型文档 ([https://spacy.io/models/en#en_core_web_md)。](https://spacy.io/models/en#en_core_web_md%29。)  
54  参见 nessvec 源码 ([https://gitlab.com/tangibleai/nessvec](https://gitlab.com/tangibleai/nessvec)) 及教学视频 ([https://prosoci.org/nessvec-videos)。](https://prosoci.org/nessvec-videos%29。)  
55  Niel Chah 的 word2vec4everything 仓库： [https://github.com/nchah/word2vec4everything。](https://github.com/nchah/word2vec4everything。)  
56  “Illini” 指一群人（通常是橄榄球队员和球迷），而非伊利诺伊州的单一区域——大多数“战斗的伊利尼”球迷居住的地方。  
57  参见“图（抽象数据类型）（Graph (Abstract Data Type)）”，维基百科 ([https://en.wikipedia.org/wiki/Graph_(abstract_data_type))。](https://en.wikipedia.org/wiki/Graph_%28abstract_data_type%29%29。)  
58  在 spaCy 中，Span 对象的 vector 属性默认取词元向量的平均值 ([https://spacy.io/api/span#vector)。](https://spacy.io/api/span#vector%29。)  
59  参见“名词短语（Noun Phrase）”，维基百科 ([https://en.wikipedia.org/wiki/Noun_phrase)。](https://en.wikipedia.org/wiki/Noun_phrase%29。)  
60  参见“Span.noun_chunks”，spaCy ([https://spacy.io/api/span#noun_chunks)。](https://spacy.io/api/span#noun_chunks%29。)  
61  参见“范数（数学）：欧几里得范数（Norm (mathematics): Euclidean Norm）”，维基百科 ([https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm)。](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm%29。)  
62  参见 Urvashi Jaitley“为什么数据归一化对机器学习模型很重要（Why Data Normalization Is Necessary for Machine Learning Models）” ([http://nmg.bz/aJ2z)。](http://nmg.bz/aJ2z%29。)  
63  参见“关联矩阵（Affinity Matrix）”，DeepAI ([https://deepai.org/machine-learning-glossary-and-terms/affinity-matrix)。](https://deepai.org/machine-learning-glossary-and-terms/affinity-matrix%29。)  
64  参见 NetworkX 官方网站了解更多信息：[https://networkx.org/。](https://networkx.org/。)  
65  NetworkX 文档提供了更详细的介绍 ([https://networkx.org/documentation/stable/reference/introduction.html#data-structure)。](https://networkx.org/documentation/stable/reference/introduction.html#data-structure%29。)  
66  参见“凯撒密码（Caesar Cipher）”，维基百科 ([https://en.wikipedia.org/wiki/Caesar_cipher)。](https://en.wikipedia.org/wiki/Caesar_cipher%29。)  
67  参见“替换密码（Substitution Cipher）”，维基百科 ([https://en.wikipedia.org/wiki/Substitution_cipher)。](https://en.wikipedia.org/wiki/Substitution_cipher%29。)  
68  参见“Word2Vec 的非 NLP 应用：Towards Data Science”，Kwvk ([https://archive.ph/n5yw3)。](https://archive.ph/n5yw3%29。)  
69  nessvec 与 nlpia2 包都包含 FastText、GloVe 与 Word2Vec 加载器 ([https://gitlab.com/tangibleai/nessvec)，而](https://gitlab.com/tangibleai/nessvec%29，而) nlpia2 还提供脚本 ch06_dota2_wiki_heroes_hist.py ([https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/etl/ch06_dota2_wiki_heroes_hist.py](https://gitlab.com/tangibleai/nlpia2/-/blob/main/src/nlpia2/etl/ch06_dota2_wiki_heroes_hist.py)) 以下载 Dota 2 英雄数据表。

